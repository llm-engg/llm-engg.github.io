
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A comprehensive hands-on course on Large Language Models for industry professionals">
      
      
      
        <link rel="canonical" href="https://llm-engg.github.io/slides/05_llm_implmentation/slides-2/">
      
      
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.21">
    
    
      
        <title>Slides 2 - Large Language Models - A Hands-on Approach</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.2a3383ac.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-LLD65KQHZH"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-LLD65KQHZH",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-LLD65KQHZH",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="deep-purple" data-md-color-accent="purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#topics-covered" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Large Language Models - A Hands-on Approach" class="md-header__button md-logo" aria-label="Large Language Models - A Hands-on Approach" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Large Language Models - A Hands-on Approach
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Slides 2
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="deep-purple" data-md-color-accent="purple"  aria-hidden="true"  type="radio" name="__palette" id="__palette_0">
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../schedule/" class="md-tabs__link">
        
  
  
    
  
  Schedule

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../assignments/" class="md-tabs__link">
        
  
  
    
  
  Assignments and Labs

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Large Language Models - A Hands-on Approach" class="md-nav__button md-logo" aria-label="Large Language Models - A Hands-on Approach" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Large Language Models - A Hands-on Approach
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../schedule/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Schedule
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../assignments/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Assignments and Labs
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#topics-covered" class="md-nav__link">
    <span class="md-ellipsis">
      Topics Covered
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gpt-2-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      GPT-2 Architecture
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gpt-2-architecture_1" class="md-nav__link">
    <span class="md-ellipsis">
      GPT-2 Architecture
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#autoregressive-text-generation" class="md-nav__link">
    <span class="md-ellipsis">
      Autoregressive Text Generation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#logits-to-token-selection" class="md-nav__link">
    <span class="md-ellipsis">
      Logits to Token Selection
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#understanding-the-generation-flow" class="md-nav__link">
    <span class="md-ellipsis">
      Understanding the Generation Flow
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#greedy-decoding-loop" class="md-nav__link">
    <span class="md-ellipsis">
      Greedy Decoding Loop
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#why-untrained-models-generate-gibberish" class="md-nav__link">
    <span class="md-ellipsis">
      Why Untrained Models Generate Gibberish
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#training-overview" class="md-nav__link">
    <span class="md-ellipsis">
      Training Overview
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#evaluating-generative-text-models" class="md-nav__link">
    <span class="md-ellipsis">
      Evaluating Generative Text Models
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Evaluating Generative Text Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#why-evaluation-matters" class="md-nav__link">
    <span class="md-ellipsis">
      Why Evaluation Matters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cross-entropy-loss-for-language-modeling" class="md-nav__link">
    <span class="md-ellipsis">
      Cross Entropy Loss for Language Modeling
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#calculating-cross-entropy-loss" class="md-nav__link">
    <span class="md-ellipsis">
      Calculating Cross Entropy Loss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#understanding-cross-entropy-loss" class="md-nav__link">
    <span class="md-ellipsis">
      Understanding Cross Entropy Loss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#perplexity-interpretable-metric" class="md-nav__link">
    <span class="md-ellipsis">
      Perplexity: Interpretable Metric
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#training-vs-validation-loss" class="md-nav__link">
    <span class="md-ellipsis">
      Training vs Validation Loss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#processing-data-for-training" class="md-nav__link">
    <span class="md-ellipsis">
      Processing Data for Training
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-training-loop" class="md-nav__link">
    <span class="md-ellipsis">
      The Training Loop
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#core-code-training-function" class="md-nav__link">
    <span class="md-ellipsis">
      Core Code: Training Function
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#loading-and-saving-model-weights" class="md-nav__link">
    <span class="md-ellipsis">
      Loading and Saving Model Weights
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Loading and Saving Model Weights">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#core-code-save-and-load" class="md-nav__link">
    <span class="md-ellipsis">
      Core Code: Save and Load
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-points" class="md-nav__link">
    <span class="md-ellipsis">
      Key Points
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llm-loss-surfaces" class="md-nav__link">
    <span class="md-ellipsis">
      LLM Loss Surfaces
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary-key-takeaways" class="md-nav__link">
    <span class="md-ellipsis">
      Summary: Key Takeaways
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Summary: Key Takeaways">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#text-generation-section-47" class="md-nav__link">
    <span class="md-ellipsis">
      Text Generation (Section 4.7)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#evaluation-section-51" class="md-nav__link">
    <span class="md-ellipsis">
      Evaluation (Section 5.1)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training-section-52" class="md-nav__link">
    <span class="md-ellipsis">
      Training (Section 5.2)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-persistence-section-54" class="md-nav__link">
    <span class="md-ellipsis">
      Model Persistence (Section 5.4)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<div class="center-slide">

# LLMs : A Hands-on Approach 

### GPT - 2  : Implementation and Training

</div>

<hr />
<h2 id="topics-covered">Topics Covered</h2>
<ul>
<li>
<p><strong>GPT-2 Architecture Review</strong></p>
<ul>
<li>Layer Normalization</li>
<li>Self-Attention in GPT-2</li>
<li>Feed-Forward Network (FFN)</li>
<li>Residual Connections</li>
</ul>
</li>
<li>
<p><strong>Training GPT-2</strong></p>
</li>
<li>
<p>Text generation</p>
</li>
<li>Greedy decoding loop</li>
<li>Training loop</li>
</ul>
<hr />
<h2 id="gpt-2-architecture">GPT-2 Architecture</h2>
<ul>
<li>GPT model stacks multiple transformer decoder blocks</li>
<li>Each block has:</li>
<li>Masked Multi-Head Self-Attention layer</li>
<li>Feed-Forward Neural Network (FFN)</li>
<li>Layer Normalization and Residual Connections</li>
<li>Final output layer</li>
</ul>
<p><img alt="" src="../images/trf-block-2.png" /></p>
<hr />
<h2 id="gpt-2-architecture_1">GPT-2 Architecture</h2>
<ul>
<li>12 Transformer blocks</li>
<li>768-dimensional hidden states</li>
<li>12 attention heads</li>
<li>Vocabulary size: 50,257 tokens</li>
</ul>
<div class="center-slide">


![](images/gpt2-final.png)

</div>

<hr />
<div class="center-slide">

## Text Generation
</div>

<hr />
<h2 id="autoregressive-text-generation">Autoregressive Text Generation</h2>
<p>LLMs generate text <strong>one token at a time</strong> through an iterative process:</p>
<ol>
<li>Start with an input context (e.g., "Hello, I am")</li>
<li>Model predicts the next token probability distribution</li>
<li>Select the next token (highest probability = greedy decoding)</li>
<li>Append token to context</li>
<li>Repeat until desired length</li>
</ol>
<p><img alt="alt text" src="../images/gen_steps.png" /></p>
<p><strong>Key Insight</strong>: The model consumes its own previous outputs as future inputs - this is the <strong>autoregressive</strong> property.</p>
<hr />
<h2 id="logits-to-token-selection">Logits to Token Selection</h2>
<p>The GPT output is <strong>logits</strong>, not probabilities.</p>
<ul>
<li>Logits ∈ ℝ^V</li>
<li>Softmax converts logits -&gt; probability distribution</li>
<li>Argmax selects most likely token</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="n">probas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">token_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">probas</span><span class="p">)</span>
</code></pre></div>
<p><img alt="alt text" src="../images/output.png" /></p>
<hr />
<h2 id="understanding-the-generation-flow">Understanding the Generation Flow</h2>
<p><strong>Input → Token IDs → Model → Logits → Softmax → Token Selection → Output</strong></p>
<table>
<thead>
<tr>
<th>Step</th>
<th>Operation</th>
<th>Shape Transformation</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Tokenize input</td>
<td>"Hello, I am" → [15496, 11, 314, 716]</td>
</tr>
<tr>
<td>2</td>
<td>Model forward</td>
<td>(1, 4) → (1, 4, 50257)</td>
</tr>
<tr>
<td>3</td>
<td>Extract last logits</td>
<td>(1, 4, 50257) → (1, 50257)</td>
</tr>
<tr>
<td>4</td>
<td>Softmax</td>
<td>logits → probabilities</td>
</tr>
<tr>
<td>5</td>
<td>Argmax</td>
<td>select token ID 257 → "a"</td>
</tr>
<tr>
<td>6</td>
<td>Concatenate</td>
<td>extend sequence for next iteration</td>
</tr>
</tbody>
</table>
<p><img src="images/gen_details.png" style="width: 100%; height: auto;"></p>
<hr />
<h2 id="greedy-decoding-loop">Greedy Decoding Loop</h2>
<p>Greedy decoding selects the <strong>highest-probability token</strong> at each step.</p>
<p>Properties:</p>
<ul>
<li>Deterministic</li>
<li>Fast</li>
<li>Often repetitive / dull</li>
</ul>
<p><img alt="alt text" src="../images/greedy.png" /></p>
<p>Greedy decoding = always take the most confident step.</p>
<ul>
<li>Does Greedy Decoding produce best text?</li>
<li>Why not generate whole sentences?</li>
<li>How different from encoder and decoder models?</li>
</ul>
<hr />
<h2 id="why-untrained-models-generate-gibberish">Why Untrained Models Generate Gibberish</h2>
<p>Before training:</p>
<ul>
<li>Weights are random</li>
<li>Logits are random</li>
<li>Token probabilities are near-uniform</li>
</ul>
<p>With vocab size = 50,257:</p>
<blockquote>
<p>Initial probability ≈ 1 / 50,257 ≈ 0.00002</p>
</blockquote>
<p>Thus generated text is effectively random noise.</p>
<hr />
<div class="center-slide">

  ## Training the GPT
</div>

<hr />
<h2 id="training-overview">Training Overview</h2>
<ul>
<li>Training Loop</li>
<li>Loss Optimization</li>
<li>Data Loading</li>
<li>Model Evaluation</li>
<li>Using Pretrained weights</li>
</ul>
<hr />
<h2 id="evaluating-generative-text-models">Evaluating Generative Text Models</h2>
<h3 id="why-evaluation-matters">Why Evaluation Matters</h3>
<p>Before training, we need metrics to:</p>
<ul>
<li>Measure model performance quantitatively</li>
<li>Track training progress</li>
<li>Detect overfitting</li>
<li>Compare different models</li>
</ul>
<p><strong>Key Challenge</strong>: How do we numerically assess the quality of generated text?</p>
<p><strong>Solution</strong>: Use the model's own probability estimates vs true probabilties for the "correct" next tokens.</p>
<hr />
<h2 id="cross-entropy-loss-for-language-modeling">Cross Entropy Loss for Language Modeling</h2>
<p>The training objective: <strong>Maximize the probability of the correct next token</strong></p>
<p>Given:</p>
<ul>
<li><strong>Inputs</strong>: Token IDs the model sees</li>
<li><strong>Targets</strong>: Token IDs the model should predict (inputs shifted by 1)</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">16833</span><span class="p">,</span> <span class="mi">3626</span><span class="p">,</span> <span class="mi">6100</span><span class="p">],</span>   <span class="c1"># [&quot;every effort moves&quot;,</span>
                       <span class="p">[</span><span class="mi">40</span><span class="p">,</span>    <span class="mi">1107</span><span class="p">,</span> <span class="mi">588</span><span class="p">]])</span>    <span class="c1">#  &quot;I really like&quot;]</span>

<span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">3626</span><span class="p">,</span> <span class="mi">6100</span><span class="p">,</span> <span class="mi">345</span>  <span class="p">],</span>  <span class="c1"># [&quot; effort moves you&quot;,</span>
                        <span class="p">[</span><span class="mi">1107</span><span class="p">,</span> <span class="mi">588</span><span class="p">,</span> <span class="mi">11311</span><span class="p">]])</span>  <span class="c1">#  &quot; really like chocolate&quot;]</span>
</code></pre></div>
<div style="margin-top: 20px; margin-bottom: 20px;">
</div>

<p><strong>Cross-entropy</strong> loss measures how well the model's predicted probabilities match the true next tokens.</p>
<p><strong>Mathematical Formula:</strong></p>
<p>$$
\text{Loss} = -\frac{1}{N} \sum_{i=1}^{N} \log P(x_i^{\text{target}})
$$</p>
<p>Where:</p>
<ul>
<li>$N$ = total number of tokens</li>
<li>$P(x_i^{\text{target}})$ = predicted probability for the correct token at position $i$</li>
<li>The negative sign converts it to a loss (we want to maximize probability = minimize negative log probability)</li>
</ul>
<hr />
<h2 id="calculating-cross-entropy-loss">Calculating Cross Entropy Loss</h2>
<p><img alt="alt text" src="images/ce-1.png.png" /></p>
<hr />
<h2 id="understanding-cross-entropy-loss">Understanding Cross Entropy Loss</h2>
<p><strong>Step-by-step computation</strong></p>
<ol>
<li><strong>Get logits</strong> from model: shape <code>(batch, seq_len, vocab_size)</code></li>
<li><strong>Apply softmax</strong> to get probabilities</li>
<li><strong>Extract target probabilities</strong>: probability assigned to correct tokens</li>
<li><strong>Apply logarithm</strong>: log probabilities are more numerically stable</li>
<li><strong>Average</strong> over all tokens</li>
<li><strong>Negate</strong>: we minimize negative log-likelihood</li>
</ol>
<div class="highlight"><pre><span></span><code><span class="c1"># PyTorch does all 6 steps in one function:</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits_flat</span><span class="p">,</span> <span class="n">targets_flat</span><span class="p">)</span>
</code></pre></div>
<p><strong>Initial loss</strong> (untrained model): ~10.99</p>
<p><strong>Target loss</strong> (well-trained): approaches ~0</p>
<hr />
<h2 id="perplexity-interpretable-metric">Perplexity: Interpretable Metric</h2>
<p><strong>Perplexity</strong> = <code>exp(cross_entropy_loss)</code></p>
<p>Interpretation: The effective vocabulary size the model is "uncertain" about at each step.</p>
<p>Interpretation:</p>
<ul>
<li>Effective number of equally likely tokens</li>
<li>Lower is better</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="n">perplexity</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
<span class="c1"># For initial loss of 10.79: perplexity ≈ 48,725</span>
<span class="c1"># Meaning: model is unsure among ~48,725 tokens</span>
</code></pre></div>
<table>
<thead>
<tr>
<th>Loss</th>
<th>Perplexity</th>
<th>Interpretation</th>
</tr>
</thead>
<tbody>
<tr>
<td>10.79</td>
<td>48,725</td>
<td>Random guessing (vocab size: 50,257)</td>
</tr>
<tr>
<td>5.0</td>
<td>148</td>
<td>Moderate uncertainty</td>
</tr>
<tr>
<td>2.0</td>
<td>7.4</td>
<td>Low uncertainty</td>
</tr>
<tr>
<td>0.5</td>
<td>1.65</td>
<td>Very confident</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="training-vs-validation-loss">Training vs Validation Loss</h2>
<p>During training, split data into training and validation sets</p>
<div class="highlight"><pre><span></span><code><span class="n">train_ratio</span> <span class="o">=</span> <span class="mf">0.90</span>
<span class="n">split_idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">train_ratio</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">text_data</span><span class="p">))</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">text_data</span><span class="p">[:</span><span class="n">split_idx</span><span class="p">]</span>
<span class="n">val_data</span> <span class="o">=</span> <span class="n">text_data</span><span class="p">[</span><span class="n">split_idx</span><span class="p">:]</span>
</code></pre></div>
<p><strong>What to watch for:</strong></p>
<ul>
<li>Training loss <strong>decreases</strong>, Validation loss <strong>decreases</strong> -&gt; Model is learning and generalizing</li>
<li>Training loss <strong>decreases</strong>, Validation loss <strong>increases</strong> -&gt; <strong>OVERFITTING!</strong></li>
</ul>
<hr />
<h2 id="processing-data-for-training">Processing Data for Training</h2>
<p><img alt="alt text" src="../images/data-loading.png" /></p>
<ul>
<li>Inputs and targets are created by shifting token IDs by one position</li>
<li>Given a text corpus</li>
<li>split into training and validation sets</li>
<li>for each set encode it to token IDs</li>
<li>create input-target pairs</li>
</ul>
<p>In Pytorch</p>
<ul>
<li><strong>Dataset</strong> Encodes all text to token IDs</li>
<li><strong>DataLoader</strong> handles batching and shuffling:</li>
</ul>
<hr />
<h2 id="the-training-loop">The Training Loop</h2>
<p>Training updates model weights to minimize loss through <strong>backpropagation</strong> and <strong>gradient descent</strong>.</p>
<p><img alt="alt text" src="../images/train-loop.png" /></p>
<hr />
<h2 id="core-code-training-function">Core Code: Training Function</h2>
<div class="highlight"><pre><span></span><code>    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  <span class="c1"># Enable dropout</span>

        <span class="k">for</span> <span class="n">input_batch</span><span class="p">,</span> <span class="n">target_batch</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>  <span class="c1"># Reset gradients</span>

            <span class="n">loss</span> <span class="o">=</span> <span class="n">calc_loss_batch</span><span class="p">(</span><span class="n">input_batch</span><span class="p">,</span> <span class="n">target_batch</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>        <span class="c1"># Calculate gradients</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>       <span class="c1"># Update weights</span>
</code></pre></div>
<hr />
<h1 id="loading-and-saving-model-weights">Loading and Saving Model Weights</h1>
<p>Persistence Matters</p>
<p><strong>We must save trained models to:</strong></p>
<ul>
<li>Avoid retraining</li>
<li>Share models with others</li>
<li>Resume training later</li>
<li>Deploy to production</li>
</ul>
<hr />
<h2 id="core-code-save-and-load">Core Code: Save and Load</h2>
<div class="highlight"><pre><span></span><code><span class="c1"># ============ SAVE ============</span>

<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s2">&quot;model.pth&quot;</span><span class="p">)</span>

<span class="c1"># Save model + optimizer (for resuming training)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">({</span>
    <span class="s2">&quot;model_state_dict&quot;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
    <span class="s2">&quot;optimizer_state_dict&quot;</span><span class="p">:</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
<span class="p">},</span> <span class="s2">&quot;model_and_optimizer.pth&quot;</span><span class="p">)</span>


<span class="c1"># ============ LOAD ============</span>
<span class="c1"># Load weights into fresh model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPTModel</span><span class="p">(</span><span class="n">GPT_CONFIG_124M</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;model.pth&quot;</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">device</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>  <span class="c1"># Set to evaluation mode</span>

<span class="c1"># Resume training</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;model_and_optimizer.pth&quot;</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPTModel</span><span class="p">(</span><span class="n">GPT_CONFIG_124M</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s2">&quot;model_state_dict&quot;</span><span class="p">])</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s2">&quot;optimizer_state_dict&quot;</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  <span class="c1"># Set to training mode</span>
</code></pre></div>
<hr />
<h2 id="key-points">Key Points</h2>
<p><strong><code>state_dict</code></strong>: Dictionary mapping layer names to parameter tensors</p>
<p><strong><code>map_location</code></strong>: Ensures model loads on correct device (CPU/GPU)</p>
<p><strong><code>model.eval()</code> vs <code>model.train()</code></strong>:</p>
<ul>
<li><code>eval()</code>: Disables dropout, batch norm uses running stats</li>
<li><code>train()</code>: Enables dropout, batch norm uses batch stats</li>
</ul>
<p><strong>Why save optimizer state?</strong></p>
<ul>
<li>AdamW stores momentum and adaptive learning rate history per parameter</li>
<li>Without it, optimizer resets → suboptimal convergence</li>
<li>Essential for resuming training</li>
</ul>
<hr />
<h2 id="llm-loss-surfaces">LLM Loss Surfaces</h2>
<p>LLM training optimizes a <strong>high-dimensional non-convex loss surface</strong> defined by:</p>
<blockquote>
<p>L(θ) = −E[log p_θ(tokenₜ₊₁ | contextₜ)]</p>
</blockquote>
<p>Key properties:</p>
<ul>
<li>Billions of parameters</li>
<li>Extremely overparameterized</li>
<li>Many equivalent minima</li>
<li>Flat basins dominate</li>
</ul>
<hr />
<h1 id="summary-key-takeaways">Summary: Key Takeaways</h1>
<h2 id="text-generation-section-47">Text Generation (Section 4.7)</h2>
<ul>
<li>Autoregressive: one token at a time</li>
<li>Greedy decoding: always pick highest probability</li>
<li>Context window limits how far back model "remembers"</li>
</ul>
<h2 id="evaluation-section-51">Evaluation (Section 5.1)</h2>
<ul>
<li>Cross entropy loss measures prediction quality</li>
<li>Perplexity = effective vocabulary uncertainty</li>
<li>Training/validation split detects overfitting</li>
</ul>
<h2 id="training-section-52">Training (Section 5.2)</h2>
<ul>
<li>Standard PyTorch loop: zero_grad → forward → backward → step</li>
<li>AdamW optimizer with weight decay</li>
<li>Monitor both losses to detect overfitting</li>
</ul>
<h2 id="model-persistence-section-54">Model Persistence (Section 5.4)</h2>
<ul>
<li>Save <code>state_dict</code> for efficient storage</li>
<li>Save optimizer state to resume training</li>
<li>Use <code>model.eval()</code> for consistent inference</li>
</ul>









  




                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "navigation.sections", "navigation.expand", "navigation.top", "search.highlight", "content.code.copy"], "search": "../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
    
  </body>
</html>