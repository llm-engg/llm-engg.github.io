
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A comprehensive hands-on course on Large Language Models for industry professionals">
      
      
      
        <link rel="canonical" href="https://llm-engg.github.io/slides/06_modern_architectures/notes/">
      
      
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.21">
    
    
      
        <title>Modern Architectures - Large Language Models - A Hands-on Approach</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.2a3383ac.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-LLD65KQHZH"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-LLD65KQHZH",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-LLD65KQHZH",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="deep-purple" data-md-color-accent="purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#modern-architectures" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Large Language Models - A Hands-on Approach" class="md-header__button md-logo" aria-label="Large Language Models - A Hands-on Approach" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Large Language Models - A Hands-on Approach
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Modern Architectures
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="deep-purple" data-md-color-accent="purple"  aria-hidden="true"  type="radio" name="__palette" id="__palette_0">
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../schedule/" class="md-tabs__link">
        
  
  
    
  
  Schedule

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../assignments/" class="md-tabs__link">
        
  
  
    
  
  Assignments and Labs

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Large Language Models - A Hands-on Approach" class="md-nav__button md-logo" aria-label="Large Language Models - A Hands-on Approach" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Large Language Models - A Hands-on Approach
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../schedule/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Schedule
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../assignments/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Assignments and Labs
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="modern-architectures">Modern Architectures</h1>
<h1 id="outline-and-goals">Outline and goals</h1>
<ul>
<li>Quick recap of the ‘standard’ transformer (what you implement)</li>
<li>What do most of the large LMs have in common?</li>
<li>What are common variations to the architecture / training process?</li>
</ul>
<p><strong>Today’s theme:</strong> the best way to learn is hands-on experience
the second best way is to try to learn from others’ experience</p>
<hr />
<h1 id="starting-point-the-original-transformer">Starting point: the ‘original’ transformer</h1>
<p><strong>Review: choices in the standard transformer</strong></p>
<p><strong>Position embedding:</strong> sines and cosines
$$ PE_{(pos,2i)} = sin(pos/10000^{2i/d_{\text{model}}}) $$
$$ PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{\text{model}}}) $$</p>
<p><strong>FFN:</strong> ReLU
$$ \text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2 $$</p>
<p><strong>Norm type:</strong> post-norm, LayerNorm</p>
<p><em>[Diagram showing the standard transformer architecture with Attention, Feed Forward, Add &amp; Norm blocks]</em></p>
<hr />
<h1 id="what-we-implemented-gpt-2-decoder-only-transformer">What we implemented – GPT- 2 (Decoder-only transformer)</h1>
<p><strong>Differences from original transformer:</strong>
*   <strong>LayerNorm</strong> is in front of the block (pre-norm)
*   <strong>Learned absolute position embeddings</strong> (not sinusoidal)
*   FF layers use <strong>GeLU activation</strong>, not ReLU
*   <strong>Bias terms included</strong> in linear layers and LayerNorm</p>
<p><em>[Diagram showing GPT-2 architecture with Pre-Norm, Learned Position Embeddings, and GeLU]</em></p>
<p><strong>What do current models use?</strong></p>
<hr />
<h1 id="how-should-we-think-about-architectures">How should we think about architectures?</h1>
<p>Lots of architecture. We'll cover few of them in the next class. </p>
<hr />
<h1 id="lets-look-at-the-data-on-dense-architectures">Let’s look at the data (on dense architectures)</h1>
<p><strong>Learn from the many other models (and papers) out there</strong></p>
<p><em>[Large table screenshot listing various models (GPT, T5, LLaMA, Mistral, etc.) and their specifications]</em></p>
<p><strong>We will talk through many major architecture and hyperparameter variants.</strong></p>
<ul>
<li>What do all these models have in common?</li>
<li>What parts vary?</li>
<li>What can we learn from this?</li>
</ul>
<hr />
<h1 id="what-are-we-going-to-cover">What are we going to cover?</h1>
<p><strong>Common architecture variations</strong>
*   Activations, FFN
*   Attention variants
*   Position embeddings</p>
<p><strong>Hyperparameters that (do or don’t) matter</strong>
*   What is <code>ff_dim</code>? Do <code>multi_head</code> dims always sum to <code>model_dim</code>?
*   How many vocab elements?</p>
<hr />
<h1 id="architecture-variations">Architecture variations..</h1>
<p><strong>Let’s think about the core architecture piece</strong></p>
<p><em>[Table highlighting Norm, Position embedding, and Activations columns]</em></p>
<hr />
<h1 id="pre-vs-post-norm">Pre-vs-post norm</h1>
<p><em>[Diagram comparing Post-LN Transformer vs Pre-LN Transformer]</em></p>
<p><strong>Post-LN Transformer</strong></p>
<p>$x_{l,i}^{post,1} = \text{MultiHeadAtt}(x_{l,i}^{post}, [x_{l,1}^{post}, \dots, x_{l,n}^{post}])$</p>
<p>$x_{l,i}^{post,2} = x_{l,i}^{post} + x_{l,i}^{post,1}$</p>
<p>$x_{l,i}^{post,3} = \text{LayerNorm}(x_{l,i}^{post,2})$</p>
<p>$x_{l,i}^{post,4} = \text{ReLU}(x_{l,i}^{post,3}W^{1,l} + b^{1,l})W^{2,l} + b^{2,l}$</p>
<p>$x_{l,i}^{post,5} = x_{l,i}^{post,3} + x_{l,i}^{post,4}$</p>
<p>$x_{l+1,i}^{post} = \text{LayerNorm}(x_{l,i}^{post,5})$</p>
<p><strong>Pre-LN Transformer</strong></p>
<p>$x_{l,i}^{pre,1} = \text{LayerNorm}(x_{l,i}^{pre})$</p>
<p>$x_{l,i}^{pre,2} = \text{MultiHeadAtt}(x_{l,i}^{pre,1}, [x_{l,1}^{pre,1}, \dots, x_{l,n}^{pre,1}])$</p>
<p>$x_{l,i}^{pre,3} = x_{l,i}^{pre} + x_{l,i}^{pre,2}$</p>
<p>$x_{l,i}^{pre,4} = \text{LayerNorm}(x_{l,i}^{pre,3})$</p>
<p>$x_{l,i}^{pre,5} = \text{ReLU}(x_{l,i}^{pre,4}W^{1,l} + b^{1,l})W^{2,l} + b^{2,l}$</p>
<p>$x_{l+1,i}^{pre} = x_{l,i}^{pre,3} + x_{l,i}^{pre,5}$</p>
<p>Final LayerNorm: </p>
<p>$x_{Final,i}^{pre} \leftarrow \text{LayerNorm}(x_{L+1,i}^{pre})$</p>
<p>Set up LayerNorm so that it doesn’t affect the main residual signal path (on the left)</p>
<p><strong>Almost all modern LMs use pre-norm</strong></p>
<hr />
<h1 id="new-things-double-norm">New things – ‘double’ norm.</h1>
<p><strong>If putting LayerNorms in residual streams is bad.. Why not post-norm outside the stream?</strong></p>
<p><em>[Diagram showing LayerNorm added after the addition step, but outside the main residual path]</em></p>
<p><strong>Recent models:</strong> Grok, Gemma 2. Olmo 2 <em>only</em> does non-residual post norm</p>
<hr />
<h1 id="layernorm-vs-rmsnorm">LayerNorm vs RMSNorm</h1>
<p><strong>Original transformer: LayerNorm</strong> – normalizes the mean and variance across $d_{\text{model}}$
$$ y = \frac{x - E[x]}{\sqrt{\text{Var}[x] + \epsilon}} * \gamma + \beta $$
<strong>Notable models:</strong>
GPT3/2/1, OPT, GPT-J, BLOOM</p>
<p><strong>Many modern LMs: RMSNorm</strong> – does not subtract mean or add a bias term
$$ y = \frac{x}{\sqrt{||x||_2^2 + \epsilon}} * \gamma $$
<strong>Notable models:</strong>
LLaMA-family, PaLM, Chinchilla, T5</p>
<hr />
<h1 id="why-rmsnorm">Why RMSNorm?</h1>
<p><strong>Modern explanation – it’s faster (and just as good).</strong>
*   <strong>Fewer operations</strong> (no mean calculation)
*   <strong>Fewer parameters</strong> (no bias term to store)</p>
<p>$$ y = \frac{x - E[x]}{\sqrt{\text{Var}[x] + \epsilon}} * \gamma + \beta $$</p>
<p><strong>Does this explanation make sense?</strong></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Operator class</th>
<th style="text-align: left;">% flop</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\Delta$ Tensor contraction</td>
<td style="text-align: left;">99.80</td>
</tr>
<tr>
<td style="text-align: left;">$\square$ Stat. normalization</td>
<td style="text-align: left;">0.17</td>
</tr>
<tr>
<td style="text-align: left;">$\bigcirc$ Element-wise</td>
<td style="text-align: left;">0.03</td>
</tr>
</tbody>
</table>
<p>Matrix multiplies are the <em>vast</em> majority of FLOPs (and memory)
[Ivanov et al 2023]</p>
<hr />
<h1 id="why-rmsnorm-2">Why RMSNorm (2)</h1>
<p><strong>Important lesson:</strong> FLOPS are not runtime! (we will discuss this in far more detail later)</p>
<p>[Ivanov et al 2023]</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Operator class</th>
<th style="text-align: left;">% flop</th>
<th style="text-align: left;">% Runtime</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\Delta$ Tensor contraction</td>
<td style="text-align: left;">99.80</td>
<td style="text-align: left;">61.0</td>
</tr>
<tr>
<td style="text-align: left;">$\square$ Stat. normalization</td>
<td style="text-align: left;">0.17</td>
<td style="text-align: left;">25.5</td>
</tr>
<tr>
<td style="text-align: left;">$\bigcirc$ Element-wise</td>
<td style="text-align: left;">0.03</td>
<td style="text-align: left;">13.5</td>
</tr>
</tbody>
</table>
<p><em>[Diagram showing arithmetic intensity]</em>
Left top ("43G") is FLOPS
Right top ("153") is the FLOP-to-memory ratio</p>
<p><strong>RMSNorm can still matter due to the importance of <em>data movement</em></strong></p>
<hr />
<h1 id="more-generally-dropping-bias-terms">More generally: dropping bias terms</h1>
<p><strong>Most modern transformers don’t have bias terms.</strong></p>
<p>Original Transformer:
$$ \text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2 $$</p>
<p>Most implementations (if they’re not gated):
$$ \text{FFN}(x) = \sigma(xW_1)W_2 $$</p>
<p><strong>Reasons:</strong> memory (similar to RMSnorm) and optimization stability</p>
<hr />
<h1 id="layernorm-recap">LayerNorm: recap</h1>
<ul>
<li>
<p><strong>Basically everyone does pre-norm.</strong></p>
<ul>
<li>Intuition – keep the good parts of residual connections</li>
<li>Observations – nicer gradient propagation, fewer spike</li>
<li>Some people add a second norm outside the residual stream (NOT post-norm)</li>
</ul>
</li>
<li>
<p><strong>Most people do RMSnorm</strong></p>
<ul>
<li>In practice, works as well as LayerNorm</li>
<li>But, has fewer parameters to move around, which saves on wallclock time</li>
<li>People more generally drop bias terms since the compute/param tradeoffs are not great.</li>
</ul>
</li>
</ul>
<hr />
<h1 id="activations">Activations</h1>
<p><strong>A whole zoo of activations ..</strong></p>
<p>ReLU, GeLU, Swish, ELU, GLU, GeGLU, ReGLU, SeLU, SwiGLU, LiGLU</p>
<p><strong>What are these things? What do people use? Does it matter?</strong></p>
<hr />
<h1 id="a-few-of-the-common-activations">A few of the common activations</h1>
<p><strong>ReLU</strong>
$$ FF(x) = \max(0, xW_1) W_2 $$
<em>[Graph of ReLU]</em>
<strong>Notable models:</strong>
Original transformer, T5, Gopher, Chinchilla, OPT</p>
<p><strong>GeLU</strong>
$$ FF(x) = \text{GELU}(xW_1)W_2 $$
$$ GELU(x) := x\Phi(x) $$
<em>[Graph of GeLU]</em>
<strong>Notable models:</strong>
GPT1/2/3, GPTJ, GPT-Neox, BLOOM</p>
<p><strong>SwiGLU / GeGLU (next slide..)</strong>
<strong>Notable models:</strong>
Llama, PaLM, T5 v1.1, <em>most models post 2023</em></p>
<hr />
<h1 id="gated-activations-glu">Gated activations (*GLU)</h1>
<p><strong>GLUs modify the ‘first part’ of a FF layer</strong>
$$ FF(x) = \max(0, xW_1) W_2 $$</p>
<p><strong>Instead of a linear + ReLU, augment the above with an (entrywise) linear term</strong>
$$ \max(0, xW_1) \rightarrow \max(0, xW_1) \otimes (xV) $$</p>
<p><strong>This gives the gated variant (ReGLU) – note that we have an extra parameter (V)</strong>
$$ \text{FF}_{\text{ReGLU}}(x) = (\max(0, xW_1) \otimes xV) W_2 $$</p>
<hr />
<h1 id="gated-variants-of-standard-ff-layers">Gated variants of standard FF layers</h1>
<p><strong>GeGLU</strong>
$$ \text{FFN}_{\text{GEGLU}}(x, W, V, W_2) = (\text{GELU}(xW) \otimes xV)W_2 $$
<strong>Notable models:</strong>
T5 v1.1, mT5, LaMDA, Phi3, Gemma 2, Gemma 3</p>
<p><strong>SwiGLU (swish is $x * \text{sigmoid}(x)$)</strong>
$$ \text{FFN}_{\text{SwiGLU}}(x, W, V, W_2) = (\text{Swish}_1(xW) \otimes xV)W_2 $$
<strong>Notable models:</strong>
LLaMa 1/2/3, PaLM, Mistral, OlMo, <em>most models post 2023</em></p>
<p>Note: Gated models use smaller dimensions for the $d_{ff}$ by 2/3</p>
<hr />
<h1 id="serial-vs-parallel-layers">Serial vs Parallel layers</h1>
<p><strong>Normal transformer blocks are serial – they compute attention, then the MLP</strong></p>
<p><em>[Diagram of Serial Transformer Block]</em>
Add -&gt; Dropout -&gt; Feed-Forward -&gt; Norm -&gt; Add -&gt; Dropout -&gt; Attention -&gt; Norm</p>
<p><strong>Could we parallelize the transformer block?</strong></p>
<hr />
<h1 id="parallel-layers">Parallel layers</h1>
<p><strong>A few models (GPTJ, PaLM, GPT-NeoX) do parallel layers. Originally in GPT-J</strong></p>
<p><strong>Parallel Layers</strong> – We use a "parallel" formulation in each Transformer block (Wang &amp; Komatsuzaki, 2021), rather than the standard "serialized" formulation. Specifically, the standard formulation can be written as:
$$ y = x + \text{MLP}(\text{LayerNorm}(x + \text{Attention}(\text{LayerNorm}(x))) $$
Whereas the parallel formulation can be written as:
$$ y = x + \text{MLP}(\text{LayerNorm}(x)) + \text{Attention}(\text{LayerNorm}(x)) $$
The parallel formulation results in roughly 15% faster training speed at large scales, since the MLP and Attention input matrix multiplications can be fused. Ablation experiments showed a small quality degradation at 8B scale but no quality degradation at 62B scale, so we extrapolated that the effect of parallel layers should be quality neutral at the 540B scale.</p>
<p><strong>If implemented right, LayerNorm can be shared, and matrix multiplies can be fused</strong></p>
<p><strong>Recent Models:</strong> Cohere Command A, Falcon 2 11B, Command R+</p>
<hr />
<h1 id="summary-architectures">Summary: architectures</h1>
<p><strong>Pre-vs-post norm:</strong>
*   Everyone does pre-norm (except OPT350M), likely with good reason.</p>
<p><strong>Layer vs RMSnorm:</strong>
*   RMSnorm has clear compute wins, sometimes even performance</p>
<p><strong>Gating:</strong>
*   GLUs seem generally better, though differences are small</p>
<p><strong>Serial vs parallel layers:</strong>
*   No extremely serious ablations, but has a compute win.</p>
<p><em>[Table summary of architectures visible on the right]</em></p>
<hr />
<h1 id="many-variations-in-position-embeddings">Many variations in position embeddings</h1>
<p><strong>Sine embeddings:</strong> add sines and cosines that enable localization
$$ Embed(x, i) = v_x + PE_{pos} $$
$$ PE_{(pos,2i)} = sin(pos/10000^{2i/d_{\text{model}}}) $$
$$ PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{\text{model}}}) $$
<strong>Notable models:</strong>
Original transformer</p>
<p><strong>Absolute embeddings:</strong> add a position vector to the embedding
$$ Embed(x, i) = v_x + u_i $$
<strong>Notable models:</strong>
GPT1/2/3, OPT</p>
<p><strong>Relative embeddings:</strong> add a vector to the attention computation
$$ e_{ij} = \frac{x_i W^Q (x_j W^K + a_{ij}^K)^T}{\sqrt{d_z}} $$
<strong>Notable models:</strong>
T5, Gopher, Chinchilla</p>
<p><strong>Rope embeddings</strong> (next slides..)
<strong>Notable models:</strong>
GPTJ, PaLM, LLaMA
<em>Most 2024+ models</em></p>
<hr />
<h1 id="rope-rotary-position-embeddings">RoPE: rotary position embeddings</h1>
<p><strong>High level thought process: a relative position embedding should be some $f(x, i)$ s.t.</strong>
$$ \langle f(x, i), f(y, j) \rangle = g(x, y, i - j) $$
That is, the attention function <em>only</em> gets to depend on the relative position (i-j). How do existing embeddings not fulfill this goal?</p>
<ul>
<li><strong>Sine:</strong> Has various cross-terms that are not relative
    $$ \langle Embed(x, i), Embed(y, i) \rangle = \langle v_x, v_y \rangle + \langle PE_i, v_y \rangle ... $$</li>
<li><strong>Absolute:</strong> obviously not relative</li>
<li><strong>Relative embeddings:</strong>
    $$ e_{ij} = \frac{x_i W^Q (x_j W^K + a_{ij}^K)^T}{\sqrt{d_z}} $$
    is not an inner product</li>
</ul>
<hr />
<h1 id="rope-rotary-position-embeddings_1">RoPE: rotary position embeddings</h1>
<p><strong>How can we solve this problem?</strong>
*   We want our embeddings to be invariant to absolute position
*   We know that inner products are invariant to arbitrary rotation.</p>
<p><em>[Diagram illustrating vectors rotating]</em>
Position independent embedding -&gt; Rotate "we" by '0 positions', "know" by '1 position' -&gt; Rotate "we" by '2 positions', "know" by '3 positions'. The relative angle between vectors remains constant.</p>
<hr />
<h1 id="rope-rotary-position-embeddings_2">RoPE: rotary position embeddings</h1>
<p><strong>There are many rotations, which one do you pick?</strong></p>
<p><em>[Diagram showing rotation in 2D pairs]</em>
Just pair up the coordinates and rotate them in 2d (motivation: complex numbers)</p>
<p>[Su et al 2021]</p>
<hr />
<h1 id="the-actual-rope-math">The actual RoPE math</h1>
<p><strong>Multiply with sines and cosines</strong></p>
<p>$$ f_{{q,k}}(x_m, m) = \boldsymbol{R}<em>{\Theta,m}^d \boldsymbol{W}</em>{{q,k}} x_m \quad (14) $$</p>
<p>$$ \boldsymbol{R}<em>{\Theta,m}^d = \begin{pmatrix} \cos m\theta_1 &amp; -\sin m\theta_1 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \ \sin m\theta_1 &amp; \cos m\theta_1 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \ 0 &amp; 0 &amp; \cos m\theta_2 &amp; -\sin m\theta_2 &amp; \cdots &amp; 0 &amp; 0 \ 0 &amp; 0 &amp; \sin m\theta_2 &amp; \cos m\theta_2 &amp; \cdots &amp; 0 &amp; 0 \ \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; \cos m\theta</em>{d/2} &amp; -\sin m\theta_{d/2} \ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; \sin m\theta_{d/2} &amp; \cos m\theta_{d/2} \end{pmatrix} \quad (15) $$</p>
<p><strong>Difference with sine embeddings – not additive, no cross terms</strong></p>
<hr />
<h1 id="implementation-and-code-for-rope">Implementation and code for RoPE</h1>
<p><div class="highlight"><pre><span></span><code><span class="n">query_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
<span class="n">key_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
<span class="n">value_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

<span class="c1"># Flash attention requires the input to have the shape</span>
<span class="c1"># batch_size x seq_length x head_dim x hidden_dim</span>
<span class="c1"># therefore we just need to keep the original shape</span>
<span class="n">query_states</span> <span class="o">=</span> <span class="n">query_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">q_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">key_states</span> <span class="o">=</span> <span class="n">key_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">q_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">value_states</span> <span class="o">=</span> <span class="n">value_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">q_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">cos</span><span class="p">,</span> <span class="n">sin</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb</span><span class="p">(</span><span class="n">value_states</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">)</span>
<span class="n">query_states</span><span class="p">,</span> <span class="n">key_states</span> <span class="o">=</span> <span class="n">apply_rotary_pos_emb</span><span class="p">(</span><span class="n">query_states</span><span class="p">,</span> <span class="n">key_states</span><span class="p">,</span> <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span><span class="p">)</span>

<span class="o">...</span>
</code></pre></div>
Same stuff as the usual multi-head self attention below</p>
<p><strong>Note: embedding at <em>each attention operation</em> to enforce position invariance</strong></p>
<hr />
<h1 id="hyperparameters">Hyperparameters</h1>
<p>Transformer hyperparameter questions you might have had in 224n..</p>
<ul>
<li>How much bigger should the feedforward size be compared to hidden size?</li>
<li>How many heads, and should num_heads always divide hidden size?</li>
<li>What should my vocab size be?</li>
</ul>
<p><strong>And other model setting questions</strong>
*   Do people even regularize these huge LMs?
*   How do people scale these models - very deep or very wide?</p>
<hr />
<p><strong>Feedforward – model dimension ratio.</strong></p>
<p>$$ \text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2 $$</p>
<p>There are two dimensions that are relevant – the feedforward dim ($d_{ff}$) and model dim ($d_{model}$). What should their relationship be?</p>
<p>$$ \boldsymbol{d_{ff} = 4 d_{model}} $$</p>
<p>This is <em>almost always</em> true. There’s just a few exceptions.</p>
<hr />
<h1 id="surprising-consensus-hyperparameter-2">Surprising (?) consensus hyperparameter 2</h1>
<p><strong>Head-dim*num-heads to model-dim ratio. As a reminder, slide from 224n.</strong></p>
<blockquote>
<p><strong>Multi-head self-attention is computationally efficient</strong>
*   Even though we compute $h$ many attention heads, it's not really more costly.
    *   We compute $XQ \in \mathbb{R}^{n \times d}$, and then reshape to $\mathbb{R}^{n \times h \times d/h}$. (Likewise for $XK, XV$.)</p>
</blockquote>
<h2 id="the-total-cost-is-still-on2-d-same-as-single-head-attention-with-dimension-d">&gt; *   The total cost is still $O(n^2 d)$, same as single-head attention with dimension $d$.</h2>
<h1 id="what-are-typical-vocabulary-sizes">What are typical vocabulary sizes?</h1>
<p><strong>Monolingual models – 30-50k vocab</strong></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Token count</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Original transformer</td>
<td style="text-align: left;">37000</td>
</tr>
<tr>
<td style="text-align: left;">GPT</td>
<td style="text-align: left;">40257</td>
</tr>
<tr>
<td style="text-align: left;">GPT2/3</td>
<td style="text-align: left;">50257</td>
</tr>
<tr>
<td style="text-align: left;">T5/T5v1.1</td>
<td style="text-align: left;">32128</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA</td>
<td style="text-align: left;">32000</td>
</tr>
</tbody>
</table>
<p><strong>Multilingual / production systems 100-250k</strong></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Token count</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">mT5</td>
<td style="text-align: left;">250000</td>
</tr>
<tr>
<td style="text-align: left;">PaLM</td>
<td style="text-align: left;">256000</td>
</tr>
<tr>
<td style="text-align: left;">GPT4</td>
<td style="text-align: left;">100276</td>
</tr>
<tr>
<td style="text-align: left;">Command A</td>
<td style="text-align: left;">255000</td>
</tr>
<tr>
<td style="text-align: left;">DeepSeek</td>
<td style="text-align: left;">100000</td>
</tr>
<tr>
<td style="text-align: left;">Qwen 15B</td>
<td style="text-align: left;">152064</td>
</tr>
<tr>
<td style="text-align: left;">Yi</td>
<td style="text-align: left;">64000</td>
</tr>
</tbody>
</table>
<p><strong>Monolingual vocabs don’t need to be huge, but multilingual ones do</strong></p>
<hr />
<h1 id="dropout-and-other-regularization">Dropout and other regularization</h1>
<p><strong>Do we need regularization during pretraining?</strong></p>
<p><strong>Arguments against:</strong>
*   There is <em>a lot</em> of data (trillions of tokens), more than parameters.
*   SGD only does a single pass on a corpus (hard to memorize)</p>
<p>This is all quite reasonable.. but what do people do in practice?</p>
<hr />
<h1 id="summary-hyperparameters">Summary: hyperparameters</h1>
<p><strong>Feedforward</strong>
*   Factor-of-4 rule of thumb (8/3 for GLUs) is standard (with some evidence)</p>
<p><strong>Head dim</strong>
*   Head dim*Num head = D model is standard – but low to no validation</p>
<hr />
<h1 id="attention-heads">Attention heads</h1>
<p><strong>GQA / MQA :</strong> Saving inference costs by reducing the number of heads</p>
<p><strong>Sparse or sliding window attention (GPT4/Mistral):</strong> restricting the attention pattern to reduce compute cost</p>
<hr />
<h1 id="gqamqa-reducing-attention-head-cost">GQA/MQA – Reducing attention head cost</h1>
<p><strong>Let’s think about the compute involved for attention</strong></p>
<p><em>[Diagram showing attention calculation $XQ K^T X^T$]</em></p>
<p>$$ \text{softmax} \left( X Q K^T X^T \right) XV = P \cdot V = \text{output} \in \mathbb{R}^{n \times d} $$</p>
<p><strong>Total arithmetic operations ($bnd^2$), total memory accesses ($bnd + bhn^2 + d^2$)</strong></p>
<p>Arithmetic intensity is high $O \left( \left(\frac{1}{k} + \frac{1}{bn} \right)^{-1} \right)$ - we can keep our GPUs running</p>
<hr />
<h1 id="gqamqa-reducing-attention-head-cost_1">GQA/MQA – Reducing attention head cost</h1>
<p><strong>What about the <em>incremental</em> case when we generate text?</strong></p>
<p><strong>Key difference:</strong> can’t parallelize the generation process – needs to be step by step</p>
<p><strong>In this case – we need to incrementaly re-compute/update attention via the ‘KV cache’</strong></p>
<p><em>[Diagram showing KV Caching process]</em>
[Animation from https://medium.com/@joaolages/kv-caching-explained-276520203249]</p>
<hr />
<h1 id="gqamqa-reducing-attention-head-cost_2">GQA/MQA – Reducing attention head cost</h1>
<p><strong>What’s the incremental arithmetic intensity?</strong></p>
<p><strong>Total arithmetic operations ($bnd^2$), total memory accesses ($bn^2d + nd^2$)</strong></p>
<p>Arithmetic intensity is not good $O \left( \left(\frac{n}{d} + \frac{1}{b} \right)^{-1} \right)$ - need large batches + short seq length (n) or big model dimensions (d)</p>
<p><strong>Is there some way around this? The n/d term is difficult to reduce.</strong></p>
<hr />
<h1 id="mqa-just-have-fewer-key-dimensions">MQA – just have fewer key dimensions.</h1>
<p><strong>Key idea – have multiple queries, but just one dimension for keys and values</strong></p>
<p><em>[Diagram showing Multi-Query Attention with shared Keys and Values]</em></p>
<p><strong>We have much fewer items to move in and out of memory (KV Cache)</strong></p>
<p><strong>Total memory access ($bnd + bn^2k + nd^2$), Arithmetic intensity $O \left( \left(\frac{1}{d} + \frac{n}{dh} + \frac{1}{b} \right)^{-1} \right)$</strong></p>
<p>[figure from https://blog.fireworks.ai/multi-query-attention-is-all-you-need-db072e758055]</p>
<hr />
<h1 id="recent-extension-gqa">Recent extension – GQA</h1>
<p><strong>Don’t go all the way to one dimension of KV – have fewer dims</strong></p>
<p><em>[Diagram comparing Multi-head, Grouped-query, and Multi-query attention]</em></p>
<p><strong>Simple knob to control expressiveness (key-query ratio) and inference efficiency</strong></p>
<hr />
<h1 id="does-mqa-hurt-sometimes">Does MQA hurt? Sometimes..</h1>
<p><strong>Small PPL hit w/ MQA [Shazeer 2019]</strong></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Attention</th>
<th style="text-align: left;">$h$</th>
<th style="text-align: left;">$d_k, d_v$</th>
<th style="text-align: left;">$d_{ff}$</th>
<th style="text-align: left;">dev-PPL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">multi-head</td>
<td style="text-align: left;">8</td>
<td style="text-align: left;">128</td>
<td style="text-align: left;">8192</td>
<td style="text-align: left;"><strong>29.9</strong></td>
</tr>
<tr>
<td style="text-align: left;">multi-query</td>
<td style="text-align: left;">8</td>
<td style="text-align: left;">128</td>
<td style="text-align: left;">9088</td>
<td style="text-align: left;">30.2</td>
</tr>
<tr>
<td style="text-align: left;">multi-head</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">128</td>
<td style="text-align: left;">9984</td>
<td style="text-align: left;">31.2</td>
</tr>
<tr>
<td style="text-align: left;">multi-head</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">64</td>
<td style="text-align: left;">9984</td>
<td style="text-align: left;">31.1</td>
</tr>
<tr>
<td style="text-align: left;">multi-head</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">32</td>
<td style="text-align: left;">9984</td>
<td style="text-align: left;">31.0</td>
</tr>
<tr>
<td style="text-align: left;">multi-head</td>
<td style="text-align: left;">8</td>
<td style="text-align: left;">16</td>
<td style="text-align: left;">9984</td>
<td style="text-align: left;">30.9</td>
</tr>
</tbody>
</table>
<p><strong>Low/no hit w/ GQA [Ainslie 2023]</strong>
<em>[Graphs showing Performance vs Time per sample]</em></p>
<hr />
<h1 id="sparse-sliding-window-attention">Sparse / sliding window attention</h1>
<p><strong>Attending to the entire context can be expensive (quadratic).</strong></p>
<p><strong>Build sparse / structured attention that trades off expressiveness vs runtime (GPT3)</strong></p>
<p><em>[Diagrams showing attention matrices: (a) Transformer (full), (b) Sparse Transformer (strided), (c) Sparse Transformer (fixed)]</em></p>
<p>[Child et al 2019]</p>
<hr />
<h1 id="sliding-window-attention">Sliding window attention</h1>
<p><strong>Another variation on this idea – sliding window attention</strong></p>
<p><em>[Diagram comparing Vanilla Attention vs Sliding Window Attention]</em></p>
<p><strong>Just use the main part of the strided pattern – let depth extend effective context (Mistral)</strong></p>
<hr />
<h1 id="current-standard-trick-interleave-full-and-lr-attention">Current standard trick – interleave ‘full’ and ‘LR’ attention</h1>
<p><strong>From Cohere Command A – Every $4^{\text{th}}$ layer is a full attention</strong></p>
<p><em>[Diagram showing Command A Transformer Block sequence: SWA -&gt; SWA -&gt; SWA -&gt; Full]</em></p>
<p><strong>Long-range info via NoPE, short-range info via RoPE + SWA.</strong></p>
<p><strong>Other models</strong> – LLaMA 4, Gemma does SWA+Full RoPE.</p>
<hr />
<h1 id="recap-conclusion-etc">Recap, conclusion, etc.</h1>
<p><strong>Many aspects (arch, hparams) of transformers are in common across the big LMs</strong></p>
<p><em>[Large table summarizing model parameters]</em></p>
<p><strong>Major differences? Position embeddings, activations, tokenization</strong></p>
<h2 id="-">---</h2>
<h1 id="llm-architecture-evolution-2023-2025">LLM Architecture Evolution: 2023 → 2025</h1>
<p><strong>Big Picture: From scaling dense Transformers → engineering-driven efficiency</strong></p>
<ul>
<li>MoE maturity</li>
<li>Long-context stability</li>
<li>Active parameters ≠ total parameters</li>
</ul>
<hr />
<h1 id="2023-scaling-era-baseline-transformers">2023: Scaling Era (Baseline Transformers)</h1>
<p><strong>Representative models:</strong> GPT-4, LLaMA-2</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Component</th>
<th style="text-align: left;">State</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Architecture</strong></td>
<td style="text-align: left;">Dense Transformers dominate<br>Sequential Attention → MLP blocks</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Normalization</strong></td>
<td style="text-align: left;">Pre-Norm becomes standard (Post-Norm effectively dead)<br>Mix of LayerNorm and early RMSNorm</td>
</tr>
<tr>
<td style="text-align: left;"><strong>FFN / Activations</strong></td>
<td style="text-align: left;">Transition to SwiGLU underway but not universal</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Position Encoding</strong></td>
<td style="text-align: left;">RoPE standard, limited long-context stability</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Context</strong></td>
<td style="text-align: left;">8k–32k typical, 128k is exceptional</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Training</strong></td>
<td style="text-align: left;">BF16/FP16<br>FLOPs scale ≈ "just make it bigger"</td>
</tr>
</tbody>
</table>
<p><strong>Key limitation:</strong> Inefficient scaling: cost ∝ parameters</p>
<hr />
<h1 id="2024-efficiency-structure-take-over">2024: Efficiency &amp; Structure Take Over</h1>
<p><strong>Representative models:</strong> DeepSeek-V3, Mistral-Small-3, Grok-2.5</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Component</th>
<th style="text-align: left;">State</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Architecture</strong></td>
<td style="text-align: left;">MoE returns seriously (not research toys anymore)<br>Hybrid attention (GQA, sliding window)</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Normalization</strong></td>
<td style="text-align: left;">RMSNorm wins (LayerNorm mostly gone)</td>
</tr>
<tr>
<td style="text-align: left;"><strong>FFN / Activations</strong></td>
<td style="text-align: left;">SwiGLU is default<br>Shared experts appear in MoE</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Position Encoding</strong></td>
<td style="text-align: left;">NTK-scaled / extended RoPE</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Context</strong></td>
<td style="text-align: left;">32k–128k becomes normal</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Training</strong></td>
<td style="text-align: left;">Still BF16, but FLOPs efficiency matters</td>
</tr>
</tbody>
</table>
<p><strong>Key shift:</strong> Active parameters ≠ total parameters. Cost/performance optimization mindset</p>
<hr />
<h1 id="2025-engineering-first-llms">2025: Engineering-First LLMs</h1>
<p><strong>Representative models:</strong> DeepSeek-R1, Llama-4, Qwen-3, Gemini-3</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Component</th>
<th style="text-align: left;">State</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Architecture</strong></td>
<td style="text-align: left;">MoE is mature (routing, shared experts, stability solved)<br>Dense models still exist, but only when justified</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Block Design</strong></td>
<td style="text-align: left;">Experimental parallel Attention + MLP appears<br>Kernel-fusion-friendly layouts</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Normalization</strong></td>
<td style="text-align: left;">RMSNorm + Pre-Norm is universal</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Position Encoding</strong></td>
<td style="text-align: left;">Long-context-safe RoPE variants are mandatory</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Context</strong></td>
<td style="text-align: left;">128k is baseline<br>Frontier models: 200k → 1M tokens</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Training</strong></td>
<td style="text-align: left;">FP8 adoption begins<br>FLOPs/token is the real metric</td>
</tr>
</tbody>
</table>
<hr />









  




                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "navigation.sections", "navigation.expand", "navigation.top", "search.highlight", "content.code.copy"], "search": "../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
    
  </body>
</html>