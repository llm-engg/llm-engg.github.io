<!doctype html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Slides</title>

  <link rel="stylesheet" href="../reveal/dist/reveal.css">
  <link rel="stylesheet" href="../reveal/dist/theme/solarized.css">
  <!-- PDF export support: append ?print-pdf to URL, then Ctrl+P -->
  <script>
    var link = document.createElement('link');
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match(/print-pdf/gi)
      ? '../reveal/dist/print/pdf.css'
      : '../reveal/dist/print/paper.css';
    document.getElementsByTagName('head')[0].appendChild(link);
  </script>

  <!-- Bigger fonts for classrooms -->
  <style>
    /* Responsive: fill viewport */
    html, body {
      margin: 0;
      padding: 0;
      width: 100%;
      height: 100%;
      overflow: hidden;
    }
    .reveal {
      font-size: 32px;
      width: 100% !important;
      height: 100% !important;
    }
    .reveal .slides {
      width: 90% !important;
      max-width: 90% !important;
      margin-left: 5% !important;
      margin-right: 5% !important;
    }
    .reveal code {
      font-size: 1.0em;
    }
    /* Left-align all slide content */
    .reveal .slides section {
      text-align: left;
    }
    .reveal .slides section ul,
    .reveal .slides section ol {
      display: block;
      text-align: left;
    }
    .reveal .slides section li {
      text-align: left;
    }
    /* Center-align headings */
    .reveal .slides section h1,
    .reveal .slides section h2,
    .reveal .slides section h3 {
      text-align: center;
    }
    /* Center-align tables */
    .reveal .slides section table {
      margin-left: auto;
      margin-right: auto;
    }
    /* Footer styling */
    .reveal .footer {
      position: absolute;
      bottom: 1em;
      left: 1em;
      font-size: 0.5em;
      color: #657b83;
      z-index: 100;
    }
    /* Constrain slides to prevent overflow */
    .reveal .slides section {
      overflow: hidden !important;
      height: 100% !important;
      box-sizing: border-box !important;
    }
    /* Image sizing and background blend for solarized theme */
    .reveal .slides section img,
    .reveal .slides section p img,
    .reveal img {
      max-width: 100% !important;
      max-height: 600px !important;
      width: auto !important;
      height: auto !important;
      object-fit: contain !important;
      display: block !important;
      margin: 0.04em auto !important;
      background-color: #fdf6e3 !important;
      padding: 0.04em !important;
      border-radius: 2px !important;
      border: none !important;
      box-shadow: none !important;
    }
    /* Allow side-by-side images in flex containers */
    .reveal .slides section [style*="display: flex"] img,
    .reveal .slides section [style*="display:flex"] img {
      display: inline-block !important;
      margin: 0 !important;
      max-width: 100% !important;
      max-height: 500px !important;
    }
    /* Float images for text wrap */
    .reveal .slides section img.float-right {
      float: right !important;
      margin: 0 0 0.5em 1em !important;
      max-width: 50% !important;
      max-height: 500px !important;
    }
    .reveal .slides section img.float-left {
      float: left !important;
      margin: 0 1em 0.5em 0 !important;
      max-width: 50% !important;
      max-height: 500px !important;
    }
    /* Centered slide for section breaks */
    .reveal .slides section .center-slide {
      position: absolute !important;
      top: 50% !important;
      left: 50% !important;
      transform: translate(-50%, -50%) !important;
      text-align: center !important;
      width: 90% !important;
    }
    .reveal .slides section .center-slide h1,
    .reveal .slides section .center-slide h2,
    .reveal .slides section .center-slide h3,
    .reveal .slides section .center-slide p {
      text-align: center !important;
      width: 100% !important;
      margin: 0 !important;
    }
    /* Fragment animation: semi-fade-out */
    .reveal .slides section .fragment.semi-fade-out {
      opacity: 1;
      visibility: inherit;
      transition: opacity 0.3s ease;
    }
    .reveal .slides section .fragment.semi-fade-out.visible {
      opacity: 0.3;
    }
    /* Fragment animation: grow with highlight */
    .reveal .slides section .fragment.grow {
      opacity: 1;
      visibility: inherit;
      transition: transform 0.3s ease;
    }
    .reveal .slides section .fragment.grow.visible {
      transform: scale(1.0);
    }
    /* Course header styling */
    .course-header {
      position: fixed;
      top: 10px;
      left: 20px;
      font-size: 16px;
      opacity: 0.6;
    }
  </style>

  <link rel="stylesheet" href="../reveal/plugin/highlight/monokai.css">
  <!-- Chalkboard plugin -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js-plugins@latest/chalkboard/style.css">
</head>
<body>


<div class="reveal">
  <div class="footer"> <a href="https://llm-engg.github.io">Large Language Models : A Hands-on Approach – CCE, IISc</a></div>
  <div class="slides">
    <section><div class="center-slide">

<h1>LLMs : A Hands-on Approach</h1>
<h3>Modern Architectures</h3>
</div>

</section>
<section><h2>Topics Covered</h2>
<ul>
<li><strong>GPT-2 Review</strong><ul>
<li>Training Loop</li>
</ul>
</li>
<li><strong>Modern LLM Architectures</strong><ul>
<li>Norm Types</li>
<li>Activation Functions</li>
<li>Positional Encodings</li>
<li>Attention Variants</li>
<li>Hyperparameters</li>
</ul>
</li>
</ul>
</section>
<section><h2>Recap : GPT-2 Training Loop</h2>
<p>During Training we update model weights to minimize loss through <strong>backpropagation</strong> and <strong>gradient descent</strong>.</p>
<p><img src="images/train-loop.png" alt="alt text"></p>
<p><strong>Training Loop in code</strong></p>
<pre><code class="language-python">
    for epoch in range(num_epochs):
        model.train()  # Enable dropout

        for input_batch, target_batch in train_loader:
            optimizer.zero_grad()  # Reset gradients

            loss = calc_loss_batch(input_batch, target_batch, model, device)
            loss.backward()        # Calculate gradients
            optimizer.step()       # Update weights
</code></pre>
</section>
<section><h2>Loading and Saving Model Weights</h2>
<p><strong>We must save trained models to:</strong></p>
<ul>
<li>Avoid retraining</li>
<li>Share models with others</li>
<li>Resume training later</li>
<li>Deploy to production</li>
</ul>
<pre><code class="language-python"># ============ SAVE ============

torch.save(model.state_dict(), &quot;model.pth&quot;)

# Save model + optimizer (for resuming training)
torch.save({
    &quot;model_state_dict&quot;: model.state_dict(),
    &quot;optimizer_state_dict&quot;: optimizer.state_dict(),
}, &quot;model_and_optimizer.pth&quot;)


# ============ LOAD ============
# Load weights into fresh model
model = GPTModel(GPT_CONFIG_124M)
model.load_state_dict(torch.load(&quot;model.pth&quot;, map_location=device))
model.eval()  # Set to evaluation mode

# Resume training
checkpoint = torch.load(&quot;model_and_optimizer.pth&quot;, map_location=device)
model = GPTModel(GPT_CONFIG_124M)
model.load_state_dict(checkpoint[&quot;model_state_dict&quot;])

optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)
optimizer.load_state_dict(checkpoint[&quot;optimizer_state_dict&quot;])
model.train()  # Set to training mode
</code></pre>
</section>
<section><h2>LLM Loss Surfaces</h2>
<p>LLM training optimizes a <strong>high-dimensional non-convex loss surface</strong> defined by:</p>
<p>$
\mathcal{L}(\theta) = -\frac{1}{N} \sum_{i=1}^{N} \log P_\theta(x_i^{\text{target}})
$</p>
<p>Key properties:</p>
<ul>
<li>Billions of parameters</li>
<li>Extremely overparameterized</li>
<li>Many equivalent minima</li>
<li>Flat basins dominate</li>
</ul>
<p>More details in :</p>
<ul>
<li><a href="https://arxiv.org/html/2505.17646v2">Unveiling the Basin-Like Loss Landscape in Large Language Models</a></li>
<li><a href="https://www.youtube.com/watch?v=lyZorUc8Gm4">Visualizing the Loss Landscape of Neural Nets</a></li>
</ul>
</section>
<section><div class="center-slide">

<h2>Modern Architectures</h2>
</div>

</section>
<section><h2>GPT-2 Architecture</h2>
<p><strong>Position embedding</strong>: learned, absolute</p>
<p><strong>FFN</strong>: GELU</p>
<p>$ \text{FFN}(x) = GELU(xW_1 + b_1)W_2 + b_2 $</p>
<p><strong>Norm type</strong>: Pre-Norm, LayerNorm</p>
<p><img src="images/gpt-2.png" alt="alt text"></p>
</section>
<section><h2>Current Models</h2>
<iframe src="https://docs.google.com/spreadsheets/d/e/2PACX-1vQsF7QOjxAI1f7ud_oYNLRBq6qa3ZzLqtMMF_1xOKKbi5qb6atwvgeYIp4pYjuGXHDTKXMO0IdxBaVw/pubhtml?gid=1330227995&amp;single=true&amp;widget=true&amp;headers=false" style="width:100%;height:100%;border:none;"></iframe>

</section>
<section><h2>Llama 2, LlaMA 3 and Qwen 3 Architectures</h2>
<p><strong>Position embedding</strong>: RoPE (rotary position embeddings)</p>
<p><strong>FFN</strong>: *GLU variant (SwiGLU for LLaMA, GeGLU for Qwen)</p>
<p>$
\textbf{SwiGLU}(x) = \text{Swish}(xW) \otimes (xV)W_2
$</p>
<p><strong>Norm type</strong>: Post-Norm, RMSNorm</p>
<p><img src="images/llama-2.png" alt="alt text"></p>
</section>
<section><h2>Pre-Norm vs Post-Norm</h2>
<p><strong>Almost all models post-2020 use pre-norm.</strong></p>
<p><img src="images/pre-post-ln.png" alt="alt text"></p>
<p><strong>Original Transformer</strong> : Post Norm</p>
<p><code>x → Attention(x) → Add → LayerNorm → FFN → Add → LayerNorm</code></p>
<p><strong>GPT 2</strong> : Pre-Norm</p>
<p><code>x → LayerNorm → Attention → Add → LayerNorm → FFN → Add</code></p>
</section>
<section><h2>Pre-Norm vs Post-Norm</h2>
<image src="images/pre-post-ln.png" >

<p><strong>Why pre-norm wins:</strong></p>
<ul>
<li>Better gradient flow throrugh residual connections. </li>
<li>Practical evidence: almost all modern LLMs use pre-norm</li>
</ul>
<p><strong>Note</strong> : Double norm also used in some models, but not as common as pre-norm. It applies LayerNorm both before and after the sub-layer.</p>
<p><em>Question</em> : <code>BERT was trained with post-norm and it was huge success. But most models use pre-norm. Why?</code></p>
</section>
<section><h2>LayerNorm vs RMSNorm</h2>
<div style="text-align: center;"> 
Strong consensus toward RMSNorm
</div>




<div style="display: flex; gap: 2rem;">

<div style="flex: 1;">

<p><strong>LayerNorm</strong> (original): </p>
<p>Normalize by subtracting mean and dividing by std dev, then scale ($\gamma$) and shift ($\beta$):</p>
<p>$y = \frac{x - E[x]}{\sqrt{Var[x] + \epsilon}} \cdot \gamma + \beta$</p>
<p><em>Models</em> : GPT-1/2/3, OPT, GPT-J, BLOOM</p>
</div>

<div style="flex: 1; border-left: 2px solid #333; padding-left: 2rem;">

<p><strong>RMSNorm</strong> (modern): </p>
<p>Drop the mean subtraction and bias term:</p>
<p>$y = \frac{x}{\sqrt{||x||_2^2 + \epsilon}} \cdot \gamma$</p>
<p><em>Models</em> : LLaMA family, DeepSeek V3, Qwen3 etc</p>
</div>


</div>

<p><strong>Why RMSNorm</strong></p>
<ul>
<li>Fewer operations: RMSNorm requires fewer computations (no mean subtraction, no bias term) which reduces both FLOPs and memory bandwidth.</li>
</ul>
</section>
<section><h2>Dropping bias Terms in FFN and LayerNorm</h2>
<p>Most modern transformers have <strong>no bias terms</strong> in linear layers or LayerNorm.</p>
<p>Original: $FFN(x) = \max(0, xW_1 + b_1)W_2 + b_2$</p>
<p>Modern: $FFN(x) = \sigma(xW_1)W_2$
SiLU activation is used instead of ReLU, but the key point is that <strong>bias terms are removed</strong>.</p>
<p><strong>Reasons:</strong></p>
<ol>
<li>Same memory/data movement argument as RMSNorm -- fewer parameters to load</li>
<li><strong>Optimization stability</strong> -- empirically, dropping bias terms stabilizes training of very large networks</li>
</ol>
<p><em><strong>LayerNorm Recap</strong></em></p>
<ul>
<li>Most models use RMSNorm</li>
<li>Almost all models use pre-norm</li>
</ul>
</section>
<section><h2>Activations &amp; Gated Linear Units (Strong trend toward SwiGLU/GeGLU)</h2>
<p><strong>Evolution of activations:</strong></p>
<table>
<thead>
<tr>
<th>Activation</th>
<th>Formula</th>
<th>Notable Models</th>
</tr>
</thead>
<tbody><tr>
<td>ReLU</td>
<td>$FF(x) = \max(0, xW_1)W_2$</td>
<td>Original transformer, T5, Gopher, OPT</td>
</tr>
<tr>
<td>GeLU</td>
<td>$FF(x) = GELU(xW_1)W_2$ where $GELU(x) = x\Phi(x)$</td>
<td>GPT-1/2/3, GPT-J, BLOOM</td>
</tr>
<tr>
<td>SwiGLU</td>
<td>$FF(x) = (Swish(xW) \otimes xV)W_2$</td>
<td>LLaMA 1/2/3, PaLM, Mistral, <em>most post-2023</em></td>
</tr>
<tr>
<td>GeGLU</td>
<td>$FF(x) = (GELU(xW) \otimes xV)W_2$</td>
<td>T5 v1.1, mT5, Phi3, Gemma 2/3</td>
</tr>
</tbody></table>
<p>where <code>Swish(x) = x * sigmoid(x)</code> and $\otimes$ is elementwise multiplication.</p>
</section>
<section><h2>Gated Linear Units (GLU)</h2>
<p><strong>What do GLUs do?</strong></p>
<ul>
<li>GLUs add a <strong>gating mechanism</strong></li>
<li>Hidden representation element-wise multiplied by a gate $xV$ (learned linear projection)</li>
<li>$xV$ controls information flow through the MLP</li>
</ul>
<p>$\text{Standard:} \quad \sigma(xW_1) \rightarrow \sigma(xW_1) \otimes (xV) \quad \text{(gated)}$</p>
<div style="margin-bottom:30px"></div>

<p><img src="images/glu.png" alt="alt text"></p>
</section>
<section><h2>Gated Linear Units (GLU)</h2>
<p><strong>More number of parameters?</strong></p>
<p><em>The extra parameter V</em> means GLU models have 3 weight matrices <em>(W, V, W2)</em> instead of 2. </p>
<p>How to keep parameter count the same? (memory is the real bottleneck, not compute)</p>
</section>
<section><h2>Gated Linear Units (GLU)</h2>
<p><strong>More number of parameters?</strong></p>
<p><em>The extra parameter V</em> means GLU models have 3 weight matrices <em>(W, V, W2)</em> instead of 2. </p>
<p>How to keep parameter count the same? (memory is the real bottleneck, not compute)</p>
<p><strong>Scale the FF Params</strong> </p>
</section>
<section><h2>Gated Linear Units (GLU)</h2>
<p><strong>More number of parameters?</strong></p>
<p><em>The extra parameter V</em> means GLU models have 3 weight matrices <em>(W, V, W2)</em> instead of 2. </p>
<p>How to keep parameter count the same? (memory is the real bottleneck, not compute)</p>
<p><strong>Scale the FF Params</strong> </p>
<ul>
<li>Standard MLP<ul>
<li>$W_1 \in \mathbb{R}^{d \times d_{ff}}$ + $W_2 \in \mathbb{R}^{d_{ff} \times d}$ = $2 \cdot d \cdot d_{ff}$ params</li>
<li>Total FFN params = $2 \cdot d \cdot 4 d$ = $8 \cdot d^2$.</li>
</ul>
</li>
</ul>
</section>
<section><h2>Gated Linear Units (GLU)</h2>
<p><strong>More number of parameters?</strong></p>
<p><em>The extra parameter V</em> means GLU models have 3 weight matrices <em>(W, V, W2)</em> instead of 2. </p>
<p>How to keep parameter count the same? (memory is the real bottleneck, not compute)</p>
<p><strong>Scale the FF Params</strong> </p>
<ul>
<li><p>Standard MLP</p>
<ul>
<li>$W_1 \in \mathbb{R}^{d \times d_{ff}}$ + $W_2 \in \mathbb{R}^{d_{ff} \times d}$ = $2 \cdot d \cdot d_{ff}$ params</li>
<li>Total FFN params = $2 \cdot d \cdot 4 d$ = $8 \cdot d^2$.</li>
</ul>
</li>
<li><p>Gated MLP: </p>
<ul>
<li>$W \in \mathbb{R}^{d \times d_{ff}}$ + $V \in \mathbb{R}^{d \times d_{ff}}$ + $W_2 \in \mathbb{R}^{d_{ff} \times d}$ = $3 \cdot d \cdot d_{ff}$ params.</li>
</ul>
</li>
</ul>
</section>
<section><h2>Gated Linear Units (GLU)</h2>
<p><strong>More number of parameters?</strong></p>
<p><em>The extra parameter V</em> means GLU models have 3 weight matrices <em>(W, V, W2)</em> instead of 2. </p>
<p>How to keep parameter count the same? (memory is the real bottleneck, not compute)</p>
<p><strong>Scale the FF Params</strong> </p>
<ul>
<li><p>Standard MLP</p>
<ul>
<li>$W_1 \in \mathbb{R}^{d \times d_{ff}}$ + $W_2 \in \mathbb{R}^{d_{ff} \times d}$ = $2 \cdot d \cdot d_{ff}$ params</li>
<li>Total FFN params = $2 \cdot d \cdot 4 d$ = $8 \cdot d^2$.</li>
</ul>
</li>
<li><p>Gated MLP: </p>
<ul>
<li>$W \in \mathbb{R}^{d \times d_{ff}}$ + $V \in \mathbb{R}^{d \times d_{ff}}$ + $W_2 \in \mathbb{R}^{d_{ff} \times d}$ = $3 \cdot d \cdot d_{ff}$ params.</li>
</ul>
</li>
<li><p>To match: </p>
<ul>
<li>set $d_{ff}^{gated} = \frac{2}{3} d_{ff}^{standard} = \frac{2}{3} \cdot 4d = \frac{8}{3}d$. </li>
<li>Total FFN params = $3 \cdot d \cdot \frac{8}{3}d = 8 \cdot d^2$.</li>
</ul>
</li>
</ul>
<p><strong>Scaling Factors:</strong> </p>
<ul>
<li>Standard MLP: $d_{ff} = 4 \cdot d$</li>
<li>Gated MLP: $d_{ff} = \frac{8}{3} \cdot d \approx 2.67 \cdot d$</li>
</ul>
</section>
<section><h2>Serial vs Parallel Layers</h2>
<p><strong>Normal transformer blocks are serial – they compute attention, then the MLP</strong></p>
<p>Standard transformer block can be written as:</p>
<p>$ 
y = x + \text{MLP}(\text{LayerNorm}(x + \text{Attention}(\text{LayerNorm}(x))) 
$</p>
<p>Whereas the parallel formulation can be written as:</p>
<p>$ 
y = x + \text{MLP}(\text{LayerNorm}(x)) + \text{Attention}(\text{LayerNorm}(x)) 
$</p>
<p><img src="images/parallel.png" alt="alt text"></p>
<p><a href="https://arxiv.org/html/2311.01906">image source</a></p>
</section>
<section><h2>Further Reading</h2>
<p><a href="https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison">The Big LLM Architecture Comparison</a></p>
</section>
  </div>
</div>



<script src="../reveal/dist/reveal.js"></script>
<script src="../reveal/plugin/markdown/markdown.js"></script>
<script src="../reveal/plugin/highlight/highlight.js"></script>
<script src="../reveal/plugin/math/math.js"></script>
<script src="../reveal/plugin/notes/notes.js"></script>
<script src="https://cdn.jsdelivr.net/npm/reveal.js-mermaid-plugin@11.6.0/plugin/mermaid/mermaid.js"></script>
<script src="https://cdn.jsdelivr.net/npm/reveal.js-menu@2.1.0/menu.js"></script>
<!-- Chalkboard plugin -->
<script src="https://cdn.jsdelivr.net/npm/reveal.js-plugins@latest/chalkboard/plugin.js"></script>

<script>
Reveal.initialize({
  hash: true,
  slideNumber: "c/t",
  center: false,
  transition: "none",
  controls: true,
  controlsTutorial: true,
  index:true,
  controlsLayout: "bottom-right",
  controlsBackArrows: "faded",
  // Responsive: fill browser width
  width: "100%",
  height: "100%",
  margin: 0.04,
  minScale: 0.2,
  maxScale: 2.0,
  mermaid:{

  },
  menu: {
    side: "left",
    titleSelector: "h1, h2, h3",
    useTextContentForMissingTitles: true
  },
  chalkboard: {
    boardmarkerWidth: 4,
    chalkWidth: 5,
    chalkEffect: 0.5,
    theme: "chalkboard",
    transition: 800,
    readOnly: false
  },
  plugins: [ RevealMarkdown, RevealHighlight, RevealMath, RevealNotes, RevealMermaid, RevealMenu, RevealChalkboard ]
});

</script>

</body>
</html>
