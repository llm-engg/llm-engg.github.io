
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A comprehensive hands-on course on Large Language Models for industry professionals">
      
      
      
        <link rel="canonical" href="https://llm-engg.github.io/slides/03_tokenization/slides/">
      
      
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.21">
    
    
      
        <title>Slides - Large Language Models - A Hands-on Approach</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.2a3383ac.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-LLD65KQHZH"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-LLD65KQHZH",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-LLD65KQHZH",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="deep-purple" data-md-color-accent="purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#large-language-models-a-hands-on-approach" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Large Language Models - A Hands-on Approach" class="md-header__button md-logo" aria-label="Large Language Models - A Hands-on Approach" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Large Language Models - A Hands-on Approach
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Slides
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="deep-purple" data-md-color-accent="purple"  aria-hidden="true"  type="radio" name="__palette" id="__palette_0">
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../schedule/" class="md-tabs__link">
        
  
  
    
  
  Schedule

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../assignments/" class="md-tabs__link">
        
  
  
    
  
  Assignments and Labs

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Large Language Models - A Hands-on Approach" class="md-nav__button md-logo" aria-label="Large Language Models - A Hands-on Approach" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Large Language Models - A Hands-on Approach
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../schedule/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Schedule
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../assignments/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Assignments and Labs
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#large-language-models-a-hands-on-approach" class="md-nav__link">
    <span class="md-ellipsis">
      Large Language Models: A Hands on Approach
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Large Language Models: A Hands on Approach">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tokenization" class="md-nav__link">
    <span class="md-ellipsis">
      Tokenization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#topics-covered" class="md-nav__link">
    <span class="md-ellipsis">
      Topics Covered
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#models-of-the-week" class="md-nav__link">
    <span class="md-ellipsis">
      Models of the Week
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Models of the Week">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#translategemma" class="md-nav__link">
    <span class="md-ellipsis">
      TranslateGemma
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#medgemma" class="md-nav__link">
    <span class="md-ellipsis">
      MedGemma
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#glm-47-flash" class="md-nav__link">
    <span class="md-ellipsis">
      GLM-4.7-Flash
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#motivation" class="md-nav__link">
    <span class="md-ellipsis">
      Motivation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#data-preprocessing-pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      Data Preprocessing Pipeline
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#graph-lr-araw-text-btokenization-b-ctoken-ids-c-dembedding-layer-d-einput-embeddings-e-fllm" class="md-nav__link">
    <span class="md-ellipsis">
      graph LR A[Raw Text] --&gt; B[Tokenization] B --&gt; C[Token IDs] C --&gt; D[Embedding Layer] D --&gt; E[Input Embeddings] E --&gt; F[LLM]
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-full-pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      The Full Pipeline
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#what-is-tokenization" class="md-nav__link">
    <span class="md-ellipsis">
      What Is Tokenization?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#real-world-example-gpt-2-tokenizer" class="md-nav__link">
    <span class="md-ellipsis">
      Real World Example: GPT-2 Tokenizer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tokenization-pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      Tokenization Pipeline
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tokenization-spectrum" class="md-nav__link">
    <span class="md-ellipsis">
      Tokenization Spectrum
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#word-level-tokenization" class="md-nav__link">
    <span class="md-ellipsis">
      Word-Level Tokenization
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#word-level-pros-and-cons" class="md-nav__link">
    <span class="md-ellipsis">
      Word-Level: Pros and Cons
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#character-level-tokenization" class="md-nav__link">
    <span class="md-ellipsis">
      Character-Level Tokenization
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#subword-tokenization" class="md-nav__link">
    <span class="md-ellipsis">
      Subword Tokenization
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#popular-subword-algorithms" class="md-nav__link">
    <span class="md-ellipsis">
      Popular Subword Algorithms
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#byte-pair-encoding-bpe" class="md-nav__link">
    <span class="md-ellipsis">
      Byte Pair Encoding (BPE)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bpe-walkthrough" class="md-nav__link">
    <span class="md-ellipsis">
      BPE Walkthrough
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bpe-walkthrough-continued" class="md-nav__link">
    <span class="md-ellipsis">
      BPE Walkthrough (continued)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bpe-result" class="md-nav__link">
    <span class="md-ellipsis">
      BPE Result
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bpe-pros-and-cons" class="md-nav__link">
    <span class="md-ellipsis">
      BPE: Pros and Cons
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#byte-level-bpe-gpt-2-style" class="md-nav__link">
    <span class="md-ellipsis">
      Byte-Level BPE (GPT-2 Style)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multimodal-tokenization" class="md-nav__link">
    <span class="md-ellipsis">
      Multimodal Tokenization
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vocabulary-size-trade-offs" class="md-nav__link">
    <span class="md-ellipsis">
      Vocabulary Size Trade-offs
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vocabulary-impact-on-parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Vocabulary Impact on Parameters
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model-vocabulary-comparison" class="md-nav__link">
    <span class="md-ellipsis">
      Model Vocabulary Comparison
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    <span class="md-ellipsis">
      References
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#thank-you" class="md-nav__link">
    <span class="md-ellipsis">
      Thank You
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



  <h1>Slides</h1>

<h2 id="large-language-models-a-hands-on-approach">Large Language Models: A Hands on Approach</h2>
<h3 id="tokenization">Tokenization</h3>
<hr />
<h2 id="topics-covered">Topics Covered</h2>
<ul>
<li>Preprocessing text data for LLMs</li>
<li>Tokenization techniques</li>
<li>Byte Pair Encoding (BPE)</li>
<li>Converting tokens into vectors</li>
</ul>
<hr />
<h2 id="models-of-the-week">Models of the Week</h2>
<h3 id="translategemma"><a href="https://blog.google/innovation-and-ai/technology/developers-tools/translategemma/">TranslateGemma</a></h3>
<ul>
<li>SOTA Open weights multilingual translation model</li>
<li>Multimodal capabilities: text + images for context</li>
<li>4B, 12B and 27B versions</li>
</ul>
<h3 id="medgemma"><a href="https://deepmind.google/models/gemma/medgemma/">MedGemma</a></h3>
<ul>
<li>SOTA Open weights medical imaging and document understanding model</li>
<li>CT/MRI/Histopathology Processing, Medical Document Understanding, Multi-domain Classification</li>
</ul>
<h3 id="glm-47-flash"><a href="https://huggingface.co/zai-org/GLM-4.7-Flash">GLM-4.7-Flash</a></h3>
<ul>
<li>Coding, Tool use and Reasoning abilities</li>
<li>30B-A3B MoE model</li>
</ul>
<hr />
<h2 id="motivation">Motivation</h2>
<p>Why does tokenization matter?</p>
<ul>
<li><strong>Cost</strong>: Billing is per token, not per word</li>
<li><strong>Context limits</strong>: Tokenization decides what fits vs what gets truncated</li>
<li><strong>Reasoning failures</strong>: Try "Say Nameeee" vs "Say Name eee" in DeepSeek</li>
<li><strong>Multilingual bias</strong>: Some languages need more tokens for same meaning</li>
</ul>
<hr />
<h2 id="data-preprocessing-pipeline">Data Preprocessing Pipeline</h2>
<ul>
<li>Cannot feed raw text directly into LLMs. </li>
<li>Need numerical representations.</li>
</ul>
<p><img alt="Data Preprocessing" src="../images/preprocess.png" /></p>
<h2 id="graph-lr-araw-text-btokenization-b-ctoken-ids-c-dembedding-layer-d-einput-embeddings-e-fllm"><div class="highlight"><pre><span></span><code>graph LR
    A[Raw Text] --&gt; B[Tokenization]
    B --&gt; C[Token IDs]
    C --&gt; D[Embedding Layer]
    D --&gt; E[Input Embeddings]
    E --&gt; F[LLM]
</code></pre></div></h2>
<h2 id="the-full-pipeline">The Full Pipeline</h2>
<p><strong>Unified Architecture</strong></p>
<p><img alt="Unified Architecture" src="../images/unified.png" /></p>
<hr />
<h2 id="what-is-tokenization">What Is Tokenization?</h2>
<p>Tokenization breaks text into smaller units called <strong>tokens</strong>.</p>
<p>Tokens can be words, subwords, or characters.</p>
<p><strong>Example:</strong> "The transformer architecture is revolutionary."</p>
<table>
<thead>
<tr>
<th>Approach</th>
<th>Tokens</th>
<th>Count</th>
</tr>
</thead>
<tbody>
<tr>
<td>Words</td>
<td><code>["The", "transformer", "architecture", "is", "revolutionary", "."]</code></td>
<td>6</td>
</tr>
<tr>
<td>Characters</td>
<td><code>["T", "h", "e", " ", "t", "r", "a", ...]</code></td>
<td>45</td>
</tr>
<tr>
<td>Subwords</td>
<td><code>["The", " transform", "er", " architecture", " is", " revolution", "ary", "."]</code></td>
<td>8</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="real-world-example-gpt-2-tokenizer">Real World Example: GPT-2 Tokenizer</h2>
<p><a href="https://platform.openai.com/tokenizer">Try it yourself</a></p>
<p><img alt="GPT-2 Tokenizer" src="../images/gpt2-tokenizer.png" /></p>
<hr />
<h2 id="tokenization-pipeline">Tokenization Pipeline</h2>
<p><img alt="Tokenization Pipeline" src="../images/tokenization-pipeline.png" /></p>
<hr />
<h2 id="tokenization-spectrum">Tokenization Spectrum</h2>
<div class="highlight"><pre><span></span><code>Bytes → Characters → Subwords → Words
  ↑                              ↑
 256 tokens                     Millions of tokens
 Long sequences                 Short sequences
</code></pre></div>
<p>Trade-off between vocabulary size and sequence length.</p>
<hr />
<h2 id="word-level-tokenization">Word-Level Tokenization</h2>
<p>Split text into words based on spaces and punctuation.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">re</span>
<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;Hello, world. This, is a test.&quot;</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;(\s)&#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
<span class="c1"># [&#39;Hello,&#39;, &#39; &#39;, &#39;world.&#39;, &#39; &#39;, &#39;This,&#39;, &#39; &#39;, &#39;is&#39;, &#39; &#39;, &#39;a&#39;, &#39; &#39;, &#39;test.&#39;]</span>
</code></pre></div>
<p><img alt="Word Tokenization" src="../images/word_tokenization.png" /></p>
<hr />
<h2 id="word-level-pros-and-cons">Word-Level: Pros and Cons</h2>
<p><strong>Advantages:</strong>
- Short sequences (one token per word)
- Linguistically intuitive
- Fast attention (fewer tokens)</p>
<p><strong>Disadvantages:</strong>
- Huge vocabulary (English needs 100K+ words)
- OOV problem: Unknown words → <code>[UNK]</code>
- Morphological blindness: "run", "runs", "running" are unrelated
- Language-specific: Some languages don't use spaces</p>
<hr />
<h2 id="character-level-tokenization">Character-Level Tokenization</h2>
<p>Split text into individual characters.</p>
<p><strong>Advantages:</strong>
- No OOV tokens (any text can be encoded)
- Tiny vocabulary (~100-300 tokens)
- Handles typos and neologisms</p>
<p><strong>Disadvantages:</strong>
- Very long sequences (5-10x longer)
- Slow attention (more tokens)
- Poor semantics (characters lack meaning)
- Harder to learn word structure</p>
<hr />
<h2 id="subword-tokenization">Subword Tokenization</h2>
<p>The sweet spot: split rare words, keep common words whole.</p>
<table>
<thead>
<tr>
<th>Word</th>
<th>Subword Tokens</th>
<th>Interpretation</th>
</tr>
</thead>
<tbody>
<tr>
<td>the</td>
<td><code>["the"]</code></td>
<td>Common → single token</td>
</tr>
<tr>
<td>transformer</td>
<td><code>["trans", "former"]</code></td>
<td>Split into known pieces</td>
</tr>
<tr>
<td>unhappiness</td>
<td><code>["un", "happi", "ness"]</code></td>
<td>Morphemes preserved</td>
</tr>
<tr>
<td>GPT-4</td>
<td><code>["G", "PT", "-", "4"]</code></td>
<td>Unknown → character fallback</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="popular-subword-algorithms">Popular Subword Algorithms</h2>
<ol>
<li><strong>Byte Pair Encoding (BPE)</strong> - Frequency-based merging</li>
<li><strong>WordPiece</strong> - Probability-based merging (BERT)</li>
<li><strong>SentencePiece</strong> - Unigram language model (LLaMA)</li>
</ol>
<hr />
<h2 id="byte-pair-encoding-bpe">Byte Pair Encoding (BPE)</h2>
<p><em>Sennrich et al. (2016)</em></p>
<p><strong>Core idea:</strong> Iteratively merge the most frequent pair of tokens.</p>
<ol>
<li>Start with vocabulary of individual characters (or bytes)</li>
<li>Count all adjacent pairs in the corpus</li>
<li>Merge the most frequent pair into a new token</li>
<li>Repeat until reaching desired vocabulary size</li>
</ol>
<hr />
<h2 id="bpe-walkthrough">BPE Walkthrough</h2>
<p><strong>Input:</strong> "low lower lowest"</p>
<p><strong>Step 1 - Initialize tokens:</strong>
<div class="highlight"><pre><span></span><code>[&#39;l&#39;, &#39;o&#39;, &#39;w&#39;, &#39; &#39;, &#39;l&#39;, &#39;o&#39;, &#39;w&#39;, &#39;e&#39;, &#39;r&#39;, &#39; &#39;, &#39;l&#39;, &#39;o&#39;, &#39;w&#39;, &#39;e&#39;, &#39;s&#39;, &#39;t&#39;]
</code></pre></div></p>
<p><strong>Step 2 - Count pairs:</strong>
<div class="highlight"><pre><span></span><code>(&#39;l&#39;, &#39;o&#39;): 3    (&#39;o&#39;, &#39;w&#39;): 3    (&#39;w&#39;, &#39; &#39;): 3
(&#39; &#39;, &#39;l&#39;): 2    (&#39;e&#39;, &#39;r&#39;): 1    (&#39;e&#39;, &#39;s&#39;): 1
</code></pre></div></p>
<p>--</p>
<h2 id="bpe-walkthrough-continued">BPE Walkthrough (continued)</h2>
<p><strong>Step 3 - Merge most frequent pair</strong> <code>('l', 'o')</code> → <code>'lo'</code>
<div class="highlight"><pre><span></span><code>[&#39;lo&#39;, &#39;w&#39;, &#39; &#39;, &#39;lo&#39;, &#39;w&#39;, &#39;e&#39;, &#39;r&#39;, &#39; &#39;, &#39;lo&#39;, &#39;w&#39;, &#39;e&#39;, &#39;s&#39;, &#39;t&#39;]
</code></pre></div></p>
<p><strong>Step 4 - Count pairs again:</strong>
<div class="highlight"><pre><span></span><code>(&#39;lo&#39;, &#39;w&#39;): 3    (&#39;w&#39;, &#39; &#39;): 3    (&#39; &#39;, &#39;lo&#39;): 2
</code></pre></div></p>
<p><strong>Step 5 - Merge</strong> <code>('lo', 'w')</code> → <code>'low'</code>
<div class="highlight"><pre><span></span><code>[&#39;low&#39;, &#39; &#39;, &#39;low&#39;, &#39;e&#39;, &#39;r&#39;, &#39; &#39;, &#39;low&#39;, &#39;e&#39;, &#39;s&#39;, &#39;t&#39;]
</code></pre></div></p>
<p>--</p>
<h2 id="bpe-result">BPE Result</h2>
<p><strong>Final tokenization:</strong>
<div class="highlight"><pre><span></span><code>&quot;low lower lowest&quot; → [&quot;low&quot;, &quot; &quot;, &quot;low&quot;, &quot;er&quot;, &quot; &quot;, &quot;low&quot;, &quot;est&quot;]
</code></pre></div></p>
<p><strong>Can encode any word:</strong>
<div class="highlight"><pre><span></span><code>&quot;lowestness&quot; → [&quot;low&quot;, &quot;est&quot;, &quot;ness&quot;]
</code></pre></div></p>
<p><img alt="Subwords" src="../images/subwords.png" /></p>
<hr />
<h2 id="bpe-pros-and-cons">BPE: Pros and Cons</h2>
<p><strong>Advantages:</strong>
- Balances vocabulary size and sequence length
- Handles OOV words by breaking into subwords
- Captures morphological structure</p>
<p><strong>Disadvantages:</strong>
- Greedy algorithm (may not find optimal tokenization)
- Training corpus dependent</p>
<hr />
<h2 id="byte-level-bpe-gpt-2-style">Byte-Level BPE (GPT-2 Style)</h2>
<p>GPT-2 uses byte-level BPE:</p>
<ul>
<li>Start with <strong>256 byte tokens</strong> (not Unicode characters)</li>
<li>All text is UTF-8 encoded first</li>
<li>Merges operate on bytes, not characters</li>
<li>Avoids merges beyond word boundaries</li>
</ul>
<p><strong>Advantages:</strong>
- Handles any language without special tokenization
- Works with emojis, rare scripts, binary data
- No <code>[UNK]</code> tokens ever needed</p>
<hr />
<h2 id="multimodal-tokenization">Multimodal Tokenization</h2>
<p>Modern models tokenize more than text:</p>
<ul>
<li><strong>Text</strong>: Subword tokenization (BPE, WordPiece)</li>
<li><strong>Images</strong>: Patch-based (e.g., ViT) - split into 16x16 patches</li>
<li><strong>Video</strong>: Frame + patch tokenization</li>
<li><strong>Audio</strong>: Spectrogram frames</li>
</ul>
<div class="highlight"><pre><span></span><code>graph LR
    T[Text Input] --&gt; TT[Text Tokenizer]
    I[Image Input] --&gt; IT[Image Tokenizer]
    A[Audio Input] --&gt; AT[Audio Tokenizer]
    V[Video Input] --&gt; VT[Video Tokenizer]

    TT --&gt; U[Unified Token Sequence]
    IT --&gt; U
    AT --&gt; U
    VT --&gt; U

    U --&gt; M[Transformer Model]
</code></pre></div>
<hr />
<h2 id="vocabulary-size-trade-offs">Vocabulary Size Trade-offs</h2>
<p><strong>Smaller Vocabulary:</strong>
- Faster training and inference
- Lower memory usage
- Longer sequences</p>
<p><strong>Larger Vocabulary:</strong>
- Shorter sequences
- Better semantic representation
- Higher memory usage</p>
<hr />
<h2 id="vocabulary-impact-on-parameters">Vocabulary Impact on Parameters</h2>
<p>$$
\text{Embedding Matrix} = V \times E
$$</p>
<p>$$
\text{Output Layer} = H \times V
$$</p>
<p>Where $V$ = vocabulary size, $E$ = embedding dimension, $H$ = hidden dimension</p>
<hr />
<h2 id="model-vocabulary-comparison">Model Vocabulary Comparison</h2>
<table>
<thead>
<tr>
<th>Model</th>
<th>Vocabulary Size</th>
<th>Tokenizer</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-2</td>
<td>50,257</td>
<td>BPE</td>
</tr>
<tr>
<td>GPT-4</td>
<td>~100,000</td>
<td>BPE variant</td>
</tr>
<tr>
<td>BERT</td>
<td>30,522</td>
<td>WordPiece</td>
</tr>
<tr>
<td>LLaMA</td>
<td>32,000</td>
<td>SentencePiece</td>
</tr>
<tr>
<td>Claude</td>
<td>~100,000</td>
<td>BPE variant</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="references">References</h2>
<ol>
<li><a href="https://www.youtube.com/watch?v=zduSFxRajkE">Let's Build the GPT Tokenizer</a> - Andrej Karpathy</li>
<li><a href="https://huggingface.co/spaces/HuggingFaceTB/smol-training-playbook#the-tokenizer">The Smol Training Playbook</a></li>
<li><a href="https://arxiv.org/abs/1508.07909">Neural Machine Translation of Rare Words with Subword Units</a> - Sennrich et al., 2016</li>
</ol>
<hr />
<h2 id="thank-you">Thank You</h2>
<p>Questions?</p>









  




                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "navigation.sections", "navigation.expand", "navigation.top", "search.highlight", "content.code.copy"], "search": "../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
    
  </body>
</html>