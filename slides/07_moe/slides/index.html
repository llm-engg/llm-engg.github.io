
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A comprehensive hands-on course on Large Language Models for industry professionals">
      
      
      
        <link rel="canonical" href="https://llm-engg.github.io/slides/07_moe/slides/">
      
      
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.21">
    
    
      
        <title>Slides - Large Language Models - A Hands-on Approach</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.2a3383ac.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-LLD65KQHZH"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-LLD65KQHZH",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-LLD65KQHZH",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="deep-purple" data-md-color-accent="purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#overview" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Large Language Models - A Hands-on Approach" class="md-header__button md-logo" aria-label="Large Language Models - A Hands-on Approach" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Large Language Models - A Hands-on Approach
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Slides
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="deep-purple" data-md-color-accent="purple"  aria-hidden="true"  type="radio" name="__palette" id="__palette_0">
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../schedule/" class="md-tabs__link">
        
  
  
    
  
  Schedule

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../assignments/" class="md-tabs__link">
        
  
  
    
  
  Assignments and Labs

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Large Language Models - A Hands-on Approach" class="md-nav__button md-logo" aria-label="Large Language Models - A Hands-on Approach" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Large Language Models - A Hands-on Approach
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../schedule/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Schedule
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../assignments/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Assignments and Labs
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#overview" class="md-nav__link">
    <span class="md-ellipsis">
      Overview
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#motivation" class="md-nav__link">
    <span class="md-ellipsis">
      Motivation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#decoder-only-llms" class="md-nav__link">
    <span class="md-ellipsis">
      Decoder-only LLMs
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#decoder-only-llms_1" class="md-nav__link">
    <span class="md-ellipsis">
      Decoder-only LLMs
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#moe-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      MoE Architecture
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#moe-architecture_1" class="md-nav__link">
    <span class="md-ellipsis">
      MoE Architecture
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#why-are-moes-getting-popular" class="md-nav__link">
    <span class="md-ellipsis">
      Why are MoEs getting popular?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#why-are-moes-getting-popular_1" class="md-nav__link">
    <span class="md-ellipsis">
      Why are MoEs getting popular?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#moe-architectures" class="md-nav__link">
    <span class="md-ellipsis">
      MoE Architectures
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#routing-mechanisms" class="md-nav__link">
    <span class="md-ellipsis">
      Routing Mechanisms
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#top-k-routing-in-detail" class="md-nav__link">
    <span class="md-ellipsis">
      Top-K routing in detail
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#top-k-routing-in-more-detail" class="md-nav__link">
    <span class="md-ellipsis">
      Top-K routing in more detail
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Top-K routing in more detail">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#why-not-just-softmax-without-top-k" class="md-nav__link">
    <span class="md-ellipsis">
      Why not just softmax without top-K?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#router-collapse" class="md-nav__link">
    <span class="md-ellipsis">
      Router Collapse
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Router Collapse">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-problem" class="md-nav__link">
    <span class="md-ellipsis">
      The Problem
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#consequences" class="md-nav__link">
    <span class="md-ellipsis">
      Consequences
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mitigating-router-collapse" class="md-nav__link">
    <span class="md-ellipsis">
      Mitigating Router Collapse
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Mitigating Router Collapse">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#strategies" class="md-nav__link">
    <span class="md-ellipsis">
      Strategies
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#expert-configuration" class="md-nav__link">
    <span class="md-ellipsis">
      Expert Configuration
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#expert-configuration_1" class="md-nav__link">
    <span class="md-ellipsis">
      Expert Configuration
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#expert-configuration_2" class="md-nav__link">
    <span class="md-ellipsis">
      Expert Configuration
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Expert Configuration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#expert-routing-configurations-for-major-moes" class="md-nav__link">
    <span class="md-ellipsis">
      Expert Routing Configurations for Major MoEs
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#training-moes" class="md-nav__link">
    <span class="md-ellipsis">
      Training MoEs
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#heuristic-balancing-losses" class="md-nav__link">
    <span class="md-ellipsis">
      Heuristic Balancing Losses
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#expert-capacity" class="md-nav__link">
    <span class="md-ellipsis">
      Expert Capacity
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Expert Capacity">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#hardware-efficiency-vs-dynamic-routing" class="md-nav__link">
    <span class="md-ellipsis">
      Hardware Efficiency vs. Dynamic Routing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#handling-overflow-dropped-tokens" class="md-nav__link">
    <span class="md-ellipsis">
      Handling Overflow (Dropped Tokens)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#computing-output-of-moe-layer" class="md-nav__link">
    <span class="md-ellipsis">
      Computing output of MoE layer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#total-vs-activate-parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Total vs Activate Parameters
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#total-vs-activate-parameters_1" class="md-nav__link">
    <span class="md-ellipsis">
      Total vs Activate Parameters
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#deepseek-v3-moe" class="md-nav__link">
    <span class="md-ellipsis">
      DeepSeek V3 MoE
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#deepseek-v3-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      DeepSeek V3 Architecture
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



  <h1>Slides</h1>

<div class="center-slide">

# LLMs : A Hands-on Approach 

### Mixture of Experts

</div>

<hr />
<h2 id="overview">Overview</h2>
<ul>
<li>Motivation</li>
<li>MoE Architecture</li>
<li>What do MoEs look like in LLMs?</li>
<li>Dense vs Sparse MoE</li>
<li>Routing Mechanisms</li>
<li>Expert Configuration</li>
<li>DeepSeek V3 MoE Architecture</li>
</ul>
<hr />
<h2 id="motivation">Motivation</h2>
<p><img alt="alt text" src="../images/timeline.png" /></p>
<hr />
<h2 id="decoder-only-llms">Decoder-only LLMs</h2>
<p><img alt text="decoder-only architecture" src="images/llm-decoder.png" class="float-right"/></p>
<ul>
<li>Decoder-only is the predominant architecture for LLMs</li>
<li>
<p>Core components</p>
<ul>
<li>Self-attention</li>
<li>Feedforward (FFN)</li>
<li>LayerNorm and residual connections</li>
</ul>
</li>
<li>
<p>Most parameters are in the FFN layers</p>
</li>
</ul>
<hr />
<h2 id="decoder-only-llms_1">Decoder-only LLMs</h2>
<p><img alt text="flops_vs_params" src="images/flops-params.png" class="float-right"/></p>
<ul>
<li>Most parameters are in the FFN layers</li>
<li>The FFN layers are the main bottleneck for scaling up model capacity</li>
<li>Increasing FFN size leads to quadratic growth in parameters and compute</li>
</ul>
<hr />
<h2 id="moe-architecture">MoE Architecture</h2>
<ul>
<li>Replace big FFN with multiple smaller FFNs (experts)</li>
<li>Only a subset of experts are active for each input token</li>
<li>Routing mechanism decides which experts to activate</li>
<li>Increase parameters without increasing compute</li>
</ul>
<p><img alt="alt text" src="../images/llm-moe.png" /></p>
<hr />
<h2 id="moe-architecture_1">MoE Architecture</h2>
<ul>
<li>All layers can be MoE</li>
<li>Some layers (e.g., every 2nd layer) can be MoE in an interleaved fashion.</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">moe_layer</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">experts</span><span class="p">,</span> <span class="n">router</span><span class="p">,</span> <span class="n">top_k</span><span class="p">):</span> 
    <span class="c1"># Ask the router &quot;which experts should handle this token?&quot; </span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">router</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> 

    <span class="c1"># With N total experts, pick only top_k (top_k &lt;&lt; N) </span>
    <span class="n">top_k_logits</span><span class="p">,</span> <span class="n">top_k_experts</span> <span class="o">=</span> <span class="n">top_k</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">top_k</span><span class="p">)</span> 
    <span class="c1"># Compute experts&#39; mixing weights</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">top_k_logits</span><span class="p">)</span>

    <span class="c1"># Mix only top_k experts together to provide the final output    </span>
    <span class="n">output</span> <span class="o">=</span> <span class="mi">0</span> 
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">expert_idx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">top_k_experts</span><span class="p">):</span> 
        <span class="n">output</span> <span class="o">+=</span> <span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">experts</span><span class="p">[</span><span class="n">expert_idx</span><span class="p">](</span><span class="n">token</span><span class="p">)</span> 
    <span class="k">return</span> <span class="n">output</span> 
</code></pre></div>
<hr />
<h2 id="why-are-moes-getting-popular">Why are MoEs getting popular?</h2>
<ul>
<li>
<p><strong>Efficient Scaling:</strong> : Add model capacity (total parameters) without increasing active parameters (compute) significantly</p>
</li>
<li>
<p><strong>Efficient Pretraining and Inference</strong> - Faster training and inference compared to dense models of similar capacity</p>
</li>
</ul>
<p><img alt="alt text" src="../images/moe-flops.png" /></p>
<hr />
<h2 id="why-are-moes-getting-popular_1">Why are MoEs getting popular?</h2>
<ul>
<li><strong>Improved Performance:</strong> - Better performance on many tasks by leveraging specialization and ensemble effects</li>
<li>DeepSeek V2 236B MoE with 21B active parameters outperforms dense models with 100B+ parameters</li>
</ul>
<p><img alt="alt text" src="../images/moe-vs-dense.png" /></p>
<hr />
<h2 id="moe-architectures">MoE Architectures</h2>
<p>Three things vary across MoE architectures:</p>
<ol>
<li><strong>Routing function</strong> - how tokens get assigned to experts</li>
<li><strong>Expert sizes</strong> - how many experts, how large each one is, shared vs routed</li>
<li><strong>Training objectives</strong> - how to train the router and keep experts balanced</li>
</ol>
<hr />
<h2 id="routing-mechanisms">Routing Mechanisms</h2>
<ul>
<li>Token choice top K</li>
<li>Expert choice top K</li>
<li>Hash-based routing</li>
<li>Learnable routing networks</li>
</ul>
<p><img alt text="routing mechanisms" src="images/routings.png" ></p>
<hr />
<h2 id="top-k-routing-in-detail">Top-K routing in detail</h2>
<p><strong>1. Compute Gating Scores</strong> (e.g., dot product)</p>
<div style="text-align: center;">

$$s_{i,t} = \text{Softmax}_i\left(\mathbf{u}_t^{lT} \mathbf{e}_i^l\right)$$

- $\mathbf{u}_t^l$ : input token representation at layer $l$ 
- $\mathbf{e}_i^l$ : expert embedding for expert $i$ at layer $l$

</div>

<p><strong>2. Select Top-K Experts</strong></p>
<p><img alt="alt text" src="../images/topk.png" /></p>
<p><strong>3. Compute Mixed Output</strong></p>
<div style="text-align: center;">
$$ \mathbf{h}_t = \sum_{i=1}^{N} g_{i,t} \cdot \text{FFN}_i(\mathbf{u}_t) + \mathbf{u}_t $$

where $\text{FFN}_i$ is the feedforward network for expert $i$
</div>

<hr />
<h2 id="top-k-routing-in-more-detail">Top-K routing in more detail</h2>
<h4 id="why-not-just-softmax-without-top-k">Why not just softmax without top-K?</h4>
<!-- .element: class="fragment" -->

<ul>
<li>You immediately lose the systems efficiency</li>
</ul>
<!-- .element: class="fragment" -->
<ul>
<li>Without top-K, you pay the training cost of all N experts per token</li>
</ul>
<!-- .element: class="fragment" -->
<ul>
<li><strong>The whole point of MoE is sparse activation</strong> during both training and inference</li>
</ul>
<!-- .element: class="fragment" -->

<p><img alt="alt text" src="../images/dense-moe.png" /></p>
<hr />
<h2 id="router-collapse">Router Collapse</h2>
<h4 id="the-problem">The Problem</h4>
<ul>
<li>
<p>Gating network routes tokens to only a small subset of experts</p>
</li>
<li>
<p>Leaves most experts <strong>underutilized or completely inactive</strong></p>
</li>
<li>
<p><strong>The "Rich-get-richer" effect</strong>: Specialized experts attract more tokens $\rightarrow$ get more gradients $\rightarrow$ become better $\rightarrow$ attract even more tokens</p>
</li>
<li>
<p><strong>Degenerate policy</strong>: Router learns $s_{i,t} \approx 0$ for most experts</p>
</li>
</ul>
<p>$$
\text{Effective capacity} \ll N \times \text{expert size}
$$</p>
<h4 id="consequences">Consequences</h4>
<ul>
<li><strong>Wasted memory</strong>: Unused experts consume GPU memory but add no value</li>
<li><strong>Reduced model quality</strong>: Equivalent to training a much smaller dense model</li>
<li><strong>Poor generalization</strong>: Active experts become overloaded and overfit</li>
</ul>
<hr />
<h2 id="mitigating-router-collapse">Mitigating Router Collapse</h2>
<h4 id="strategies">Strategies</h4>
<ul>
<li>
<p><strong>Auxiliary Balancing Loss</strong>: Penalize uneven distribution (e.g., $F \cdot P$ loss)</p>
</li>
<li>
<p><strong>Expert Capacity</strong>: Cap the tokens per expert per batch to force overflow</p>
</li>
<li>
<p><strong>Random Noise</strong>: Add noise to router logits to encourage exploration</p>
</li>
<li>
<p><strong>Expert Choice</strong>: Experts pick tokens (rather than tokens picking experts)</p>
</li>
</ul>
<hr />
<h2 id="expert-configuration">Expert Configuration</h2>
<p><strong>Fine-Grained Experts</strong>
- Make experts smaller and use more of them.
- Instead of $N$ full-sized FFN copies, make each expert <strong>much smaller</strong> (1/4 to 1/14 of standard FFN size)</p>
<p><em>The fine-grained ratio = (expert intermediate dim) / (standard FFN intermediate dim)</em></p>
<p><img alt="alt text" src="../images/moe-experts.png" /></p>
<hr />
<h2 id="expert-configuration_1">Expert Configuration</h2>
<p><strong>Shared Experts</strong>
-  One or more experts that process <strong>all tokens</strong> regardless of routing
- <em>Shared experts</em> provide a common processing pathway for all tokens, ensuring that every token benefits from some shared knowledge.
- <em>Routed experts</em> can specialize on different subsets of tokens. This hybrid approach can improve performance and stability.</p>
<p><img alt="alt text" src="../images/moe-shared.png" /></p>
<hr />
<h2 id="expert-configuration_2">Expert Configuration</h2>
<p><img alt="alt text" src="../images/moe-ablations.png" /></p>
<hr />
<h3 id="expert-routing-configurations-for-major-moes">Expert Routing Configurations for Major MoEs</h3>
<table>
<thead>
<tr>
<th>Model</th>
<th>Routed</th>
<th>Active</th>
<th>Shared</th>
<th>Fine-grained ratio</th>
</tr>
</thead>
<tbody>
<tr>
<td>GShard</td>
<td>2048</td>
<td>2</td>
<td>0</td>
<td>--</td>
</tr>
<tr>
<td>Switch Transformer</td>
<td>64</td>
<td>1</td>
<td>0</td>
<td>--</td>
</tr>
<tr>
<td>Mixtral</td>
<td>8</td>
<td>2</td>
<td>0</td>
<td>--</td>
</tr>
<tr>
<td>DBRX</td>
<td>16</td>
<td>4</td>
<td>0</td>
<td>--</td>
</tr>
<tr>
<td>Grok</td>
<td>8</td>
<td>2</td>
<td>0</td>
<td>--</td>
</tr>
<tr>
<td>DeepSeek V1</td>
<td>64</td>
<td>6</td>
<td>2</td>
<td>1/4</td>
</tr>
<tr>
<td>Qwen 1.5</td>
<td>60</td>
<td>4</td>
<td>4</td>
<td>1/8</td>
</tr>
<tr>
<td>DeepSeek V3</td>
<td>256</td>
<td>8</td>
<td>1</td>
<td>1/14</td>
</tr>
<tr>
<td>OLMoE</td>
<td>64</td>
<td>8</td>
<td>0</td>
<td>1/8</td>
</tr>
<tr>
<td>MiniMax</td>
<td>32</td>
<td>2</td>
<td>0</td>
<td>~1/4</td>
</tr>
<tr>
<td>Llama 4 (Maverick)</td>
<td>128</td>
<td>1</td>
<td>1</td>
<td>1/2</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="training-moes">Training MoEs</h2>
<p><strong>The Core Challenge</strong></p>
<ul>
<li><strong>Sparsity vs. Differentiability</strong>: We need sparsity for training-time efficiency.</li>
<li><strong>Problem</strong>: Sparse gating decisions (hard top-K selection) are <strong>not differentiable</strong>.</li>
<li>Gradient descent cannot directly optimize the discrete "choice" of an expert.</li>
</ul>
<p><strong>The Efficiency Trade-off</strong></p>
<ul>
<li>Activating all experts simplifies gradients but destroys compute efficiency.</li>
<li><strong>FLOPs Cost</strong>: "Having a model that's 256 times more expensive to train is a total no-go."</li>
<li><strong>Goal</strong>: Maintain sparse execution while ensuring the routing mechanism can still be trained effectively.</li>
</ul>
<hr />
<h2 id="heuristic-balancing-losses">Heuristic Balancing Losses</h2>
<p><strong>Switch Transformer F*P Loss (Standard)</strong></p>
<p><img alt="alt text" src="../images/fp-loss.png" /></p>
<p>where:
- $f_i = \frac{1}{T} \sum_{x \in \mathcal{B}} \mathbb{1}{\text{argmax } p(x) = i}$ is the fraction of tokens dispatched to expert $i$
- $P_i = \frac{1}{T} \sum_{x \in \mathcal{B}} p_i(x)$ is the mean router probability for expert $i$
- $\alpha$ is the balancing coefficient
- $N$ is the number of experts</p>
<hr />
<h2 id="expert-capacity">Expert Capacity</h2>
<h4 id="hardware-efficiency-vs-dynamic-routing">Hardware Efficiency vs. Dynamic Routing</h4>
<p><strong>Expert Capacity</strong>: The maximum number of tokens assigned to each expert per batch.</p>
<p>$$ \text{Expert Capacity} = \frac{\text{Tokens per Batch}}{\text{Number of Experts}} \times \text{Capacity Factor} $$</p>
<p><img alt text="expert capacity" src="images/expert-capacity.png" class="float-right"/></p>
<h4 id="handling-overflow-dropped-tokens">Handling Overflow (Dropped Tokens)</h4>
<ul>
<li><strong>Token Dropping</strong>: Occurs when the number of tokens routed to an expert exceeds its capacity.</li>
<li><strong>Residual Bypass</strong>: Dropped tokens skip expert computation and flow directly through the residual connection to the next layer.</li>
</ul>
<hr />
<h2 id="computing-output-of-moe-layer">Computing output of MoE layer</h2>
<ul>
<li>
<p>For each selected expert, compute the output for the assigned tokens</p>
</li>
<li>
<p>Combine the outputs from the selected experts (e.g., weighted sum) and add the residual connection</p>
</li>
</ul>
<p><img alt="alt text" src="../images/moe-output.png" /></p>
<p><a href="https://github.com/cat-state/modded-nanogpt-moe/blob/main/train_gpt_moe.py#L215">Example MoE Training code</a></p>
<hr />
<h2 id="total-vs-activate-parameters">Total vs Activate Parameters</h2>
<ul>
<li><strong>Total Parameters</strong>: The entire set of model parameters, including all available experts (both active and inactive).</li>
<li><strong>Active Parameters</strong>: The parameters actually triggered to process a single input token. This determines the compute cost (FLOPs) per token.</li>
<li><strong>Key Advantage</strong>: MoEs scale model capacity (Total) while maintaining the inference latency and training cost of a much smaller dense model (Active).</li>
</ul>
<p><img alt="alt text" src="../images/active-1.png" /></p>
<hr />
<h2 id="total-vs-activate-parameters_1">Total vs Activate Parameters</h2>
<p><strong>Mixtral 8x7B MoE</strong></p>
<p><img alt text="mixtral" src="images/active-2.png" class="float-right"/></p>
<ul>
<li><strong>Total Parameters</strong>: ~46.7B</li>
<li><strong>Active Parameters</strong>: ~12.9B </li>
<li><strong>Configuration</strong>:<ul>
<li>8 experts per layer</li>
<li>Top-2 gating ($K=2$ experts active per token)</li>
</ul>
</li>
<li><strong>Parameter Sharing</strong>:<ul>
<li>Only the Feed-Forward Network (FFN) blocks are replicated.</li>
<li><strong>Shared components</strong>: Attention layers, LayerNorms, and Embeddings.</li>
</ul>
</li>
</ul>
<p>$$ \text{Active Params} = \text{Shared} + (K \times \text{Expert FFN}) $$
$$ \text{Total Params} = \text{Shared} + (N \times \text{Expert FFN}) $$</p>
<p><img alt text="mixtral-params" src="images/active-3.png" class="float-right"/></p>
<hr />
<h2 id="deepseek-v3-moe">DeepSeek V3 MoE</h2>
<p><strong>DeepSeek V2</strong> </p>
<ul>
<li>a 236 billion parameter MoE with 21 billion active parameters</li>
</ul>
<p><img alt="alt text" src="../images/deepseek-v2.png" /></p>
<p><strong>DeepSeek V3</strong>
- a 671B parameter MoE with 37B active parameters</p>
<hr />
<h2 id="deepseek-v3-architecture">DeepSeek V3 Architecture</h2>
<p><img alt text="deepseek-v3-params" src="images/deepseek-v3.png" class="float-right"/></p>
<ul>
<li><strong>Sparse Activation</strong>: 671B total parameters, only 37B active per token</li>
<li><strong>Shared + Routed Experts</strong>: Combines fine-grained routed experts (256) with 1 shared expert, 8 activated experts processing all tokens</li>
<li><strong>Multi-Head Latent Attention (MLA)</strong>: Compresses KV cache into latent space, synergizes with sparse MoE for efficient inference</li>
<li><strong>Multi-Token Prediction</strong>: Jointly optimizes predicting multiple tokens in parallel for improved coherence and throughput</li>
</ul>
<hr />
<div class="center-slide">

## Questions? 
</div>









  




                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "navigation.sections", "navigation.expand", "navigation.top", "search.highlight", "content.code.copy"], "search": "../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
    
  </body>
</html>