{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Large Language Models : A Hands on Approach","text":"<p>Jan - May, 2026 @ Center for Continuing Education, Indian Institute of Science</p>"},{"location":"#logistics","title":"Logistics","text":"<ul> <li>Duration: 18 weeks (Jan - May 2026)</li> <li>Format: Online, Tue and Thu, 7:30 - 9:00 PM IST</li> <li>Contact: TBA</li> </ul>"},{"location":"#registration","title":"Registration","text":"<p>To register please visit CCE Webpage for the course.</p>"},{"location":"#course-description","title":"Course Description","text":"<p>LLMs have become mainstay of NLP and are transforming every domain, from software development, research, and business intelligence to education. However, deploying them efficiently remains a specialized engineering challenge.</p> <p>This course provides an engineering-focused exploration of Large Language Models (LLMs). Participants will go from understanding transformer architectures and GPU internals to mastering fine-tuning, inference optimization, and large-scale deployment across GPUs, clusters, and edge devices. Through a theory-to-practice approach, including case studies, hands-on labs, and projects, learners will cover key topics such as model architecture, fine-tuning techniques, inference optimization, serving strategies, and applications in retrieval-augmented generation (RAG) and agentic systems.</p>"},{"location":"#learning-outcomes","title":"Learning Outcomes","text":"<p>By the end of this course, participants will be able to:</p> <ul> <li>Understand LLM Architecture: Master transformer architectures, attention mechanisms, and modern LLM variants (GPT-OSS, Qwen, Gemma, etc.).</li> <li>Optimize Inference: Implement efficient inference strategies including quantization, KV caching, and serving via inference engines like vLLM.</li> <li>Fine-tune Models: Apply various fine-tuning techniques including LoRA, QLoRA, instruction tuning, and preference alignment.</li> <li>Build RAG Systems: Design and implement Retrieval-Augmented Generation pipelines.</li> <li>Develop AI Agents: Create tool-using agents with the ReAct framework.</li> <li>Deploy at Scale: Set up production-ready LLM serving infrastructure with cost optimization.</li> <li>Multimodal Models: Work with vision-language models and speech.</li> <li>Evaluation: Understand evaluation strategies for LLMs and RAG systems.</li> </ul>"},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Proficiency in Python and familiarity with any deep learning framework (PyTorch preferred).  </li> <li>Basic understanding of neural networks.  </li> <li>Working knowledge of Linux, Docker, and Git.  </li> <li>Optional but recommended: experience with GPU computing.</li> </ul>"},{"location":"#course-navigation","title":"Course Navigation","text":"<ul> <li> <p>Weekly Schedule</p> </li> <li> <p>Assignments and Labs</p> </li> </ul>"},{"location":"assignments/","title":"Assignments and Labs","text":"<p>TBD</p>"},{"location":"course-details/","title":"Course Details","text":""},{"location":"course-details/#course-overview","title":"Course Overview","text":"<p>This is a comprehensive hands-on course on Large Language Models (LLMs) designed specifically for industry professionals. The course covers the complete spectrum of LLM engineering, from foundational concepts to advanced deployment and optimization strategies.</p>"},{"location":"course-details/#course-objectives","title":"Course Objectives","text":"<p>By the end of this course, participants will be able to:</p> <ul> <li>Understand LLM Architecture: Master transformer architectures, attention mechanisms, and modern LLM variants (GPT, BERT, Mixtral, etc.)</li> <li>Optimize Inference: Implement efficient inference strategies including quantization, KV caching, and multi-GPU serving</li> <li>Fine-tune Models: Apply various fine-tuning techniques including LoRA, QLoRA, instruction tuning, and preference alignment</li> <li>Build RAG Systems: Design and implement Retrieval-Augmented Generation pipelines with vector databases and evaluation</li> <li>Develop AI Agents: Create tool-using agents with ReAct frameworks and multi-agent orchestration</li> <li>Deploy at Scale: Set up production-ready LLM serving infrastructure with cost optimization</li> <li>Handle Multimodal AI: Work with vision-language models and speech integration</li> <li>Ensure Security: Implement security measures against prompt injection and other LLM-specific threats</li> </ul>"},{"location":"course-details/#course-structure","title":"Course Structure","text":"<p>The course is organized into 18 weeks covering 8 major themes:</p>"},{"location":"course-details/#1-llm-foundations-weeks-1-3","title":"1. LLM Foundations (Weeks 1-3)","text":"<ul> <li>Transformer architecture deep dive</li> <li>Tokenization and pretraining objectives</li> <li>Modern architectures (GPT, Qwen, Gemma)</li> <li>Scaling laws and emergent properties</li> </ul>"},{"location":"course-details/#2-gpu-infrastructure-week-4","title":"2. GPU &amp; Infrastructure (Week 4)","text":"<ul> <li>GPU architecture and CUDA programming</li> <li>Multi-GPU and multi-node parallelism</li> <li>Hardware selection and cluster building</li> </ul>"},{"location":"course-details/#3-inference-optimization-weeks-5-7","title":"3. Inference Optimization (Weeks 5-7)","text":"<ul> <li>Inference bottlenecks and profiling</li> <li>Quantization techniques (INT8, INT4, GPTQ, AWQ)</li> <li>Inference engines (vLLM, TensorRT-LLM)</li> <li>Multi-GPU serving strategies</li> </ul>"},{"location":"course-details/#4-fine-tuning-weeks-8-10","title":"4. Fine-tuning (Weeks 8-10)","text":"<ul> <li>Parameter-efficient fine-tuning (LoRA, QLoRA)</li> <li>Instruction tuning and data curation</li> <li>Preference alignment (RLHF, DPO)</li> <li>Reasoning and chain-of-thought training</li> </ul>"},{"location":"course-details/#5-rag-systems-week-11","title":"5. RAG Systems (Week 11)","text":"<ul> <li>Vector databases and hybrid search</li> <li>RAG evaluation and optimization</li> <li>Graph RAG and advanced retrieval</li> </ul>"},{"location":"course-details/#6-ai-agents-weeks-12-13","title":"6. AI Agents (Weeks 12-13)","text":"<ul> <li>ReAct framework and tool calling</li> <li>Agent fine-tuning and evaluation</li> <li>Multi-agent orchestration</li> </ul>"},{"location":"course-details/#7-advanced-topics-weeks-14-17","title":"7. Advanced Topics (Weeks 14-17)","text":"<ul> <li>Model evaluation and monitoring</li> <li>Multimodal models (vision, audio)</li> <li>Edge deployment and tiny models</li> <li>Security and privacy engineering</li> <li>Emerging architectures (Mamba, hybrid models)</li> </ul>"},{"location":"course-details/#8-student-presentations-week-18","title":"8. Student Presentations (Week 18)","text":"<ul> <li>Project showcases and peer learning</li> </ul>"},{"location":"course-details/#course-timings","title":"Course Timings","text":"<ul> <li>Duration: 18 weeks</li> <li>Format: Weekly sessions with hands-on labs</li> <li>Target Audience: Industry professionals and advanced practitioners</li> <li>Prerequisites: Python programming, basic ML knowledge</li> <li>Level: Advanced/Professional</li> </ul>"},{"location":"course-details/#hands-on-approach","title":"Hands-on Approach","text":"<p>Each week includes:</p> <ul> <li>Theory Sessions: Core concepts and architecture deep dives</li> <li>Practical Labs: Implementation exercises and code walkthroughs</li> <li>Case Studies: Real-world examples and industry applications</li> <li>Projects: Progressive building of LLM applications</li> </ul>"},{"location":"course-details/#key-tools-technologies","title":"Key Tools &amp; Technologies","text":"<ul> <li>Frameworks: PyTorch, Hugging Face Transformers, vLLM</li> <li>Infrastructure: CUDA, NCCL, DeepSpeed, FSDP</li> <li>Deployment: Docker, Kubernetes, cloud platforms</li> <li>Monitoring: Prometheus, Grafana, PyTorch Profiler</li> <li>Development: Python, Jupyter, Git, MkDocs</li> </ul>"},{"location":"course-details/#assessment","title":"Assessment","text":"<ul> <li>Weekly hands-on assignments (60%)</li> <li>Mid-term project (20%)</li> <li>Final project presentation (20%)</li> </ul>"},{"location":"course-details/#prerequisites","title":"Prerequisites","text":"<ul> <li>Programming: Proficient in Python</li> <li>Machine Learning: Understanding of neural networks, backpropagation</li> <li>Mathematics: Linear algebra, calculus, statistics</li> <li>Hardware: Basic understanding of GPU computing (preferred)</li> <li>Tools: Familiarity with Git, command line, Jupyter notebooks</li> </ul>"},{"location":"schedule/","title":"Course Schedule","text":""},{"location":"schedule/#weekwise-schedule","title":"Weekwise Schedule","text":"<p>Tentative and subject to change</p> Theme Week Topics LLM Foundations I 1.1 Orientation, Transformer architecture 1.2 Transformer Architecture - GPT 1 and 2 2.1 Tokenization, Pretraining objectives 2.2 Mixture of Experts LLM Foundation II 3.1 Case studies: State-of-the-art open-source LLM architectures 3.2 Scaling Laws, Emergent properties GPU Basics 4.1 GPU architecture deep dive 4.2 Parallelism: Multi GPU, Multi Node 5.1 On-Prem Hardware Stack Deep Dive Inference 5.2 Inference Strategies 6.1 Inference Math and Bottlenecks 6.2 Efficient Attention &amp; KV Caching Efficient Inference &amp; Quantization 7.1 Quantization Fundamentals 7.2 Inference Engines and Multi GPU Fine-Tuning Fundamentals 8.1 Full Fine-Tuning vs. PEFT \u2014 When to Use Each 8.2 Instruction Tuning 9.1 Alignment (RLHF, DPO etc) 9.2 More RL Reasoning 10.1 Reasoning &amp; Chain-of-Thought 10.2 CoT, Tree-of-Thought, Self-Consistency \u2014 Prompt Engineering as Code RAG 11.1 RAG Fundamentals - Context-engineering, embeddings, search and rerankers 11.2 Evaluating RAG Agents 12.1 ReAct Framework: Thought \u2192 Action \u2192 Observation Tool Use &amp; Function Calling 12.2 MCP introduction 12.3 Agentic RAG, Multi Agent Orchestration, Multimodal Agents Agent Finetuning 13.1 Fine Tuning for Tool calling 13.2 Agent Evaluation &amp; Safety Evaluation 14.1 Evaluation 14.2 Observability &amp; Monitoring Multimodal Models 15.1 Multi Modal Architecture: Image, Audio and Video models, Running Locally 15.2 Fine tuning multimodal models LLMs on the Edge 16.1 Edge-Optimized LLM Architectures, case studies 16.2 Edge Optimization techniques Security &amp; Privacy Engineering 17.1 Threat Model: Prompt Injection, Jailbreaking, Data Leakage Frontiers 17.2 Emerging Topics: Mamba, Qwen Next, Hybrid architectures Presentations 18.1 Student Presentations I 18.2 Student Presentations II"},{"location":"schedule/#materials","title":"Materials","text":"<ul> <li>Lecture slides and notes will be shared here as class progresses.</li> </ul>"},{"location":"slides/01_orientation/00-intro/","title":"00 intro","text":""},{"location":"slides/01_orientation/00-intro/#large-language-models-a-hands-on-approach","title":"Large Language Models : A Hands on Approach","text":""},{"location":"slides/01_orientation/00-intro/#course-orientation","title":"Course Orientation","text":""},{"location":"slides/01_orientation/00-intro/#course-overview","title":"Course Overview","text":"<ul> <li>Duration: ~18 Weeks</li> <li>Format: 2 sessions per week (Tue &amp; Thu, 7:00\u20138:30 PM IST)</li> <li>Style: Lectures, demos and labs</li> <li>Assessment: Quizzes, assignments, and final project/presentation</li> <li>Credits: 3 + 1 (Credits added in ABC)</li> </ul>"},{"location":"slides/01_orientation/00-intro/#why-this-course","title":"Why This Course?","text":"<p>LLMs have changed how we build intelligent applications.</p> <p>This course aims to bridge the gap between:</p> <p>Using LLMs via APIs \u2192 Engineering LLM systems</p>"},{"location":"slides/01_orientation/00-intro/#learning-outcomes","title":"Learning Outcomes","text":"<ul> <li>Understand LLM architectures</li> <li>Fine-tune models for specific domains and tasks</li> <li>Optimize inference for cost and latency</li> <li>Build robust LLM-powered applications (RAG, agents, tool use)</li> <li>Evaluate, debug, and improve model performance</li> <li>Leverage open-source ecosystems for LLM engineering</li> </ul>"},{"location":"slides/01_orientation/00-intro/#what-this-course-will-not-cover","title":"What This Course Will NOT Cover","text":"<ul> <li>Mathematical foundations</li> <li>Pretraining LLMs from scratch</li> <li>State of the art in research</li> <li>Data engineering pipelines</li> <li>Ethics and societal impacts</li> </ul>"},{"location":"slides/01_orientation/00-intro/#prerequisites","title":"Prerequisites","text":"<ul> <li>Programming: Python proficiency</li> <li>ML Basics: Understanding of neural networks and deep learning</li> <li>Helpful: Familiarity with PyTorch, basic NLP concepts</li> </ul>"},{"location":"slides/01_orientation/00-intro/#tools-and-resources","title":"Tools and Resources","text":""},{"location":"slides/01_orientation/00-intro/#software-stack","title":"Software Stack","text":"<ul> <li>Python</li> <li>PyTorch</li> <li>HuggingFace Ecosystem(models, libraries and datasets)</li> </ul>"},{"location":"slides/01_orientation/00-intro/#compute-stack","title":"Compute Stack","text":"Option Cost Notes Google Colab Free Limited GPU, good for experiments GCP Free Credits Free $300 for new accounts Colab Pro+ Paid More GPU time, better GPUs RunPod Paid Flexible, longer runs Lightning AI Paid Easy setup, good UX <p>Free options suffice for most of our course needs</p>"},{"location":"slides/01_orientation/00-intro/#evaluation-criteria","title":"Evaluation Criteria","text":"Component Weight Notes Quizzes 25% 5 short in-class quizzes Assignments 30% 2 assignments (Mid Feb and Mid Mar) Final Project/Presentation 40% Mar 3rd week onwards Participation 5% In-class and forum engagement <p>--</p>"},{"location":"slides/01_orientation/00-intro/#abc","title":"ABC","text":"<ul> <li>Academic Bank of Credits</li> <li>Credits will be added to your ABC account after successful course completion</li> <li>More information will be shared during the course</li> </ul>"},{"location":"slides/01_orientation/00-intro/#final-project-option","title":"Final Project Option","text":"<p>Individual or group project (max 2).</p> <p>Build an end-to-end LLM application:</p> <ul> <li>Model fine-tuning</li> <li>Inference optimization</li> <li>LLM Application (RAG, Agents)</li> <li>Open source contribution</li> <li>Other ideas welcome too</li> </ul> <p>Deliverables: Documentation + Demo / Presentation</p>"},{"location":"slides/01_orientation/00-intro/#presentation-option","title":"Presentation Option","text":"<ul> <li>Topic: Novel concept not covered in class</li> <li>Format: Research paper or case study</li> <li>Deliverables: Slides or summary document</li> <li>Evaluation: Depth of understanding, presentation quality</li> <li>Duration: 10 minutes, 8-10 slides</li> </ul>"},{"location":"slides/01_orientation/00-intro/#course-roadmap-12","title":"Course Roadmap (1/2)","text":"Module Topic 1 LLM Foundations - Transformers, GPT-2, Modern Architectures, MoE, OSS Models 2 GPUs - Architecture and Programming,  Multi-GPU Parallelism, Hardware Stack 3 Inference - Sampling, KV Caching, Quantization, Speculative Decoding, Model Serving 4 Fine-Tuning - SFT, PEFT (LoRA, QLoRA etc),RLHF (DPO, GRPO etc), Distillation 5 Reasoning - Chain-of-Thought (CoT), Test Time Scaling, Finetuning"},{"location":"slides/01_orientation/00-intro/#course-roadmap-22","title":"Course Roadmap (2/2)","text":"Module Topic 6 RAG &amp; Agents - RAG Fundamentals, ReAct, Tools, Protocols, Agents, Finetuning 7 Evaluation - Moderl Evaluations, Benchmarks, LLM-as-a-Judge 8 Multimodal - Multimodal Architectures, Finetuning 9 Edge Deployment - Edge architectures, Optimization and Deployment"},{"location":"slides/01_orientation/00-intro/#making-the-best-of-this-course","title":"Making the Best of This Course","text":"<ul> <li>Participate actively: Ask questions in class, engage in discussions</li> <li>Practice hands-on: Run the code, experiment with parameters</li> <li>Use AI tools: Leverage AI assistants for coding and learning</li> <li>Stay connected: Use forum for async discussions</li> </ul>"},{"location":"slides/01_orientation/00-intro/#qa","title":"Q&amp;A","text":""},{"location":"slides/01_orientation/00-intro/#any-questions","title":"Any questions?","text":""},{"location":"slides/01_orientation/notes/","title":"Notes","text":""},{"location":"slides/01_orientation/notes/#course-overview-and-structure","title":"Course Overview and Structure","text":"<ul> <li>Duration: ~18 Weeks</li> <li>Format: 2 sessions per week, Tuesday and Thursday (7:00 PM to 8:30 PM IST)</li> <li>Style: Mix of lectures, demos, labs, and projects</li> <li>Assessment: Assignments, quizzes, and final project</li> </ul>"},{"location":"slides/01_orientation/notes/#why-this-course","title":"Why this course?","text":"<p>LLMs have changed how we build intelligent applications. This course bridges the gap between using LLMs via APIs and engineering LLM systems you can deploy and operate in production.</p>"},{"location":"slides/01_orientation/notes/#learning-outcomes","title":"Learning Outcomes","text":"<p>You will learn how to: - Understand LLM architectures - Fine-tune models for specific domains and tasks - Optimize inference for cost and latency - Build robust LLM-powered applications (RAG, agents, tool use) - Evaluate, debug, and improve model performance systematically - Make use open-source ecosystems for LLM engineering</p>"},{"location":"slides/01_orientation/notes/#what-this-course-will-not-cover","title":"What This Course Will NOT Cover","text":"<ul> <li>Mathematical foundations</li> <li>Pretraining LLMs from scratch </li> <li>State of the art in research</li> <li>Ethics and societal impacts</li> <li>Data engineering pipelines</li> </ul>"},{"location":"slides/01_orientation/notes/#prerequisites","title":"Prerequisites","text":"<ul> <li>Programming: Python proficiency</li> <li>ML Basics: Understanding of neural networks and deep learning concepts</li> <li>Helpful: Familiarity with PyTorch, basic NLP concepts</li> </ul>"},{"location":"slides/01_orientation/notes/#tools-and-resources","title":"Tools and Resources","text":""},{"location":"slides/01_orientation/notes/#software","title":"Software","text":"<ul> <li>Python</li> <li>PyTorch</li> <li>HuggingFace (transformers, datasets, peft, trl)</li> </ul>"},{"location":"slides/01_orientation/notes/#hardware-compute","title":"Hardware / Compute","text":"Option Cost Notes Google Colab Free Limited GPU time, good for experiments GCP Free Credits Free $300 credits for new accounts Colab Pro+ Paid More GPU time, better GPUs RunPod Paid Flexible, good for longer runs Lightning AI Paid Easy setup, good UX <ul> <li>*first two will suffice for most assignments and labs, for final projects you might need paid options depending on your project scope</li> </ul>"},{"location":"slides/01_orientation/notes/#evaluation-criteria","title":"Evaluation Criteria","text":"Component Weight Description Quizzes 20% 4\u20135 short in-class quizzes Assignments 30% 2 assignments covering key modules Final Project or Presentation 50% Project : Build and deploy an end-to-end LLM application (agents or RAG/systems), or improve an existing approach (e.g., inference optimization or fine-tuning). Presentation : Cover a novel concept not covered in the class (rsearch paper or case study)"},{"location":"slides/01_orientation/notes/#final-project-or-presentation-details","title":"Final Project or Presentation Details","text":"<p>Project Option: - Build an end-to-end LLM application - Options: Fine-tuned model, RAG system, agent application - Deliverables: documentation, demo and  - Evaluation: Functionality, </p> <p>Presentation Option: - Topic: Novel concept not covered in class - Deliverables: Slides or summary document - Evaluation: Depth of understanding, Presentation quality</p>"},{"location":"slides/01_orientation/notes/#grading-policy","title":"Grading Policy","text":"<p>TODO (@yknegi): Add grading policy details ABC </p>"},{"location":"slides/01_orientation/notes/#course-roadmap","title":"Course Roadmap","text":"<p>TODO (@yknegi): remove course roadmap if not needed | Module | Topic | Key Concepts | |:---|:---|:---| | 1 | LLM Foundations | Transformers, GPT-2 style, Modern Architectures, Mixture of Experts, Open-Source LLMs | | 2 | GPUs | GPU Architecture, Multi-GPU Parallelism, Hardware Stack | | 3 | Optimizing Inference | Sampling, Memory Optimization (KV Caching), Quantization, Inference Engines (vLLM, SGLang) | | 4 | Fine-Tuning | Full Fine-Tuning, PEFT (LoRA, QLoRA), Instruction Tuning, Alignment (DPO, RLHF), Distillation, Reasoning | | 5 | RAG and Agents | RAG Fundamentals, Agentic RAG, ReAct, Tools, MCP, Multi-Agent Systems | | 6 | Evaluation | Frameworks, MMLU, LMSYS Arena, LLM-as-a-Judge, Error Analysis | | 7 | Multimodal | Multimodal Architectures, Visual Instruction Fine-tuning | | 8 | Edge Deployment | Edge Architectures, llama.cpp, Optimization, and Deployment |</p>"},{"location":"slides/01_orientation/notes/#making-the-best-of-this-course","title":"Making the Best of This Course","text":"<ul> <li>Participate actively: Ask questions, engage in discussions</li> <li>Practice hands-on: Run the code, experiment with parameters</li> <li>Stay connected: Use forum for async discussions</li> </ul>"},{"location":"slides/01_orientation/notes/#qa","title":"Q&amp;A","text":"<p>Any questions?</p>"},{"location":"slides/02_llm_basics/notes/","title":"Large Language Models (LLMs) - Introduction","text":""},{"location":"slides/02_llm_basics/notes/#what-is-a-large-language-model","title":"What is a Large Language Model?","text":"<p>A Large Language Model is a deep neural network trained on extensive datasets containing text from books, articles, websites, and other sources sometimes encompassing large portions of the entire publicly available text on the internet.</p> <p>LLMs have remarkable capabilities in understanding, generating, and interpreting human language, code, and even multimodal data (text, images, audio).</p> <p></p> <p>A language model predicts the next word in a sequence given the preceding words. The models are trained to predict the next word in a sentence, given the preceding words. This training process allows them to learn grammar, facts about the world, reasoning abilities, and even some level of common sense.</p>"},{"location":"slides/02_llm_basics/notes/#mathematical-definition","title":"Mathematical Definition","text":"<p>Mathematically, a language model estimates the probability distribution over a sequence of tokens. The joint probability of a sequence $W = (w_1, w_2, \\dots, w_T)$ is factorized using the chain rule of probability:</p> <p>$$ P(w_1, w_2, \\dots, w_T) = \\prod_{t=1}^{T} P(w_t \\mid w_1, \\dots, w_{t-1}) $$</p> <p>The core objective is to learn the conditional probability of the next token $w_t$ given the history $w_{&lt;t}$:</p> <p>$$ P(w_t \\mid w_{&lt;t}) $$</p> <p>Where: - $w_t$ is the token to be predicted at step $t$. - $w_{&lt;t} = (w_1, \\dots, w_{t-1})$ is the context of preceding tokens. - $P(w_t \\mid w_{&lt;t})$ is the probability of the next word $w_t$ conditioned on the sequence of previous words.</p> <p> [Image source][1]</p> <p>The \"large\" aspect refers to the model's size, which is typically measured in terms of: - Parameter count: Typically &gt;1B parameters (e.g., GPT-3: 175B, Llama 3: 8B-405B) [2] - Training data: Trillions of tokens (e.g., Llama 3 trained on 15T tokens) [3] - Compute: Thousands of GPU-hours for training</p> <p>These models can have billions or even trillions of parameters, enabling them to capture complex patterns in language.</p> <p></p>"},{"location":"slides/02_llm_basics/notes/#some-applications-of-large-language-models","title":"Some Applications of Large Language Models","text":"<ol> <li> <p>Chatbots and Virtual Assistants: LLMs power conversational agents that can understand and respond to user queries in a human-like manner.</p> </li> <li> <p>Natural Language Processing (NLP): LLMs are widely used in NLP tasks such as sentiment analysis, named entity recognition, and text classification.</p> </li> <li> <p>Content Generation: LLMs can generate coherent and contextually relevant text, making them useful for content creation, summarization, and translation.</p> </li> <li> <p>Code Generation: LLMs can assist in generating code snippets, automating repetitive coding tasks, and even debugging code.</p> </li> <li> <p>Education and Tutoring: LLMs can provide personalized learning experiences, answer questions, and assist with homework.</p> </li> <li> <p>Creative Writing: LLMs can collaborate with authors by generating ideas, suggesting plot twists, and even writing poetry or stories.</p> </li> <li> <p>Research Assistance: LLMs can help researchers by summarizing papers, generating hypotheses, and even conducting literature reviews.</p> </li> </ol> <p>These applications demonstrate the versatility and potential of Large Language Models in transforming how we interact with technology and information.</p>"},{"location":"slides/02_llm_basics/notes/#why-build-your-own-llm","title":"Why build your own LLM?","text":"<ol> <li>Domain-specific models - can outperform general models like ChatGPT, Claude, etc.    e.g., Models trained for law, medical question answering, etc.</li> <li>Cost-effectiveness - cheaper to run your own LLM than to use cloud-based services.</li> <li>Data Privacy - you have control over the data used to train the model, prevent sensitive data being sent to model providers</li> <li>Customized Deployment - you can deploy the model on your own infrastructure or edge devices</li> <li>Autonomy - you can control the model's behavior, update it and fix it</li> </ol>"},{"location":"slides/02_llm_basics/notes/#building-and-using-llms","title":"Building and Using LLMs","text":"<p>Building and using LLMs involves several steps, including data collection, preprocessing, model selection, training, and evaluation.</p> <p></p> <p>1. Data Collection and Preprocessing: Data collection is the first step in building an LLM. It involves gathering a large and diverse dataset of text data from various sources, such as books, articles, websites, and social media platforms. Typically several terabytes or petabytes of data are required to train a large LLM.</p> <p>2. Pretraining: Pretraining involves training a large language model on a large dataset of text data. The model is trained to predict the next token in a sequence of tokens, given the preceding tokens. This training process allows the model to learn the patterns and relationships in the data, which can then be used for a variety of downstream tasks.</p> <p>3. Fine-tuning: Fine-tuning adapts a pretrained model for specific tasks and aligns it to human preferences.</p> <p>4. Inference: Inference involves using the model to generate text based on a given input. The model is used to generate text based on a given input, which can be used for a variety of downstream tasks.</p> <p>In our course, we'll not cover data collection and pretraining from scratch, but will focus on fine-tuning, inference optimization, and building applications using LLMs. Pre-training and data engineering require significant resources and are beyond the scope of this course. We shall use existing pretrained models from open-source repositories like HuggingFace and focus on engineering aspects of LLMs.</p>"},{"location":"slides/02_llm_basics/notes/#code-example-next-token-prediction-with-huggingface","title":"Code Example: Next-Token Prediction with HuggingFace","text":"<pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\ninputs = tokenizer(\"The capital of France is\", return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_new_tokens=5)\nprint(tokenizer.decode(outputs[0]))\n# Output: \"The capital of France is Paris, and the\"\n</code></pre>"},{"location":"slides/02_llm_basics/notes/#demo","title":"Demo","text":"<ul> <li>Text Generation</li> <li>Vision-Language Model</li> <li>Audio Generation</li> </ul>"},{"location":"slides/02_llm_basics/notes/#exercise","title":"Exercise","text":"<ul> <li>Login to Google Colab</li> <li>Run shared notebooks</li> <li>Visit HuggingFace, explore trending models, spaces and datasets. Use this HuggingFace Tutorial as reference.</li> </ul>"},{"location":"slides/02_llm_basics/notes/#references","title":"References","text":"<ol> <li>The Illustrated GPT-2 - Jay Alammar</li> <li>Language Models are Few-Shot Learners (GPT-3) - Brown et al., 2020</li> <li>Llama 3 Technical Report - Meta AI, 2024</li> <li>Hands-On Large Language Models by Jay Alammar, Maarten Grootendorst</li> </ol>"},{"location":"slides/02_llm_basics/slides/","title":"Slides","text":""},{"location":"slides/02_llm_basics/slides/#large-language-models-a-hands-on-approach","title":"Large Language Models : A Hands on Approach","text":""},{"location":"slides/02_llm_basics/slides/#llm-basics","title":"LLM Basics","text":""},{"location":"slides/02_llm_basics/slides/#what-is-a-large-language-model-llm","title":"What is a Large Language Model (LLM)?","text":"<p>A deep neural network trained on extensive text datasets from books, articles, websites\u2014sometimes encompassing large portions of the entire publicly available internet.</p> <p></p>"},{"location":"slides/02_llm_basics/slides/#llm-capabilities","title":"LLM Capabilities","text":"<ul> <li>Understanding and generating human language</li> <li>Code generation and debugging</li> <li>Multimodal data processing (text, images, audio)</li> <li>Reasoning and common sense understanding</li> </ul>"},{"location":"slides/02_llm_basics/slides/#language-modeling","title":"Language Modeling","text":"<p>A language model predicts the next word given preceding words.</p> <p></p> <p>[Image source][1]</p>"},{"location":"slides/02_llm_basics/slides/#mathematical-definition","title":"Mathematical Definition","text":"<p>Joint probability of a sequence using chain rule:</p> <p>$$ P(w_1, w_2, \\dots, w_T) = \\prod_{t=1}^{T} P(w_t \\mid w_1, \\dots, w_{t-1}) $$</p> <p>Core objective: learn the conditional probability:</p> <p>$$ P(w_t \\mid w_{&lt;t}) $$</p> <p>where - $w_t$ \u2014 token to be predicted at step $t$ - $w_{&lt;t} = (w_1, \\dots, w_{t-1})$ \u2014 context of preceding tokens - $P(w_t \\mid w_{&lt;t})$ \u2014 probability of next word conditioned on previous words</p>"},{"location":"slides/02_llm_basics/slides/#what-makes-llms-large","title":"What Makes LLMs \"Large\"?","text":"Aspect Scale Parameters &gt;1B (GPT-3: 175B, Llama 3: 8B-405B) [2] Training Data Trillions of tokens (Llama 3: 15T) [3] Compute Thousands of GPU-hours"},{"location":"slides/02_llm_basics/slides/#applications-of-llms","title":"Applications of LLMs","text":"<ol> <li>Chatbots &amp; Virtual Assistants - conversational agents</li> <li>NLP Tasks \u2014 sentiment analysis, NER, classification</li> <li>Content Generation \u2014 summarization, translation</li> <li>Code Generation \u2014 snippets, debugging, automation</li> <li>Education &amp; Tutoring \u2014 personalized learning</li> <li>Creative Writing \u2014 ideas, plot twists, poetry</li> <li>Research Assistance \u2014 paper summarization, literature reviews</li> </ol>"},{"location":"slides/02_llm_basics/slides/#why-build-your-own-llm","title":"Why Build Your Own LLM?","text":"<ol> <li>Domain-specific models \u2014 outperform general models (law, medical)</li> <li>Cost-effectiveness \u2014 cheaper than cloud APIs at scale</li> <li>Data Privacy \u2014 control over sensitive data</li> <li>Custom Deployment \u2014 on-premise or edge devices</li> <li>Autonomy \u2014 control behavior, updates, fixes</li> </ol>"},{"location":"slides/02_llm_basics/slides/#building-llms-overview","title":"Building LLMs: Overview","text":"<ol> <li>Data Collection and Preprocessing</li> <li>Pretraining </li> <li>Fine-tuning</li> <li>Inference</li> </ol>"},{"location":"slides/02_llm_basics/slides/#course-focus","title":"Course Focus","text":"<p>We will focus on: - Fine-tuning existing models - Inference optimization - Building applications with LLMs</p> <p>Using state of the art open source models available openly.</p>"},{"location":"slides/02_llm_basics/slides/#demo","title":"Demo","text":"<ul> <li>Text Generation</li> <li>Vision-Language Model</li> </ul>"},{"location":"slides/02_llm_basics/slides/#warm-up-exercise","title":"Warm Up Exercise","text":"<ol> <li>Login to Google Colab</li> <li>Run shared notebooks</li> <li>Explore HuggingFace:</li> <li>Trending models</li> <li>Spaces</li> <li>Datasets</li> </ol> <p>Reference: HuggingFace Tutorial</p>"},{"location":"slides/02_llm_basics/slides/#references","title":"References","text":"<ol> <li>The Illustrated GPT-2 - Jay Alammar</li> <li>GPT-3 Paper - Brown et al., 2020</li> <li>Llama 3 Report - Meta AI, 2024</li> </ol>"},{"location":"slides/02_llm_basics/slides/#thank-you","title":"Thank You","text":"<p>Questions?</p>"},{"location":"slides/03_tokenization/slides/","title":"Slides","text":""},{"location":"slides/03_tokenization/slides/#large-language-models-a-hands-on-approach","title":"Large Language Models: A Hands on Approach","text":""},{"location":"slides/03_tokenization/slides/#tokenization","title":"Tokenization","text":""},{"location":"slides/03_tokenization/slides/#topics-covered","title":"Topics Covered","text":"<ul> <li>Preprocessing text data for LLMs</li> <li>Tokenization techniques</li> <li>Byte Pair Encoding (BPE)</li> <li>Converting tokens into vectors</li> </ul>"},{"location":"slides/03_tokenization/slides/#models-of-the-week","title":"Models of the Week","text":""},{"location":"slides/03_tokenization/slides/#translategemma","title":"TranslateGemma","text":"<ul> <li>SOTA Open weights multilingual translation model</li> <li>Multimodal capabilities: text + images for context</li> <li>4B, 12B and 27B versions</li> </ul>"},{"location":"slides/03_tokenization/slides/#medgemma","title":"MedGemma","text":"<ul> <li>SOTA Open weights medical imaging and document understanding model</li> <li>CT/MRI/Histopathology Processing, Medical Document Understanding, Multi-domain Classification</li> </ul>"},{"location":"slides/03_tokenization/slides/#glm-47-flash","title":"GLM-4.7-Flash","text":"<ul> <li>Coding, Tool use and Reasoning abilities</li> <li>30B-A3B MoE model</li> </ul>"},{"location":"slides/03_tokenization/slides/#motivation","title":"Motivation","text":"<p>Why does tokenization matter?</p> <ul> <li>Cost: Billing is per token, not per word</li> <li>Context limits: Tokenization decides what fits vs what gets truncated</li> <li>Reasoning failures: Try \"Say Nameeee\" vs \"Say Name eee\" in DeepSeek</li> <li>Multilingual bias: Some languages need more tokens for same meaning</li> </ul>"},{"location":"slides/03_tokenization/slides/#data-preprocessing-pipeline","title":"Data Preprocessing Pipeline","text":"<ul> <li>Cannot feed raw text directly into LLMs. </li> <li>Need numerical representations.</li> </ul>"},{"location":"slides/03_tokenization/slides/#graph-lr-araw-text-btokenization-b-ctoken-ids-c-dembedding-layer-d-einput-embeddings-e-fllm","title":"<pre><code>graph LR\n    A[Raw Text] --&gt; B[Tokenization]\n    B --&gt; C[Token IDs]\n    C --&gt; D[Embedding Layer]\n    D --&gt; E[Input Embeddings]\n    E --&gt; F[LLM]\n</code></pre>","text":""},{"location":"slides/03_tokenization/slides/#the-full-pipeline","title":"The Full Pipeline","text":"<p>Unified Architecture</p> <p></p>"},{"location":"slides/03_tokenization/slides/#what-is-tokenization","title":"What Is Tokenization?","text":"<p>Tokenization breaks text into smaller units called tokens.</p> <p>Tokens can be words, subwords, or characters.</p> <p>Example: \"The transformer architecture is revolutionary.\"</p> Approach Tokens Count Words <code>[\"The\", \"transformer\", \"architecture\", \"is\", \"revolutionary\", \".\"]</code> 6 Characters <code>[\"T\", \"h\", \"e\", \" \", \"t\", \"r\", \"a\", ...]</code> 45 Subwords <code>[\"The\", \" transform\", \"er\", \" architecture\", \" is\", \" revolution\", \"ary\", \".\"]</code> 8"},{"location":"slides/03_tokenization/slides/#real-world-example-gpt-2-tokenizer","title":"Real World Example: GPT-2 Tokenizer","text":"<p>Try it yourself</p> <p></p>"},{"location":"slides/03_tokenization/slides/#tokenization-pipeline","title":"Tokenization Pipeline","text":""},{"location":"slides/03_tokenization/slides/#tokenization-spectrum","title":"Tokenization Spectrum","text":"<pre><code>Bytes \u2192 Characters \u2192 Subwords \u2192 Words\n  \u2191                              \u2191\n 256 tokens                     Millions of tokens\n Long sequences                 Short sequences\n</code></pre> <p>Trade-off between vocabulary size and sequence length.</p>"},{"location":"slides/03_tokenization/slides/#word-level-tokenization","title":"Word-Level Tokenization","text":"<p>Split text into words based on spaces and punctuation.</p> <pre><code>import re\ntext = \"Hello, world. This, is a test.\"\nresult = re.split(r'(\\s)', text)\n# ['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n</code></pre> <p></p>"},{"location":"slides/03_tokenization/slides/#word-level-pros-and-cons","title":"Word-Level: Pros and Cons","text":"<p>Advantages: - Short sequences (one token per word) - Linguistically intuitive - Fast attention (fewer tokens)</p> <p>Disadvantages: - Huge vocabulary (English needs 100K+ words) - OOV problem: Unknown words \u2192 <code>[UNK]</code> - Morphological blindness: \"run\", \"runs\", \"running\" are unrelated - Language-specific: Some languages don't use spaces</p>"},{"location":"slides/03_tokenization/slides/#character-level-tokenization","title":"Character-Level Tokenization","text":"<p>Split text into individual characters.</p> <p>Advantages: - No OOV tokens (any text can be encoded) - Tiny vocabulary (~100-300 tokens) - Handles typos and neologisms</p> <p>Disadvantages: - Very long sequences (5-10x longer) - Slow attention (more tokens) - Poor semantics (characters lack meaning) - Harder to learn word structure</p>"},{"location":"slides/03_tokenization/slides/#subword-tokenization","title":"Subword Tokenization","text":"<p>The sweet spot: split rare words, keep common words whole.</p> Word Subword Tokens Interpretation the <code>[\"the\"]</code> Common \u2192 single token transformer <code>[\"trans\", \"former\"]</code> Split into known pieces unhappiness <code>[\"un\", \"happi\", \"ness\"]</code> Morphemes preserved GPT-4 <code>[\"G\", \"PT\", \"-\", \"4\"]</code> Unknown \u2192 character fallback"},{"location":"slides/03_tokenization/slides/#popular-subword-algorithms","title":"Popular Subword Algorithms","text":"<ol> <li>Byte Pair Encoding (BPE) - Frequency-based merging</li> <li>WordPiece - Probability-based merging (BERT)</li> <li>SentencePiece - Unigram language model (LLaMA)</li> </ol>"},{"location":"slides/03_tokenization/slides/#byte-pair-encoding-bpe","title":"Byte Pair Encoding (BPE)","text":"<p>Sennrich et al. (2016)</p> <p>Core idea: Iteratively merge the most frequent pair of tokens.</p> <ol> <li>Start with vocabulary of individual characters (or bytes)</li> <li>Count all adjacent pairs in the corpus</li> <li>Merge the most frequent pair into a new token</li> <li>Repeat until reaching desired vocabulary size</li> </ol>"},{"location":"slides/03_tokenization/slides/#bpe-walkthrough","title":"BPE Walkthrough","text":"<p>Input: \"low lower lowest\"</p> <p>Step 1 - Initialize tokens: <pre><code>['l', 'o', 'w', ' ', 'l', 'o', 'w', 'e', 'r', ' ', 'l', 'o', 'w', 'e', 's', 't']\n</code></pre></p> <p>Step 2 - Count pairs: <pre><code>('l', 'o'): 3    ('o', 'w'): 3    ('w', ' '): 3\n(' ', 'l'): 2    ('e', 'r'): 1    ('e', 's'): 1\n</code></pre></p> <p>--</p>"},{"location":"slides/03_tokenization/slides/#bpe-walkthrough-continued","title":"BPE Walkthrough (continued)","text":"<p>Step 3 - Merge most frequent pair <code>('l', 'o')</code> \u2192 <code>'lo'</code> <pre><code>['lo', 'w', ' ', 'lo', 'w', 'e', 'r', ' ', 'lo', 'w', 'e', 's', 't']\n</code></pre></p> <p>Step 4 - Count pairs again: <pre><code>('lo', 'w'): 3    ('w', ' '): 3    (' ', 'lo'): 2\n</code></pre></p> <p>Step 5 - Merge <code>('lo', 'w')</code> \u2192 <code>'low'</code> <pre><code>['low', ' ', 'low', 'e', 'r', ' ', 'low', 'e', 's', 't']\n</code></pre></p> <p>--</p>"},{"location":"slides/03_tokenization/slides/#bpe-result","title":"BPE Result","text":"<p>Final tokenization: <pre><code>\"low lower lowest\" \u2192 [\"low\", \" \", \"low\", \"er\", \" \", \"low\", \"est\"]\n</code></pre></p> <p>Can encode any word: <pre><code>\"lowestness\" \u2192 [\"low\", \"est\", \"ness\"]\n</code></pre></p> <p></p>"},{"location":"slides/03_tokenization/slides/#bpe-pros-and-cons","title":"BPE: Pros and Cons","text":"<p>Advantages: - Balances vocabulary size and sequence length - Handles OOV words by breaking into subwords - Captures morphological structure</p> <p>Disadvantages: - Greedy algorithm (may not find optimal tokenization) - Training corpus dependent</p>"},{"location":"slides/03_tokenization/slides/#byte-level-bpe-gpt-2-style","title":"Byte-Level BPE (GPT-2 Style)","text":"<p>GPT-2 uses byte-level BPE:</p> <ul> <li>Start with 256 byte tokens (not Unicode characters)</li> <li>All text is UTF-8 encoded first</li> <li>Merges operate on bytes, not characters</li> <li>Avoids merges beyond word boundaries</li> </ul> <p>Advantages: - Handles any language without special tokenization - Works with emojis, rare scripts, binary data - No <code>[UNK]</code> tokens ever needed</p>"},{"location":"slides/03_tokenization/slides/#multimodal-tokenization","title":"Multimodal Tokenization","text":"<p>Modern models tokenize more than text:</p> <ul> <li>Text: Subword tokenization (BPE, WordPiece)</li> <li>Images: Patch-based (e.g., ViT) - split into 16x16 patches</li> <li>Video: Frame + patch tokenization</li> <li>Audio: Spectrogram frames</li> </ul> <pre><code>graph LR\n    T[Text Input] --&gt; TT[Text Tokenizer]\n    I[Image Input] --&gt; IT[Image Tokenizer]\n    A[Audio Input] --&gt; AT[Audio Tokenizer]\n    V[Video Input] --&gt; VT[Video Tokenizer]\n\n    TT --&gt; U[Unified Token Sequence]\n    IT --&gt; U\n    AT --&gt; U\n    VT --&gt; U\n\n    U --&gt; M[Transformer Model]\n</code></pre>"},{"location":"slides/03_tokenization/slides/#vocabulary-size-trade-offs","title":"Vocabulary Size Trade-offs","text":"<p>Smaller Vocabulary: - Faster training and inference - Lower memory usage - Longer sequences</p> <p>Larger Vocabulary: - Shorter sequences - Better semantic representation - Higher memory usage</p>"},{"location":"slides/03_tokenization/slides/#vocabulary-impact-on-parameters","title":"Vocabulary Impact on Parameters","text":"<p>$$ \\text{Embedding Matrix} = V \\times E $$</p> <p>$$ \\text{Output Layer} = H \\times V $$</p> <p>Where $V$ = vocabulary size, $E$ = embedding dimension, $H$ = hidden dimension</p>"},{"location":"slides/03_tokenization/slides/#model-vocabulary-comparison","title":"Model Vocabulary Comparison","text":"Model Vocabulary Size Tokenizer GPT-2 50,257 BPE GPT-4 ~100,000 BPE variant BERT 30,522 WordPiece LLaMA 32,000 SentencePiece Claude ~100,000 BPE variant"},{"location":"slides/03_tokenization/slides/#references","title":"References","text":"<ol> <li>Let's Build the GPT Tokenizer - Andrej Karpathy</li> <li>The Smol Training Playbook</li> <li>Neural Machine Translation of Rare Words with Subword Units - Sennrich et al., 2016</li> </ol>"},{"location":"slides/03_tokenization/slides/#thank-you","title":"Thank You","text":"<p>Questions?</p>"},{"location":"slides/04_transformer_basics/notes/","title":"Notes","text":""},{"location":"slides/04_transformer_basics/notes/#transformer-architecture-introduction","title":"Transformer Architecture : Introduction","text":"<p>Most modern LLMs rely on the transformer architecture, which is a deep neural network architecture introduced in the 2017 paper \u201cAttention Is All You Need\u201d (https://arxiv.org/abs/1706.03762).</p> <p>TODO (cover transformer in depth) Jay Alammar's blog : https://jalammar.github.io/illustrated-transformer/ Annotated transformer paper : http://nlp.seas.harvard.edu/2018/04/03/attention.html Attention is all you need paper : https://arxiv.org/abs/1706.03762</p>"},{"location":"slides/04_transformer_basics/notes/#encoder-architecture","title":"Encoder Architecture","text":"<p>Self-Attention at high level  Say the following sentence is an input sentence we want to translate:</p> <p>\u201dThe animal didn't cross the street because it was too tired\u201d</p> <p>What does \u201cit\u201d in this sentence refer to? Is it referring to the street or to the animal? It\u2019s a simple question to a human, but not as simple to an algorithm.</p> <p>When the model is processing the word \u201cit\u201d, self-attention allows it to associate \u201cit\u201d with \u201canimal\u201d.</p> <p>Self-Attention in detail - The first step in calculating self-attention is to create three vectors from each of the encoder\u2019s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process.      q1 = Wq x embedding1, k1 = Wk x embedding1, v1 = Wv x embedding1</p> <ul> <li> <p>Next, we calculate a score that determines how much focus to place on other parts of the input sentence for each word. We do this by taking the dot product of the Query vector with the Key vector of each word in the sentence. This gives us a score for each word.     score1 = q1 . k1, score2 = q1 . k2, score3 = q1 . k3, ...</p> </li> <li> <p>We then divide each of these scores by the square root of the dimension of the Key vectors     score1 = score1 / sqrt(dk), score2 = score2 / sqrt(dk), ...</p> </li> <li> <p>Next, we apply a softmax function to these scores to obtain the weights on the Value vectors. The softmax function converts the scores into probabilities that sum to 1.     weights = softmax([score1, score2, score3, ...])</p> </li> <li> <p>The fifth step is to multiply each value vector by the softmax score (in preparation to sum them up).     weighted_v1 = weights[0] * v1, weighted_v2 = weights[1] * v2, ...</p> </li> <li>Finally, we sum up the weighted value vectors to get the output vector for this word.    z1 = weighted_v1 + weighted_v2 + weighted_v3 + ...</li> </ul> <p>Multi-Head Attention - Instead of performing a single self-attention calculation, the transformer architecture uses multiple self-attention calculations in parallel, known as multi-head attention. Each head has its own set of learned weight matrices (Wq, Wk, Wv) and produces its own output vector. - The outputs of all the heads are then concatenated and linearly transformed to produce the final output vector for the word. - Multiple heads allow the model to attend to different parts of the input sentence simultaneously, capturing different relationships and patterns in the data. - This is particularly useful for complex tasks like language translation, where different words may have different relationships with each other.</p> <p>Positional Encoding - Since the transformer architecture does not have any inherent notion of word order (unlike RNNs), it uses positional encoding to inject information about the position of each word in the sentence. - Positional encodings are added to the input embeddings at the bottom of the encoder and decoder stacks.</p> <p>The Residual Connection and Layer Normalization - Each sub-layer in the transformer (such as the multi-head attention layer and the feed-forward layer) has a residual connection around it, followed by layer normalization. - This helps to stabilize the training process and allows for deeper networks.</p>"},{"location":"slides/04_transformer_basics/notes/#the-decoder-side-of-the-transformer","title":"The decoder side of the Transformer","text":"<ul> <li>The output of the top encode block is K and V values</li> <li>This is fed into encoder-decoder attention blocks of the decoder blocks along with the target sequence (shifted right by one position) </li> <li>The decoder has masked self-attention layers to prevent positions from attending to subsequent positions. This masking, combined with the fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.</li> </ul> <p>The final linear and softmax layer  - The final output of the decoder stack is fed into a linear layer followed by a softmax layer to produce the final output probabilities for each token in the vocabulary.  - The token with the highest probability is selected as the output token for that position.  - </p>"},{"location":"slides/04_transformer_basics/notes/#transfromer-circuits","title":"Transfromer Circuits","text":"<p>Readling List - https://jalammar.github.io/illustrated-transformer/ - http://nlp.seas.harvard.edu/2018/04/03/attention.html - https://arxiv.org/abs/1706.03762 - Illustrated GPT-2 : https://jalammar.github.io/illustrated-gpt2/ - Krupa Dave - Everything about Transformers - Ch3 and Ch4 of LLMs from Scratch book - Pytorch overview </p>"},{"location":"slides/04_transformer_basics/slides-2/","title":"Slides 2","text":"## Self-Attention with Learned Weights"},{"location":"slides/04_transformer_basics/slides-2/#scaled-dot-product-attention","title":"Scaled Dot-Product Attention","text":"<ul> <li>Assume attention weights as random matrix</li> <li>We need to learn these weights during training</li> </ul> <p>What to attend to : Identify relevant tokens in the sequence.</p> <p>How strongly to attend: Compute attention weights over those tokens.</p> <p>Contextualize/Output: Weighted sum of token representations.</p>"},{"location":"slides/04_transformer_basics/slides-2/#introducing-q-k-v-vectors","title":"Introducing Q, K, V vectors","text":"<ul> <li>Query weight matrix ($W_q$) (for queries/input tokens)</li> <li>Key weight matrix ($W_k$) (for keys/other tokens)</li> <li>Value weight matrix ($W_v$) (for values/tokens to copy from)</li> </ul>"},{"location":"slides/04_transformer_basics/slides-2/#why-three-vectors-q-k-v","title":"Why three vectors (Q, K, V)?","text":"<ul> <li>Query (Q): Represents the current token we are focusing on. Q gets specialized to ask questions about relevance.</li> <li>Key (K): Represents all tokens we are comparing against. K gets specialized to help determine relevance to Q.</li> <li> <p>Value (V): Represents the actual information we want to copy from the input. V gets specialized to carry useful content.</p> </li> <li> <p>Q and K exist to define a similarity space, while V exists to define an information space.</p> </li> <li>Queries are optimized to ask questions</li> <li>Values are optimized to carry information</li> </ul> <p>--</p> <p>Example Library System:</p> <p>Think of each token as a book in a library. Every book has three different representations, depending on what you\u2019re doing with it.</p> <p>Query (Q): the question you\u2019re asking right now</p> <p>\"I'm looking for material about animal fatigue\"</p> <p>\"I need something that explains causal negation\"</p> <p>Important: The query is shaped by your current goal, not by what the books contain.</p> <p>That\u2019s exactly what Q does:</p> <p>It encodes what this token needs from context.</p> <p>--</p> <p>Key (K): the book\u2019s index card</p> <p>Each book has an index card (or metadata record).</p> <p>Topics, Keywords, Cross-references, Classification codes</p> <p>Important: The system never reads the whole book to decide relevance. It compares your query against the keys.</p> <p>That\u2019s K: A compact representation optimized for matching, not for content.</p> <p>--</p> <p>Value (V): the actual book content</p> <p>Once relevant books are identified, you don\u2019t copy the index cards.</p> <p>You copy: Paragraphs, Explanations, Facts,</p> <p>That\u2019s the Value: The information you actually want to transfer into your answer. | Component | Description | Example | |-----------|-------------|---------| | Query (Q) | The question you're asking | \"I need info about animal fatigue\" | | Key (K) | Index cards with topics/keywords | Book metadata, classification codes | | Value (V) | Actual content to retrieve | Paragraphs, explanations, facts to copy |</p> <p>--</p> <p>Can we use Q for V? - Using the same vector for both querying and copying can limit expressiveness. - Separate Q, K, V allow the model to learn different representations for querying and copying.</p>"},{"location":"slides/04_transformer_basics/slides-2/#q-k-v-projections","title":"Q, K, V Projections","text":"<p>$W_q$: projects tokens into query space</p> <p>$W_k$: projects tokens into key space</p> <p>$W_v$: projects tokens into value space</p> <p>$Q = X W_q  $=&gt; Turn input tokens into queries</p> <p>$K = X W_k  $=&gt; Turn input tokens into keys</p> <p>$V = X W_v  $=&gt; Turn input tokens into values</p>"},{"location":"slides/04_transformer_basics/slides-2/#self-attention-with-learned-weights","title":"Self-Attention with Learned Weights","text":"<ul> <li>Project input vectors to query, key, and value spaces</li> </ul>      $$\\mathbf{queries} = X W_q$$  $$\\mathbf{keys} = X W_k$$  $$\\mathbf{values} = X W_v$$  <ul> <li>Compute Similarity : </li> </ul>   $$\\mathbf{scores} = \\mathbf{queries} \\times \\mathbf{keys}^T$$  $$\\mathbf{attn\\_weights} = \\text{softmax}(\\mathbf{scores})$$   <ul> <li>Contextualization</li> </ul>   $$\\mathbf{context\\_vec} = \\mathbf{attn\\_weights} \\times \\mathbf{values}$$"},{"location":"slides/04_transformer_basics/slides-2/#scaled-dot-product-attention_1","title":"Scaled Dot-Product Attention","text":"$$ \\mathbf{Attention}(Q, K, V) = \\mathbf{softmax}\\left(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\right)V $$   <p>$$d_k \\text{ is the dimensionality of the key vectors (used for scaling).}$$</p> <p>Why divide by sqrt(dk)? - Prevents large dot product values when dk is large - Helps keep gradients stable during training - Dot product variance is ~dk, scaling by sqrt(dk) normalizes variance to ~1</p>"},{"location":"slides/04_transformer_basics/slides-2/#implementing-self-attention-with-learned-weights-single-token","title":"Implementing Self-Attention with Learned Weights : Single Token","text":"<p>Calculating self-attention step-by-step : Single Token</p> <p></p> <p>--</p> <ol> <li> <p>Initial Step : <pre><code>W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\nW_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\nW_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n</code></pre></p> </li> <li> <p>Compute Query, Key, Value vectors:</p> </li> </ol> <pre><code>q_2 = x_2 @ W_query  # Query for token 2\nk_2 = x_2 @ W_key    # Key for token 2\nv_2 = x_2 @ W_value  # Value for token 2\n</code></pre> <ol> <li>Compute attention weights by dot product of Query with all Key vectors:</li> </ol> <pre><code>attn_scores_2 = torch.empty(inputs.shape[0])\nattn_scores_2 = q_2.dot(k_2)\nattn_weights_2 = torch.softmax(attn_scores_2 / (k_2.shape[-1]**0.5), dim=-1)\n</code></pre> <ol> <li>Calculate value vectors: <pre><code>context_vec_2 = torch.zeros(inputs.shape[1])\ncontext_vec_2 = attn_weights[2] @ V\n</code></pre></li> </ol>"},{"location":"slides/04_transformer_basics/slides-2/#self-attention-with-learned-weights-all-tokens","title":"Self-Attention with Learned Weights :  All Tokens","text":"<pre><code>class SelfAttention(nn.Module):\n    def __init__(self, d_in, d_out, qkv_bias=False):\n\n        super().__init__()\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n\n    def forward(self, x):\n        keys = self.W_key(x)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n        attn_scores = queries @ keys.T\n        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n        context_vec = attn_weights @ values\n        return context_vec\n</code></pre>"},{"location":"slides/04_transformer_basics/slides-2/#causal-attention-masking-in-decoder-only-models","title":"Causal  Attention (Masking) in Decoder-Only Models","text":"<ul> <li>In decoder-only models, we predict next token based on previous tokens</li> <li>Note : During training, we predict all tokens in parallel</li> <li>To prevent information leakage from future tokens, we apply a causal mask to the attention scores</li> <li>At all time steps, each token can only attend to earlier tokens</li> </ul>"},{"location":"slides/04_transformer_basics/slides-2/#masking-in-causal-attention","title":"Masking in Causal Attention","text":"<ul> <li>Masking</li> </ul> <pre><code>sa = SelfAttention(d_in, d_out)\nqueries = sa.W_query(inputs)\nkeys = sa.W_key(inputs)\nattn_scores = queries @ keys.T\n</code></pre> <ul> <li>Softmax Function</li> </ul> <p>$$ \\mathrm{Softmax}(x) = \\frac{\\exp(x)}{\\sum \\exp(x)} $$</p> <ul> <li>Fill with -inf where mask is True</li> </ul> <pre><code>    # Step 1: Create mask shape (L, L)\n    seq_len = attn_scores.shape[-1]\n\n    # Step 2: Lower triangular = positions we CAN attend to\n    causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n\n    # Step 3: Set positions we CAN'T attend to as -inf\n    attn_scores.masked_fill_(causal_mask, float('-inf'))\n</code></pre>"},{"location":"slides/04_transformer_basics/slides-2/#attenion-dropout","title":"Attenion + Dropout","text":"<p>Dropout is a regularization technique that randomly sets some neuron weights to zero during training to prevent overfitting.</p> <p></p>"},{"location":"slides/04_transformer_basics/slides-2/#dropout-in-attention-weights","title":"Dropout in Attention Weights","text":"<ul> <li>By applying dropout to attention weights, we randomly ignore some attention connections during training.</li> <li>Makes the model more robust by preventing it from relying too heavily on specific attention patterns.</li> </ul> <pre><code>dropout = nn.Dropout(p=0.2)\nattn_weights = dropout(attn_weights)\n</code></pre>"},{"location":"slides/04_transformer_basics/slides-2/#putting-it-all-together-self-attention-module-with-masking-and-dropout","title":"Putting it all together: Self-Attention Module with Masking and Dropout","text":"<pre><code>    keys = self.W_key(x)\n    queries = self.W_query(x)\n    values = self.W_value(x)\n\n    attn_scores = queries @ keys.T\n\n    mask = torch.triu(torch.ones(L, L), diagonal=1).bool()  # Upper triangular\n    attn_scores.masked_fill_(mask, float('-inf'))\n\n    attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n\n    attn_weights = self.dropout(attn_weights)\n\n    context_vec = attn_weights @ values\n</code></pre>"},{"location":"slides/04_transformer_basics/slides-2/#multi-head-attention","title":"Multi-Head Attention","text":""},{"location":"slides/04_transformer_basics/slides-2/#stacking-multiple-attention-heads","title":"Stacking multiple attention heads","text":"<ul> <li>Perform multiple self-attention calculations in parallel, with own set of learned weight matrices (Wq, Wk, Wv) and  output vector for each head.</li> <li>Concatenate all to produce one context vector for each token.</li> <li>Multiple heads -&gt; attend to input sentence simultaneously -&gt; different relationships and patterns in the data.</li> </ul>"},{"location":"slides/04_transformer_basics/slides-2/#multi-head-attention-naive-implementation","title":"Multi-Head Attention : Naive Implementation","text":"<pre><code># naive version using loops\n\nclass MultiHeadAttentionWrapper(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n        super().__init__()\n        self.heads = nn.ModuleList([\n            CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n            for _ in range(num_heads)\n        ])\n\n    def forward(self, x):\n        return torch.cat([head(x) for head in self.heads], dim=-1)\n</code></pre>"},{"location":"slides/04_transformer_basics/slides-2/#multi-head-attention-efficient-implementation","title":"Multi-Head Attention : Efficient Implementation","text":"<pre><code>class MultiHeadAttention(nn.Module):\n  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n    super().__init__()\n    self.num_heads = num_heads\n    assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n    self.head_dim = d_out // num_heads\n    self.out_proj = nn.Linear(d_out, d_out)\n\n  def forward(self, X):\n\n    queries = queries.reshape(batches, num_tokens, self.num_heads, self.head_dim) # B x L x num_heads x head_dim\n    keys = keys.reshape(batches, num_tokens, self.num_heads, self.head_dim) # B x L x num_heads x head_dim\n    values = values.reshape(batches, num_tokens, self.num_heads, self.head_dim) # B x L x num_heads x head_dim\n\n    queries = queries.transpose(1, 2) # B x num_heads x L x head_dim\n    keys = keys.transpose(1, 2) # B x num_heads x L x head_dim\n    values = values.transpose(1, 2) # B x num_heads x L x head_dim\n\n    attn_scores = queries @ keys.transpose(2, 3) # (B x num_heads x L x head_dim) @ (B x num_heads x head_dim x L) =&gt; B x num_heads x L x L\n\n    # mask : # L x L =&gt; (1 x 1 x L x L)\n    attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) # B x num_heads x L x L\n\n    attn_weights = torch.softmax(attn_scores / self.head_dim ** 0.5, dim=-1) \n    attn_weights = nn.Dropout(self.dropout)(attn_weights) # B x num_heads x L x L\n\n    context_vec = attn_weights @ values # (B x num_heads x L x L) @ (B x num_heads x L x head_dim)\n\n    context_vec = context_vec.transpose(1, 2) # B x L x num_heads x head_dim\n    context_vec = context_vec.reshape(batches, num_tokens, self.d_out) # B x L x d_out\n\n    return self.out_proj(context_vec) # (B x L x d_out) @ (d_out x d_out) =&gt; B x L x d_out\n</code></pre>"},{"location":"slides/04_transformer_basics/slides-2/#summary","title":"Summary","text":"<p>Journey through Attention:</p> <ol> <li>Simple Attention: Dot products between embeddings (no learning)</li> <li>Self-Attention: Add learnable Q, K, V projections</li> <li>Causal Attention: Mask future tokens for autoregressive generation</li> <li>Multi-Head: Run multiple attention patterns in parallel</li> </ol>"},{"location":"slides/04_transformer_basics/slides-2/#what-we-didnt-cover","title":"What We Didn't Cover","text":"<ul> <li>Positional Encodings: How does attention know word order?</li> <li>Feed-Forward Networks: The other half of each transformer block</li> <li>Layer Normalization: Stabilizing training</li> <li>Residual Connections: Enabling deep networks</li> </ul> <p>\u2192 These will be covered in the next session where we implement a full transformer block and a LLM from scratch!</p>"},{"location":"slides/04_transformer_basics/slides/","title":"Slides","text":"## LLMs : A Hands-on Approach   ### Transformers"},{"location":"slides/04_transformer_basics/slides/#topics-covered","title":"Topics Covered","text":"<ul> <li>Transformer architecture</li> <li>Self-attention mechanism</li> <li>Causal Attention</li> <li>Multi-head attention</li> </ul>"},{"location":"slides/04_transformer_basics/slides/#models-of-the-week","title":"Models of the Week","text":"<p>moonshotai/Kimi-K2.5</p> <ul> <li>SOTA Open Source model</li> <li>1T+ parameters, 30T tokens pretraining data, Vocabulary Size - 160K, Context Length - 256K</li> <li>Agentic, Agent Swarm, Multi-modal capabilities</li> </ul> <p>nvidia/personaplex-7b-v1</p> <ul> <li>7B, Real-time, speech to speech model, full-duplex model </li> <li>Demo</li> </ul> <p>Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice</p> <ul> <li>Text to Speech model with voice design, voice cloning, custom voice</li> </ul>"},{"location":"slides/04_transformer_basics/slides/#recap-tokenization","title":"Recap : Tokenization","text":"<ul> <li>Words, Subwords, Characters level tokenization</li> <li>Subword tokenization is the most commonly used approach in LLMs</li> </ul>"},{"location":"slides/04_transformer_basics/slides/#text-to-token-ids","title":"Text to Token IDs","text":"<pre><code>import tiktoken\ntokenizer = tiktoken.get_encoding(\"gpt2\")\ntext = \"This is an example.\"\ntoken_ids = tokenizer.encode(text)\n# tokens = ['This', ' is', ' an', ' example', '.']\n# token_ids = [40234, 2052, 133, 389, 12]\n</code></pre>"},{"location":"slides/04_transformer_basics/slides/#tokens-to-embeddings","title":"Tokens to Embeddings","text":"<ul> <li>Each token ID maps to a unique vector in the embedding matrix</li> <li>Embedding Matrix : [vocab_size x embedding_dim] </li> <li>Example : GPT-2 Small<ul> <li>vocab_size = 50257</li> <li>embedding_dim = 768</li> </ul> </li> </ul>   ## Transformers"},{"location":"slides/04_transformer_basics/slides/#transformers","title":"Transformers","text":"<ul> <li>All LLMs rely on the Transformer architecture, introduced in the 2017 paper \"Attention Is All You Need\" (https://arxiv.org/abs/1706.03762).</li> <li>No Recurrent or Convolution layers, entirely based on Attention Mechanism</li> </ul>"},{"location":"slides/04_transformer_basics/slides/#two-types-of-transformer-architectures","title":"Two types of Transformer Architectures","text":"<ul> <li>Encoder-Decoder : Sequence to sequence tasks (translation, summarization)</li> <li>Decoder-Only : Language modeling tasks (text generation, completion)</li> <li>Most LLMs (GPT, Llama, etc.) use the Decoder-Only architecture</li> </ul>"},{"location":"slides/04_transformer_basics/slides/#key-components-of-transformer","title":"Key Components of Transformer","text":"<ul> <li>Positional Encoding </li> <li>Multi-Head Attention </li> <li>Residual Connections </li> <li>Feed Forward Networks </li> <li>Layer Normalization </li> </ul>"},{"location":"slides/04_transformer_basics/slides/#language-modeling","title":"Language Modeling","text":"<ul> <li>Given a sequence of tokens, predict the next token</li> <li>Example: A robot may not harm a ___ -&gt; human</li> <li>[PROMPT] -&gt; [MODEL] -&gt; [PREDICTION]</li> </ul>"},{"location":"slides/04_transformer_basics/slides/#decoder-only-language-model","title":"Decoder-only Language Model","text":"<ul> <li>A decoder-only language model is a stack of transformer decoder blocks</li> </ul>"},{"location":"slides/04_transformer_basics/slides/#decoder-only-llm-input-side","title":"Decoder-only LLM : Input Side","text":"<ul> <li>Input tokens are passed through multiple  decoder blocks</li> <li>Embed : Text -&gt; Token IDs -&gt; Embeddings -&gt; Decoder Blocks</li> </ul>"},{"location":"slides/04_transformer_basics/slides/#decoder-only-llm-output-side","title":"Decoder-only LLM : Output Side","text":"<ul> <li>UnEmbed : The final vector is projected to vocabulary size and softmaxed to get token probabilities</li> </ul>"},{"location":"slides/04_transformer_basics/slides/#decoder-block-internals","title":"Decoder Block Internals","text":"<ul> <li>Masked Multi-Head Self-Attention</li> <li>Feed Forward Network (FFN)</li> <li>Residual Connections</li> <li>Layer Normalization</li> </ul>   ## Attention Mechanism"},{"location":"slides/04_transformer_basics/slides/#sequence-modeling-challenges","title":"Sequence Modeling Challenges","text":"<ul> <li>Understanding context and relationships between words</li> <li>Need to keep grammatical structures aligned</li> </ul>"},{"location":"slides/04_transformer_basics/slides/#recurrent-neural-networks-rnns-for-sequence-modeling","title":"Recurrent Neural Networks (RNNs) for Sequence Modeling","text":"<ul> <li>Process all input into a hidden state, </li> <li>Pass hidden state to decoder</li> <li>Decoder uses hidden state to generate output sequence</li> </ul>"},{"location":"slides/04_transformer_basics/slides/#rnns-attention","title":"RNNs + Attention","text":"<ul> <li>Let Decoder access all Encoder hidden states</li> <li>Attend to relevant parts of input sequence when generating each output token [Bahdanau et al., 2015]</li> </ul>"},{"location":"slides/04_transformer_basics/slides/#rnns-attention_1","title":"RNNs + Attention","text":"<p>Limitations</p> <ul> <li>Sequential processing limits parallelization</li> <li>Difficulty capturing long-range dependencies</li> </ul> <p>Solution: </p> <ul> <li>Remove recurrence, process all input tokens simultaneously</li> <li>Allow each token in the input to focus on relevant parts of the input</li> </ul>"},{"location":"slides/04_transformer_basics/slides/#parallel-processing","title":"Parallel Processing","text":"<ul> <li>Encoder : Process all input tokens simultaneously</li> <li>Decoder : Generate output tokens one by one, attending to encoder states and previous tokens</li> </ul>"},{"location":"slides/04_transformer_basics/slides/#self-attention-mechanism","title":"Self-Attention Mechanism","text":"<ul> <li>Compute attention within the same sequence of tokens. Self = Same Sequence</li> <li>Get improved representation by mixing in information from other tokens that seem relevant.</li> </ul>"},{"location":"slides/04_transformer_basics/slides/#self-attention-intuition","title":"Self-Attention : Intuition","text":"*\"Self-attention is like a group conversation where everyone can hear everyone else simultaneously, rather than passing notes one by one (RNNs)\"*"},{"location":"slides/04_transformer_basics/slides/#self-attention-intuition_1","title":"Self-Attention : Intuition","text":"<ul> <li> <p>Each token : \"Who should I pay attention to?\"</p> </li> <li> <p>For every token, the model:</p> <ul> <li>treats that token as the \"current focus\"</li> <li>assigns higher weight to tokens that help interpret it</li> <li>creates an updated vector for the token:  </li> </ul> </li> </ul>"},{"location":"slides/04_transformer_basics/slides/#self-attention-vs-encoderdecoder-attention","title":"Self-Attention vs Encoder\u2013Decoder Attention","text":"<ul> <li> <p>Encoder\u2013Decoder: One sequence attends to a different sequence (e.g., translation: output attends to the input sentence).</p> </li> <li> <p>Self-attention: Sequence attends to itself (tokens attending to other tokens in the same sentence).</p> </li> </ul>"},{"location":"slides/04_transformer_basics/slides/#simple-attention-mechanism","title":"Simple Attention Mechanism","text":"<p>Input  -  Sequence of vectors (X) (source)</p> <p>Output - Sequence of vectors (Z) (context)</p>  $$ X = [x_1, x_2, \\dots, x_n], \\quad x_i \\in \\mathbb{R}^d $$  $$ Z = [z_1, z_2, \\dots, z_n], \\quad z_i \\in \\mathbb{R}^d $$  $$ z_i = \\sum_{j=1}^{n} \\text{attention\\_weight}_{ij} \\. x_j $$"},{"location":"slides/04_transformer_basics/slides/#computing-attention-weights-for-a-single-token","title":"Computing attention weights for a single token","text":"*Your **journey** starts with one step*   <p>query = \"journey\"</p> <p></p> <p>Step 1: </p> <ul> <li>Compute attention scores by dot product of \"journey\" with all tokens</li> </ul> <pre><code>query = inputs[1]\n\nattn_scores_2 = torch.empty(inputs.shape[0])\n\nfor i, x_i in enumerate(inputs):\n    attn_scores_2[i] = torch.dot(x_i, query)\n\nprint(attn_scores_2)\n</code></pre> <p>--</p>"},{"location":"slides/04_transformer_basics/slides/#computing-attention-weights-for-a-single-token_1","title":"Computing attention weights for a single token","text":"<p>Step 2:</p> <ul> <li>Apply normalization to get attention weights (additive normalization)</li> <li>Normalization using softmax is more common in practice, as it ensures all weights are positive and sum to 1.</li> </ul> <pre><code>attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n\ndef softmax_naive(x):\n    return torch.exp(x) / torch.exp(x).sum(dim=0)\n\nattn_weights_2_naive = softmax_naive(attn_scores_2)\n\nattn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n</code></pre> <p>--</p> <p>Step 3: - Compute output vector as weighted sum of value vectors</p> <p></p> <pre><code>context_vec_2 = torch.zeros(inputs.shape[1])\nfor i, x_i in enumerate(inputs):\n    context_vec_2 += attn_weights_2[i] * x_i\n</code></pre>"},{"location":"slides/04_transformer_basics/slides/#computing-attention-weigths-for-all-tokens","title":"Computing attention weigths for all tokens","text":"<ul> <li>Compute attention scores for all tokens ```python [1 | 2-10] attn_scores = torch.zeros(inputs.shape[0], inputs.shape[0])</li> </ul> <p>for i, x_i in enumerate(inputs):     for j, x_j in enumerate(inputs):         attn_scores[i, j] = torch.dot(x_i, x_j)</p> <pre><code>- Normalize scores to get attention weights\n```python\nattn_weights = torch.softmax(attn_scores, dim=-1)\n</code></pre> <ul> <li>Compute output/context    vectors for all tokens <pre><code>output_vectors = torch.zeros_like(inputs)\nfor i in range(inputs.shape[0]):\n    for j in range(inputs.shape[0]):\n        output_vectors[i] += attn_weights[i, j] * inputs[j]\n</code></pre></li> </ul> <p>--</p>"},{"location":"slides/04_transformer_basics/slides/#computing-attention-weigths-for-all-tokens_1","title":"Computing attention weigths for all tokens","text":"<ul> <li>Better implementation using matrix multiplication</li> </ul> <pre><code>attn_scores = torch.zeros(inputs.shape[0], inputs.shape[0])\n\nattn_scores = inputs @ inputs.T\nattn_weights = torch.softmax(attn_scores, dim=-1)\noutput_vectors = attn_weights @ inputs\n</code></pre>"},{"location":"slides/04_transformer_basics/slides/#summary-of-self-attention-mechanism","title":"Summary of Self-Attention Mechanism","text":"<ul> <li>Input: sequence of vectors (X) (source)</li> <li>Output: sequence of vectors (Z) (context)</li> <li>Compute attention scores against all input vectors</li> <li>Normalize scores to get attention weights</li> <li>Compute output vectors as weighted sum of input vectors</li> </ul> <pre><code>def self_attention(inputs):\n    # Step 1: Compute attention scores\n    attn_scores = inputs @ inputs.T\n    # Step 2: Normalize scores to get attention weights\n    attn_weights = torch.softmax(attn_scores, dim=-1)\n    # Step 3: Compute output vectors as weighted sum of input vectors\n    output_vectors = attn_weights @ inputs\n    return output_vectors\n</code></pre>   **How to improve this basic self-attention mechanism?**   Learn the weights used to compute attention scores!"},{"location":"slides/04_transformer_basics/slides/#references","title":"References","text":"<ul> <li>Vaswani et al., Attention Is All You Need (2017) - The original transformer paper</li> <li>Bahdanau et al., Neural Machine Translation by Jointly Learning to Align and Translate (2014) - Introduced attention for seq2seq</li> <li>Jay Alammar, The Illustrated Transformer</li> <li>Jay Alammar, The Illustrated GPT-2</li> <li>The Annotated Transformer - Harvard NLP</li> <li>Sebastian Raschka, Build a Large Language Model from Scratch - Chapters 3-4</li> </ul>"},{"location":"slides/04_transformer_basics/slides/#thank-you","title":"Thank You","text":"<p>Questions?</p>"},{"location":"slides/reveal/css/theme/","title":"Index","text":""},{"location":"slides/reveal/css/theme/#dependencies","title":"Dependencies","text":"<p>Themes are written using Sass to keep things modular and reduce the need for repeated selectors across files. Make sure that you have the reveal.js development environment installed before proceeding: https://revealjs.com/installation/#full-setup</p>"},{"location":"slides/reveal/css/theme/#creating-a-theme","title":"Creating a Theme","text":"<p>To create your own theme, start by duplicating a <code>.scss</code> file in /css/theme/source. It will be automatically compiled from Sass to CSS (see the gulpfile) when you run <code>npm run build -- css-themes</code>.</p> <p>Each theme file does four things in the following order:</p> <ol> <li> <p>Include /css/theme/template/mixins.scss Shared utility functions.</p> </li> <li> <p>Include /css/theme/template/settings.scss Declares a set of custom variables that the template file (step 4) expects. Can be overridden in step 3.</p> </li> <li> <p>Override This is where you override the default theme. Either by specifying variables (see settings.scss for reference) or by adding any selectors and styles you please.</p> </li> <li> <p>Include /css/theme/template/theme.scss The template theme file which will generate final CSS output based on the currently defined variables.</p> </li> </ol>"},{"location":"slides/reveal/examples/markdown/","title":"Markdown Demo","text":""},{"location":"slides/reveal/examples/markdown/#external-11","title":"External 1.1","text":"<p>Content 1.1</p> <p>Note: This will only appear in the speaker notes window.</p>"},{"location":"slides/reveal/examples/markdown/#external-12","title":"External 1.2","text":"<p>Content 1.2</p>"},{"location":"slides/reveal/examples/markdown/#external-2","title":"External 2","text":"<p>Content 2.1</p>"},{"location":"slides/reveal/examples/markdown/#external-31","title":"External 3.1","text":"<p>Content 3.1</p>"},{"location":"slides/reveal/examples/markdown/#external-32","title":"External 3.2","text":"<p>Content 3.2</p>"},{"location":"slides/reveal/examples/markdown/#external-33-image","title":"External 3.3 (Image)","text":""},{"location":"slides/reveal/examples/markdown/#external-34-math","title":"External 3.4 (Math)","text":"<p><code>\\[ J(\\theta_0,\\theta_1) = \\sum_{i=0} \\]</code></p>"},{"location":"slides/reveal/test/simple/","title":"Simple","text":""},{"location":"slides/reveal/test/simple/#slide-11","title":"Slide 1.1","text":"<pre><code>var a = 1;\n</code></pre>"},{"location":"slides/reveal/test/simple/#slide-12","title":"Slide 1.2","text":""},{"location":"slides/reveal/test/simple/#slide-2","title":"Slide 2","text":""}]}