{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LLM Engineering Course","text":"<p>Welcome to the Large Language Models: A Hands-on Approach course! This comprehensive 18-week program is designed for industry professionals who want to master the complete spectrum of LLM engineering.</p>"},{"location":"#what-youll-learn","title":"What You'll Learn","text":"<p>Transform from LLM user to LLM engineer through hands-on experience with:</p> <ul> <li>Architecture Mastery: Deep dive into transformers, attention mechanisms, and modern LLM variants</li> <li>Inference Optimization: Quantization, KV caching, multi-GPU serving, and production deployment</li> <li>Fine-tuning Expertise: LoRA, instruction tuning, preference alignment, and domain adaptation  </li> <li>AI Agent Development: Tool-calling agents, RAG systems, and multi-agent orchestration</li> <li>Multimodal AI: Vision-language models, speech integration, and edge deployment</li> <li>Production Readiness: Security, monitoring, evaluation, and cost optimization</li> </ul>"},{"location":"#course-navigation","title":"Course Navigation","text":"-   **Course Details**      ---      Comprehensive overview of objectives, structure, timings, and prerequisites      [View Details \u2192](course-details.md)  -   **Weekly Schedule**      ---      18-week curriculum with topics, themes, and learning progression      [View Schedule \u2192](schedule.md)  -   **Assignments**      ---      Hands-on labs, projects, and exercises for each week      [View Assignments \u2192](assignments.md)"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Programming: Proficient in Python</li> <li>ML Background: Understanding of neural networks and backpropagation  </li> <li>Hardware: Basic GPU computing knowledge (preferred)</li> <li>Tools: Familiarity with Git, command line, Jupyter notebooks</li> </ul>"},{"location":"#development-environment","title":"Development Environment","text":"<pre><code># Clone the course repository\ngit clone https://github.com/llm-engg/llm-engg.github.io.git\ncd llm-engg.github.io\n\n# Set up Python environment\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install dependencies\npip install mkdocs mkdocs-material\n\n# Start local development server\nmkdocs serve\n</code></pre>"},{"location":"#course-progression","title":"Course Progression","text":"<p>The course follows a carefully designed progression:</p> <ol> <li>Foundation (Weeks 1-4): Master transformer architectures and GPU fundamentals</li> <li>Optimization (Weeks 5-7): Learn inference optimization and production serving</li> <li>Fine-tuning (Weeks 8-10): Apply various fine-tuning and alignment techniques</li> <li>Applications (Weeks 11-13): Build RAG systems and AI agents</li> <li>Advanced (Weeks 14-17): Explore evaluation, multimodal AI, edge deployment, and security</li> <li>Showcase (Week 18): Present final projects and peer learning</li> </ol>"},{"location":"#learning-outcomes","title":"Learning Outcomes","text":"<p>By course completion, you'll be able to:</p> <ul> <li>Design and implement production-ready LLM applications</li> <li>Optimize inference for cost and performance at scale  </li> <li>Fine-tune models for specific business requirements</li> <li>Build sophisticated RAG and agent systems</li> <li>Deploy LLMs across cloud, edge, and multimodal environments</li> <li>Implement security and monitoring for LLM systems</li> </ul>"},{"location":"#why-this-course","title":"Why This Course?","text":""},{"location":"#industry-focused","title":"Industry-Focused","text":"<p>Every topic connects directly to real-world production challenges faced by companies deploying LLMs at scale.</p>"},{"location":"#hands-on-learning","title":"Hands-On Learning","text":"<p>Each week includes practical labs, code walkthroughs, and projects using industry-standard tools.</p>"},{"location":"#cutting-edge-content","title":"Cutting-Edge Content","text":"<p>Stay current with the latest architectures, optimization techniques, and emerging trends in LLM engineering.</p>"},{"location":"#expert-instruction","title":"Expert Instruction","text":"<p>Learn from practitioners who have built and deployed LLM systems in production environments.</p>"},{"location":"#community-support","title":"Community &amp; Support","text":"<ul> <li>Discussion Forums: Connect with peers and instructors</li> <li>Office Hours: Weekly Q&amp;A sessions for personalized help</li> <li>Project Collaboration: Work with classmates on challenging problems</li> <li>Industry Network: Build connections with LLM engineering professionals</li> </ul> <p>Ready to become an LLM engineering expert? Start with the Course Details to understand the full scope and requirements, then check out the Weekly Schedule to see what lies ahead!</p>"},{"location":"assignments/","title":"Assignments","text":"<p>This page contains all course assignments, projects, and hands-on exercises. Each assignment is designed to reinforce the concepts learned in the corresponding week and build practical skills in LLM engineering.</p>"},{"location":"assignments/#assignment-structure","title":"Assignment Structure","text":"<p>Each week features different types of learning activities:</p> <ul> <li>Lab Exercises: Hands-on coding and implementation</li> <li>Case Studies: Analysis of real-world LLM applications  </li> <li>Projects: Progressive building of LLM systems</li> <li>Experiments: Research and exploration tasks</li> </ul>"},{"location":"assignments/#week-by-week-assignments","title":"Week-by-Week Assignments","text":""},{"location":"assignments/#foundation-phase","title":"Foundation Phase","text":""},{"location":"assignments/#week-1-transformer-architecture","title":"Week 1: Transformer Architecture","text":"<ul> <li>Lab: Attention Visualization - implement and visualize attention mechanisms</li> <li>Project: Code walkthrough of Transformer implementation</li> <li>Exercise: Analyze GPT-2 architecture and training process</li> </ul>"},{"location":"assignments/#week-2-tokenization-moe","title":"Week 2: Tokenization &amp; MoE","text":"<ul> <li>Lab: Tokenization playground - experiment with different tokenizers</li> <li>Project: Pretrain a tiny language model from scratch</li> <li>Case Study: Compare MoE inference vs dense inference performance</li> </ul>"},{"location":"assignments/#week-3-modern-architectures","title":"Week 3: Modern Architectures","text":"<ul> <li>Lab: Modern LLM case study using Hugging Face models</li> <li>Project: Compare configs and forward passes of LLaMA, Mistral, and Qwen</li> <li>Exercise: Visualize differences in architecture (attention heads, layer norms)</li> </ul>"},{"location":"assignments/#infrastructure-phase","title":"Infrastructure Phase","text":""},{"location":"assignments/#week-4-gpu-programming","title":"Week 4: GPU Programming","text":"<ul> <li>Lab: Write CUDA kernels (vector add, matrix multiply) and benchmark vs PyTorch</li> <li>Project: Explore GPU profiling tools (nvprof, Nsight)</li> <li>Case Study: Large GPU cluster cost analysis for training DeepSeek/GPT-5</li> </ul>"},{"location":"assignments/#inference-optimization-phase","title":"Inference Optimization Phase","text":""},{"location":"assignments/#week-5-inference-fundamentals","title":"Week 5: Inference Fundamentals","text":"<ul> <li>Lab: Use PyTorch Profiler to track memory vs compute bottlenecks</li> <li>Project: Explore Hugging Face Hub &amp; inference APIs</li> <li>Exercise: Serve a model with vLLM</li> </ul>"},{"location":"assignments/#week-6-quantization","title":"Week 6: Quantization","text":"<ul> <li>Lab: Quantize models with GPTQ using AutoGPTQ or bitsandbytes</li> <li>Project: Compare fp16, int8, int4 inference (tokens/sec, VRAM use, perplexity)</li> <li>Research: Explore hallucinations induced by quantized models</li> </ul>"},{"location":"assignments/#week-7-multi-gpu-serving","title":"Week 7: Multi-GPU Serving","text":"<ul> <li>Lab: Benchmark vLLM vs HF TGI on the same model</li> <li>Project: Deploy a model on multi-GPU using vLLM</li> <li>Case Study: Deploy DeepSeek model on 100 GPUs</li> </ul>"},{"location":"assignments/#fine-tuning-phase","title":"Fine-tuning Phase","text":""},{"location":"assignments/#week-8-peft-methods","title":"Week 8: PEFT Methods","text":"<ul> <li>Lab: Fine-tune SMOL-LM3 on a simple instruction-following task</li> <li>Project: Compare LoRA vs QLoRA performance and efficiency</li> <li>Research: Investigate catastrophic forgetting mitigation</li> </ul>"},{"location":"assignments/#week-9-instruction-tuning-alignment","title":"Week 9: Instruction Tuning &amp; Alignment","text":"<ul> <li>Lab: Explore preference datasets like Anthropic/hh-rlhf</li> <li>Project: Take SFT model and align it using DPO</li> <li>Exercise: Generate synthetic preference pairs</li> </ul>"},{"location":"assignments/#week-10-reasoning","title":"Week 10: Reasoning","text":"<ul> <li>Lab: Show impact of CoT on reasoning accuracy (GSM8k, coding tasks)</li> <li>Project: Fine-tune a small model on reasoning dataset</li> <li>Research: Compare different reasoning strategies</li> </ul>"},{"location":"assignments/#applications-phase","title":"Applications Phase","text":""},{"location":"assignments/#week-11-rag-systems","title":"Week 11: RAG Systems","text":"<ul> <li>Lab: Build a small RAG pipeline with FAISS vector DB</li> <li>Project: Evaluate retrieval quality using RAGas metrics</li> <li>Case Study: Implement Graph RAG for complex documents</li> </ul>"},{"location":"assignments/#week-12-13-ai-agents","title":"Week 12-13: AI Agents","text":"<ul> <li>Lab: Implement a toy MCP wrapper around a model agent</li> <li>Project: Fine-tune a small model on tool-augmented dataset</li> <li>Exercise: Evaluate improvement in reasoning + tool accuracy</li> </ul>"},{"location":"assignments/#advanced-topics-phase","title":"Advanced Topics Phase","text":""},{"location":"assignments/#week-14-evaluation-monitoring","title":"Week 14: Evaluation &amp; Monitoring","text":"<ul> <li>Lab: Implement LLM-as-a-Judge evaluation</li> <li>Project: Monitor a live RAG/agent pipeline</li> <li>Exercise: Set up A/B testing for model comparison</li> </ul>"},{"location":"assignments/#week-15-multimodal-models","title":"Week 15: Multimodal Models","text":"<ul> <li>Lab: Run a Large Multimodal Model (LMM) locally</li> <li>Project: Fine-tune LLaVA-Phi-3 or Moondream on custom visual task</li> <li>Exercise: Test Visual Question Answering capabilities</li> </ul>"},{"location":"assignments/#week-16-edge-deployment","title":"Week 16: Edge Deployment","text":"<ul> <li>Lab: Run LLM on Raspberry Pi</li> <li>Project: Deploy Gemma-3n on Android/iOS</li> <li>Exercise: Fine-tune Gemma-3n for Kannada language</li> </ul>"},{"location":"assignments/#week-17-security-frontiers","title":"Week 17: Security &amp; Frontiers","text":"<ul> <li>Lab: Demonstrate prompt injection attack and defense</li> <li>Project: Deploy Qwen-3 Next for production use</li> <li>Exercise: Run Mamba-7B on 100K context and compare to Llama-3</li> </ul>"},{"location":"assignments/#final-phase","title":"Final Phase","text":""},{"location":"assignments/#week-18-student-presentations","title":"Week 18: Student Presentations","text":"<ul> <li>Project: Final project presentations</li> <li>Peer Review: Evaluate and discuss peer projects</li> <li>Reflection: Course wrap-up and future learning paths</li> </ul>"},{"location":"assignments/#submission-guidelines","title":"Submission Guidelines","text":""},{"location":"assignments/#code-submissions","title":"Code Submissions","text":"<ul> <li>All code must be submitted via GitHub repositories</li> <li>Include comprehensive README with setup instructions</li> <li>Provide requirements.txt or environment.yml files</li> <li>Include documentation and comments</li> </ul>"},{"location":"assignments/#reports","title":"Reports","text":"<ul> <li>Technical reports should be 2-3 pages maximum</li> <li>Include performance metrics, charts, and analysis</li> <li>Compare results with baseline or existing methods</li> <li>Discuss limitations and future improvements</li> </ul>"},{"location":"assignments/#presentations","title":"Presentations","text":"<ul> <li>10-15 minute presentations for final projects</li> <li>Include live demos when possible</li> <li>Focus on practical insights and lessons learned</li> </ul>"},{"location":"assignments/#grading-criteria","title":"Grading Criteria","text":"<ul> <li>Technical Implementation (40%): Code quality, correctness, efficiency</li> <li>Analysis &amp; Insights (30%): Understanding of concepts, thoughtful analysis</li> <li>Documentation (20%): Clear explanations, reproducible results</li> <li>Innovation (10%): Creative approaches, going beyond requirements</li> </ul>"},{"location":"assignments/#resources-support","title":"Resources &amp; Support","text":"<ul> <li>Office Hours: Weekly Q&amp;A sessions for assignment help</li> <li>Discussion Forum: Peer collaboration and problem-solving</li> <li>Computing Resources: Access to GPU clusters for large model experiments</li> <li>Reference Materials: Curated papers, blogs, and documentation</li> </ul> <p>Assignments will be released weekly and are due before the start of the following week's session. Late submissions will be penalized unless prior arrangements are made.</p>"},{"location":"course-details/","title":"Course Details","text":""},{"location":"course-details/#course-overview","title":"Course Overview","text":"<p>This is a comprehensive hands-on course on Large Language Models (LLMs) designed specifically for industry professionals. The course covers the complete spectrum of LLM engineering, from foundational concepts to advanced deployment and optimization strategies.</p>"},{"location":"course-details/#course-objectives","title":"Course Objectives","text":"<p>By the end of this course, participants will be able to:</p> <ul> <li>Understand LLM Architecture: Master transformer architectures, attention mechanisms, and modern LLM variants (GPT, BERT, Mixtral, etc.)</li> <li>Optimize Inference: Implement efficient inference strategies including quantization, KV caching, and multi-GPU serving</li> <li>Fine-tune Models: Apply various fine-tuning techniques including LoRA, QLoRA, instruction tuning, and preference alignment</li> <li>Build RAG Systems: Design and implement Retrieval-Augmented Generation pipelines with vector databases and evaluation</li> <li>Develop AI Agents: Create tool-using agents with ReAct frameworks and multi-agent orchestration</li> <li>Deploy at Scale: Set up production-ready LLM serving infrastructure with cost optimization</li> <li>Handle Multimodal AI: Work with vision-language models and speech integration</li> <li>Ensure Security: Implement security measures against prompt injection and other LLM-specific threats</li> </ul>"},{"location":"course-details/#course-structure","title":"Course Structure","text":"<p>The course is organized into 18 weeks covering 8 major themes:</p>"},{"location":"course-details/#1-llm-foundations-weeks-1-3","title":"1. LLM Foundations (Weeks 1-3)","text":"<ul> <li>Transformer architecture deep dive</li> <li>Tokenization and pretraining objectives</li> <li>Modern architectures (GPT, Qwen, Gemma)</li> <li>Scaling laws and emergent properties</li> </ul>"},{"location":"course-details/#2-gpu-infrastructure-week-4","title":"2. GPU &amp; Infrastructure (Week 4)","text":"<ul> <li>GPU architecture and CUDA programming</li> <li>Multi-GPU and multi-node parallelism</li> <li>Hardware selection and cluster building</li> </ul>"},{"location":"course-details/#3-inference-optimization-weeks-5-7","title":"3. Inference Optimization (Weeks 5-7)","text":"<ul> <li>Inference bottlenecks and profiling</li> <li>Quantization techniques (INT8, INT4, GPTQ, AWQ)</li> <li>Inference engines (vLLM, TensorRT-LLM)</li> <li>Multi-GPU serving strategies</li> </ul>"},{"location":"course-details/#4-fine-tuning-weeks-8-10","title":"4. Fine-tuning (Weeks 8-10)","text":"<ul> <li>Parameter-efficient fine-tuning (LoRA, QLoRA)</li> <li>Instruction tuning and data curation</li> <li>Preference alignment (RLHF, DPO)</li> <li>Reasoning and chain-of-thought training</li> </ul>"},{"location":"course-details/#5-rag-systems-week-11","title":"5. RAG Systems (Week 11)","text":"<ul> <li>Vector databases and hybrid search</li> <li>RAG evaluation and optimization</li> <li>Graph RAG and advanced retrieval</li> </ul>"},{"location":"course-details/#6-ai-agents-weeks-12-13","title":"6. AI Agents (Weeks 12-13)","text":"<ul> <li>ReAct framework and tool calling</li> <li>Agent fine-tuning and evaluation</li> <li>Multi-agent orchestration</li> </ul>"},{"location":"course-details/#7-advanced-topics-weeks-14-17","title":"7. Advanced Topics (Weeks 14-17)","text":"<ul> <li>Model evaluation and monitoring</li> <li>Multimodal models (vision, audio)</li> <li>Edge deployment and tiny models</li> <li>Security and privacy engineering</li> <li>Emerging architectures (Mamba, hybrid models)</li> </ul>"},{"location":"course-details/#8-student-presentations-week-18","title":"8. Student Presentations (Week 18)","text":"<ul> <li>Project showcases and peer learning</li> </ul>"},{"location":"course-details/#course-timings","title":"Course Timings","text":"<ul> <li>Duration: 18 weeks</li> <li>Format: Weekly sessions with hands-on labs</li> <li>Target Audience: Industry professionals and advanced practitioners</li> <li>Prerequisites: Python programming, basic ML knowledge</li> <li>Level: Advanced/Professional</li> </ul>"},{"location":"course-details/#hands-on-approach","title":"Hands-on Approach","text":"<p>Each week includes:</p> <ul> <li>Theory Sessions: Core concepts and architecture deep dives</li> <li>Practical Labs: Implementation exercises and code walkthroughs</li> <li>Case Studies: Real-world examples and industry applications</li> <li>Projects: Progressive building of LLM applications</li> </ul>"},{"location":"course-details/#key-tools-technologies","title":"Key Tools &amp; Technologies","text":"<ul> <li>Frameworks: PyTorch, Hugging Face Transformers, vLLM</li> <li>Infrastructure: CUDA, NCCL, DeepSpeed, FSDP</li> <li>Deployment: Docker, Kubernetes, cloud platforms</li> <li>Monitoring: Prometheus, Grafana, PyTorch Profiler</li> <li>Development: Python, Jupyter, Git, MkDocs</li> </ul>"},{"location":"course-details/#assessment","title":"Assessment","text":"<ul> <li>Weekly hands-on assignments (60%)</li> <li>Mid-term project (20%)</li> <li>Final project presentation (20%)</li> </ul>"},{"location":"course-details/#prerequisites","title":"Prerequisites","text":"<ul> <li>Programming: Proficient in Python</li> <li>Machine Learning: Understanding of neural networks, backpropagation</li> <li>Mathematics: Linear algebra, calculus, statistics</li> <li>Hardware: Basic understanding of GPU computing (preferred)</li> <li>Tools: Familiarity with Git, command line, Jupyter notebooks</li> </ul>"},{"location":"schedule/","title":"Course Schedule","text":"<p>This 18-week course is structured around key themes in LLM engineering, progressing from foundational concepts to advanced applications and deployment strategies.</p>"},{"location":"schedule/#weekly-schedule","title":"Weekly Schedule","text":"Theme Week Topics LLM Foundations I 1.1 Orientation, Transformer architecture: attention, FFNs, positional and token embeddings, layer norm 1.2 Transformer Architecture - GPT 1 and 2 2.1 Tokenization, Pretraining objectives 2.2 Mixture of Experts LLM Foundation II 3.1 Case studies: State-of-the-art open-source LLM architectures (GPT OSS, Qwen, Gemma) 3.2 Scaling Laws, Emergent properties GPU Basics 4.1 GPU architecture deep dive 4.2 Parallelism: Multi GPU, Multi Node 4.3 On-Prem Hardware Stack Deep Dive Optimizing Inference 5.1 Inference Math and Bottlenecks 5.2 Efficient Attention &amp; KV Caching Efficient Inference &amp; Quantization 6.1 Quantization Fundamentals 6.2 Quantization Continued 7.1 Inference Engines and Multi GPU 7.2 Router Models &amp; Cascade Inference, Case study: Serving large models Fine-Tuning Fundamentals 8.1 Full Fine-Tuning vs. PEFT \u2014 When to Use Each 8.2 Data &amp; Instruction Tuning 9.1 Instruction tuning &amp; alignment (RLHF, DPO, RL-free) 9.2 More RL Reasoning 10.1 Reasoning &amp; Chain-of-Thought 10.2 CoT, Tree-of-Thought, Self-Consistency \u2014 Prompt Engineering as Code RAG 11.1 RAG Fundamentals - embeddings, chunking, vector DBs, hybrid search, rerankers, query rewriting 11.2 Evaluating RAG Agents 12.1 ReAct Framework: Thought \u2192 Action \u2192 Observation Tool Use &amp; Function Calling 12.2 MCP introduction 12.3 Agentic RAG, Multi Agent Orchestration, Multimodal Agents Agent Finetuning 13.1 Fine Tuning for Tool calling 13.2 Agent Evaluation &amp; Safety Demo Evaluation 14.1 Evaluation 14.2 Observability &amp; Monitoring Multimodal Models 15.1 Multi Modal Architecture: Image, Audio and Video models, Running Locally 15.2 Fine tuning multimodal models LLMs on the Edge 16.1 Edge-Optimized LLM Architectures, case studies 16.2 Edge Optimization techniques Security &amp; Privacy Engineering 17.1 Threat Model: Prompt Injection, Jailbreaking, Data Leakage Frontiers 17.2 Emerging Topics: Mamba, Qwen Next, Hybrid architectures Presentations 18.1 Student Presentations I 18.2 Student Presentations II"},{"location":"schedule/#course-progression","title":"Course Progression","text":""},{"location":"schedule/#foundation-phase-weeks-1-4","title":"Foundation Phase (Weeks 1-4)","text":"<p>Build core understanding of transformer architectures, modern LLM variants, and GPU computing fundamentals.</p>"},{"location":"schedule/#optimization-phase-weeks-5-7","title":"Optimization Phase (Weeks 5-7)","text":"<p>Learn inference optimization techniques, quantization methods, and production serving strategies.</p>"},{"location":"schedule/#fine-tuning-phase-weeks-8-10","title":"Fine-tuning Phase (Weeks 8-10)","text":"<p>Master various fine-tuning approaches from parameter-efficient methods to preference alignment.</p>"},{"location":"schedule/#applications-phase-weeks-11-13","title":"Applications Phase (Weeks 11-13)","text":"<p>Develop RAG systems and AI agents with tool-calling capabilities.</p>"},{"location":"schedule/#advanced-topics-weeks-14-17","title":"Advanced Topics (Weeks 14-17)","text":"<p>Explore evaluation, multimodal models, edge deployment, security, and emerging architectures.</p>"},{"location":"schedule/#showcase-phase-week-18","title":"Showcase Phase (Week 18)","text":"<p>Present final projects and share learnings with peers.</p>"},{"location":"schedule/#key-learning-outcomes-by-phase","title":"Key Learning Outcomes by Phase","text":"<p>Each phase builds upon previous knowledge while introducing new concepts and practical skills essential for LLM engineering in production environments.</p>"}]}