{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Large Language Models : A Hands on Approach","text":"<p>Jan - May, 2026 @ Center for Continuing Education, Indian Institute of Science</p>"},{"location":"#logistics","title":"Logistics","text":"<ul> <li>Duration: 18 weeks (Jan - May 2026)</li> <li>Format: Online, Tue and Thu, 7:30 - 9:00 PM IST</li> <li>Contact: TBA</li> </ul>"},{"location":"#registration","title":"Registration","text":"<p>To register please visit CCE Webpage for the course.</p>"},{"location":"#course-description","title":"Course Description","text":"<p>LLMs have become mainstay of NLP and are transforming every domain, from software development, research, and business intelligence to education. However, deploying them efficiently remains a specialized engineering challenge.</p> <p>This course provides an engineering-focused exploration of Large Language Models (LLMs). Participants will go from understanding transformer architectures and GPU internals to mastering fine-tuning, inference optimization, and large-scale deployment across GPUs, clusters, and edge devices. Through a theory-to-practice approach, including case studies, hands-on labs, and projects, learners will cover key topics such as model architecture, fine-tuning techniques, inference optimization, serving strategies, and applications in retrieval-augmented generation (RAG) and agentic systems.</p>"},{"location":"#learning-outcomes","title":"Learning Outcomes","text":"<p>By the end of this course, participants will be able to:</p> <ul> <li>Understand LLM Architecture: Master transformer architectures, attention mechanisms, and modern LLM variants (GPT-OSS, Qwen, Gemma, etc.).</li> <li>Optimize Inference: Implement efficient inference strategies including quantization, KV caching, and serving via inference engines like vLLM.</li> <li>Fine-tune Models: Apply various fine-tuning techniques including LoRA, QLoRA, instruction tuning, and preference alignment.</li> <li>Build RAG Systems: Design and implement Retrieval-Augmented Generation pipelines.</li> <li>Develop AI Agents: Create tool-using agents with the ReAct framework.</li> <li>Deploy at Scale: Set up production-ready LLM serving infrastructure with cost optimization.</li> <li>Multimodal Models: Work with vision-language models and speech.</li> <li>Evaluation: Understand evaluation strategies for LLMs and RAG systems.</li> </ul>"},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Proficiency in Python and familiarity with any deep learning framework (PyTorch preferred).  </li> <li>Basic understanding of neural networks.  </li> <li>Working knowledge of Linux, Docker, and Git.  </li> <li>Optional but recommended: experience with GPU computing.</li> </ul>"},{"location":"#course-navigation","title":"Course Navigation","text":"<ul> <li> <p>Weekly Schedule</p> </li> <li> <p>Assignments and Labs</p> </li> </ul>"},{"location":"assignments/","title":"Assignments and Labs","text":"<p>TBD</p>"},{"location":"course-details/","title":"Course Details","text":""},{"location":"course-details/#course-overview","title":"Course Overview","text":"<p>This is a comprehensive hands-on course on Large Language Models (LLMs) designed specifically for industry professionals. The course covers the complete spectrum of LLM engineering, from foundational concepts to advanced deployment and optimization strategies.</p>"},{"location":"course-details/#course-objectives","title":"Course Objectives","text":"<p>By the end of this course, participants will be able to:</p> <ul> <li>Understand LLM Architecture: Master transformer architectures, attention mechanisms, and modern LLM variants (GPT, BERT, Mixtral, etc.)</li> <li>Optimize Inference: Implement efficient inference strategies including quantization, KV caching, and multi-GPU serving</li> <li>Fine-tune Models: Apply various fine-tuning techniques including LoRA, QLoRA, instruction tuning, and preference alignment</li> <li>Build RAG Systems: Design and implement Retrieval-Augmented Generation pipelines with vector databases and evaluation</li> <li>Develop AI Agents: Create tool-using agents with ReAct frameworks and multi-agent orchestration</li> <li>Deploy at Scale: Set up production-ready LLM serving infrastructure with cost optimization</li> <li>Handle Multimodal AI: Work with vision-language models and speech integration</li> <li>Ensure Security: Implement security measures against prompt injection and other LLM-specific threats</li> </ul>"},{"location":"course-details/#course-structure","title":"Course Structure","text":"<p>The course is organized into 18 weeks covering 8 major themes:</p>"},{"location":"course-details/#1-llm-foundations-weeks-1-3","title":"1. LLM Foundations (Weeks 1-3)","text":"<ul> <li>Transformer architecture deep dive</li> <li>Tokenization and pretraining objectives</li> <li>Modern architectures (GPT, Qwen, Gemma)</li> <li>Scaling laws and emergent properties</li> </ul>"},{"location":"course-details/#2-gpu-infrastructure-week-4","title":"2. GPU &amp; Infrastructure (Week 4)","text":"<ul> <li>GPU architecture and CUDA programming</li> <li>Multi-GPU and multi-node parallelism</li> <li>Hardware selection and cluster building</li> </ul>"},{"location":"course-details/#3-inference-optimization-weeks-5-7","title":"3. Inference Optimization (Weeks 5-7)","text":"<ul> <li>Inference bottlenecks and profiling</li> <li>Quantization techniques (INT8, INT4, GPTQ, AWQ)</li> <li>Inference engines (vLLM, TensorRT-LLM)</li> <li>Multi-GPU serving strategies</li> </ul>"},{"location":"course-details/#4-fine-tuning-weeks-8-10","title":"4. Fine-tuning (Weeks 8-10)","text":"<ul> <li>Parameter-efficient fine-tuning (LoRA, QLoRA)</li> <li>Instruction tuning and data curation</li> <li>Preference alignment (RLHF, DPO)</li> <li>Reasoning and chain-of-thought training</li> </ul>"},{"location":"course-details/#5-rag-systems-week-11","title":"5. RAG Systems (Week 11)","text":"<ul> <li>Vector databases and hybrid search</li> <li>RAG evaluation and optimization</li> <li>Graph RAG and advanced retrieval</li> </ul>"},{"location":"course-details/#6-ai-agents-weeks-12-13","title":"6. AI Agents (Weeks 12-13)","text":"<ul> <li>ReAct framework and tool calling</li> <li>Agent fine-tuning and evaluation</li> <li>Multi-agent orchestration</li> </ul>"},{"location":"course-details/#7-advanced-topics-weeks-14-17","title":"7. Advanced Topics (Weeks 14-17)","text":"<ul> <li>Model evaluation and monitoring</li> <li>Multimodal models (vision, audio)</li> <li>Edge deployment and tiny models</li> <li>Security and privacy engineering</li> <li>Emerging architectures (Mamba, hybrid models)</li> </ul>"},{"location":"course-details/#8-student-presentations-week-18","title":"8. Student Presentations (Week 18)","text":"<ul> <li>Project showcases and peer learning</li> </ul>"},{"location":"course-details/#course-timings","title":"Course Timings","text":"<ul> <li>Duration: 18 weeks</li> <li>Format: Weekly sessions with hands-on labs</li> <li>Target Audience: Industry professionals and advanced practitioners</li> <li>Prerequisites: Python programming, basic ML knowledge</li> <li>Level: Advanced/Professional</li> </ul>"},{"location":"course-details/#hands-on-approach","title":"Hands-on Approach","text":"<p>Each week includes:</p> <ul> <li>Theory Sessions: Core concepts and architecture deep dives</li> <li>Practical Labs: Implementation exercises and code walkthroughs</li> <li>Case Studies: Real-world examples and industry applications</li> <li>Projects: Progressive building of LLM applications</li> </ul>"},{"location":"course-details/#key-tools-technologies","title":"Key Tools &amp; Technologies","text":"<ul> <li>Frameworks: PyTorch, Hugging Face Transformers, vLLM</li> <li>Infrastructure: CUDA, NCCL, DeepSpeed, FSDP</li> <li>Deployment: Docker, Kubernetes, cloud platforms</li> <li>Monitoring: Prometheus, Grafana, PyTorch Profiler</li> <li>Development: Python, Jupyter, Git, MkDocs</li> </ul>"},{"location":"course-details/#assessment","title":"Assessment","text":"<ul> <li>Weekly hands-on assignments (60%)</li> <li>Mid-term project (20%)</li> <li>Final project presentation (20%)</li> </ul>"},{"location":"course-details/#prerequisites","title":"Prerequisites","text":"<ul> <li>Programming: Proficient in Python</li> <li>Machine Learning: Understanding of neural networks, backpropagation</li> <li>Mathematics: Linear algebra, calculus, statistics</li> <li>Hardware: Basic understanding of GPU computing (preferred)</li> <li>Tools: Familiarity with Git, command line, Jupyter notebooks</li> </ul>"},{"location":"schedule/","title":"Course Schedule","text":""},{"location":"schedule/#weekwise-schedule","title":"Weekwise Schedule","text":"<p>Tentative and subject to change</p> Theme Week Topics LLM Foundations I 1.1 Orientation, Transformer architecture 1.2 Transformer Architecture - GPT 1 and 2 2.1 Tokenization, Pretraining objectives 2.2 Mixture of Experts LLM Foundation II 3.1 Case studies: State-of-the-art open-source LLM architectures 3.2 Scaling Laws, Emergent properties GPU Basics 4.1 GPU architecture deep dive 4.2 Parallelism: Multi GPU, Multi Node 5.1 On-Prem Hardware Stack Deep Dive Inference 5.2 Inference Strategies 6.1 Inference Math and Bottlenecks 6.2 Efficient Attention &amp; KV Caching Efficient Inference &amp; Quantization 7.1 Quantization Fundamentals 7.2 Inference Engines and Multi GPU Fine-Tuning Fundamentals 8.1 Full Fine-Tuning vs. PEFT \u2014 When to Use Each 8.2 Instruction Tuning 9.1 Alignment (RLHF, DPO etc) 9.2 More RL Reasoning 10.1 Reasoning &amp; Chain-of-Thought 10.2 CoT, Tree-of-Thought, Self-Consistency \u2014 Prompt Engineering as Code RAG 11.1 RAG Fundamentals - Context-engineering, embeddings, search and rerankers 11.2 Evaluating RAG Agents 12.1 ReAct Framework: Thought \u2192 Action \u2192 Observation Tool Use &amp; Function Calling 12.2 MCP introduction 12.3 Agentic RAG, Multi Agent Orchestration, Multimodal Agents Agent Finetuning 13.1 Fine Tuning for Tool calling 13.2 Agent Evaluation &amp; Safety Evaluation 14.1 Evaluation 14.2 Observability &amp; Monitoring Multimodal Models 15.1 Multi Modal Architecture: Image, Audio and Video models, Running Locally 15.2 Fine tuning multimodal models LLMs on the Edge 16.1 Edge-Optimized LLM Architectures, case studies 16.2 Edge Optimization techniques Security &amp; Privacy Engineering 17.1 Threat Model: Prompt Injection, Jailbreaking, Data Leakage Frontiers 17.2 Emerging Topics: Mamba, Qwen Next, Hybrid architectures Presentations 18.1 Student Presentations I 18.2 Student Presentations II"},{"location":"schedule/#materials","title":"Materials","text":"<ul> <li>Lecture slides and notes will be shared here as class progresses.</li> </ul>"},{"location":"slides/01_orientation/00-intro/","title":"00 intro","text":""},{"location":"slides/01_orientation/00-intro/#large-language-models-a-hands-on-approach","title":"Large Language Models : A Hands on Approach","text":""},{"location":"slides/01_orientation/00-intro/#course-orientation","title":"Course Orientation","text":""},{"location":"slides/01_orientation/00-intro/#course-overview","title":"Course Overview","text":"<ul> <li>Duration: ~18 Weeks</li> <li>Format: 2 sessions per week (Tue &amp; Thu, 7:00\u20138:30 PM IST)</li> <li>Style: Lectures, demos and labs</li> <li>Assessment: Quizzes, assignments, and final project/presentation</li> <li>Credits: 3 + 1 (Credits added in ABC)</li> </ul>"},{"location":"slides/01_orientation/00-intro/#why-this-course","title":"Why This Course?","text":"<p>LLMs have changed how we build intelligent applications.</p> <p>This course aims to bridge the gap between:</p> <p>Using LLMs via APIs \u2192 Engineering LLM systems</p>"},{"location":"slides/01_orientation/00-intro/#learning-outcomes","title":"Learning Outcomes","text":"<ul> <li>Understand LLM architectures</li> <li>Fine-tune models for specific domains and tasks</li> <li>Optimize inference for cost and latency</li> <li>Build robust LLM-powered applications (RAG, agents, tool use)</li> <li>Evaluate, debug, and improve model performance</li> <li>Leverage open-source ecosystems for LLM engineering</li> </ul>"},{"location":"slides/01_orientation/00-intro/#what-this-course-will-not-cover","title":"What This Course Will NOT Cover","text":"<ul> <li>Mathematical foundations</li> <li>Pretraining LLMs from scratch</li> <li>State of the art in research</li> <li>Data engineering pipelines</li> <li>Ethics and societal impacts</li> </ul>"},{"location":"slides/01_orientation/00-intro/#prerequisites","title":"Prerequisites","text":"<ul> <li>Programming: Python proficiency</li> <li>ML Basics: Understanding of neural networks and deep learning</li> <li>Helpful: Familiarity with PyTorch, basic NLP concepts</li> </ul>"},{"location":"slides/01_orientation/00-intro/#tools-and-resources","title":"Tools and Resources","text":""},{"location":"slides/01_orientation/00-intro/#software-stack","title":"Software Stack","text":"<ul> <li>Python</li> <li>PyTorch</li> <li>HuggingFace Ecosystem(models, libraries and datasets)</li> </ul>"},{"location":"slides/01_orientation/00-intro/#compute-stack","title":"Compute Stack","text":"Option Cost Notes Google Colab Free Limited GPU, good for experiments GCP Free Credits Free $300 for new accounts Colab Pro+ Paid More GPU time, better GPUs RunPod Paid Flexible, longer runs Lightning AI Paid Easy setup, good UX <p>Free options suffice for most of our course needs</p>"},{"location":"slides/01_orientation/00-intro/#evaluation-criteria","title":"Evaluation Criteria","text":"Component Weight Notes Quizzes 25% 5 short in-class quizzes Assignments 30% 2 assignments (Mid Feb and Mid Mar) Final Project/Presentation 40% Mar 3rd week onwards Participation 5% In-class and forum engagement <p>--</p>"},{"location":"slides/01_orientation/00-intro/#abc","title":"ABC","text":"<ul> <li>Academic Bank of Credits</li> <li>Credits will be added to your ABC account after successful course completion</li> <li>More information will be shared during the course</li> </ul>"},{"location":"slides/01_orientation/00-intro/#final-project-option","title":"Final Project Option","text":"<p>Individual or group project (max 2).</p> <p>Build an end-to-end LLM application:</p> <ul> <li>Model fine-tuning</li> <li>Inference optimization</li> <li>LLM Application (RAG, Agents)</li> <li>Open source contribution</li> <li>Other ideas welcome too</li> </ul> <p>Deliverables: Documentation + Demo / Presentation</p>"},{"location":"slides/01_orientation/00-intro/#presentation-option","title":"Presentation Option","text":"<ul> <li>Topic: Novel concept not covered in class</li> <li>Format: Research paper or case study</li> <li>Deliverables: Slides or summary document</li> <li>Evaluation: Depth of understanding, presentation quality</li> <li>Duration: 10 minutes, 8-10 slides</li> </ul>"},{"location":"slides/01_orientation/00-intro/#course-roadmap-12","title":"Course Roadmap (1/2)","text":"Module Topic 1 LLM Foundations - Transformers, GPT-2, Modern Architectures, MoE, OSS Models 2 GPUs - Architecture and Programming,  Multi-GPU Parallelism, Hardware Stack 3 Inference - Sampling, KV Caching, Quantization, Speculative Decoding, Model Serving 4 Fine-Tuning - SFT, PEFT (LoRA, QLoRA etc),RLHF (DPO, GRPO etc), Distillation 5 Reasoning - Chain-of-Thought (CoT), Test Time Scaling, Finetuning"},{"location":"slides/01_orientation/00-intro/#course-roadmap-22","title":"Course Roadmap (2/2)","text":"Module Topic 6 RAG &amp; Agents - RAG Fundamentals, ReAct, Tools, Protocols, Agents, Finetuning 7 Evaluation - Moderl Evaluations, Benchmarks, LLM-as-a-Judge 8 Multimodal - Multimodal Architectures, Finetuning 9 Edge Deployment - Edge architectures, Optimization and Deployment"},{"location":"slides/01_orientation/00-intro/#making-the-best-of-this-course","title":"Making the Best of This Course","text":"<ul> <li>Participate actively: Ask questions in class, engage in discussions</li> <li>Practice hands-on: Run the code, experiment with parameters</li> <li>Use AI tools: Leverage AI assistants for coding and learning</li> <li>Stay connected: Use forum for async discussions</li> </ul>"},{"location":"slides/01_orientation/00-intro/#qa","title":"Q&amp;A","text":""},{"location":"slides/01_orientation/00-intro/#any-questions","title":"Any questions?","text":""},{"location":"slides/01_orientation/notes/","title":"Notes","text":""},{"location":"slides/01_orientation/notes/#course-overview-and-structure","title":"Course Overview and Structure","text":"<ul> <li>Duration: ~18 Weeks</li> <li>Format: 2 sessions per week, Tuesday and Thursday (7:00 PM to 8:30 PM IST)</li> <li>Style: Mix of lectures, demos, labs, and projects</li> <li>Assessment: Assignments, quizzes, and final project</li> </ul>"},{"location":"slides/01_orientation/notes/#why-this-course","title":"Why this course?","text":"<p>LLMs have changed how we build intelligent applications. This course bridges the gap between using LLMs via APIs and engineering LLM systems you can deploy and operate in production.</p>"},{"location":"slides/01_orientation/notes/#learning-outcomes","title":"Learning Outcomes","text":"<p>You will learn how to: - Understand LLM architectures - Fine-tune models for specific domains and tasks - Optimize inference for cost and latency - Build robust LLM-powered applications (RAG, agents, tool use) - Evaluate, debug, and improve model performance systematically - Make use open-source ecosystems for LLM engineering</p>"},{"location":"slides/01_orientation/notes/#what-this-course-will-not-cover","title":"What This Course Will NOT Cover","text":"<ul> <li>Mathematical foundations</li> <li>Pretraining LLMs from scratch </li> <li>State of the art in research</li> <li>Ethics and societal impacts</li> <li>Data engineering pipelines</li> </ul>"},{"location":"slides/01_orientation/notes/#prerequisites","title":"Prerequisites","text":"<ul> <li>Programming: Python proficiency</li> <li>ML Basics: Understanding of neural networks and deep learning concepts</li> <li>Helpful: Familiarity with PyTorch, basic NLP concepts</li> </ul>"},{"location":"slides/01_orientation/notes/#tools-and-resources","title":"Tools and Resources","text":""},{"location":"slides/01_orientation/notes/#software","title":"Software","text":"<ul> <li>Python</li> <li>PyTorch</li> <li>HuggingFace (transformers, datasets, peft, trl)</li> </ul>"},{"location":"slides/01_orientation/notes/#hardware-compute","title":"Hardware / Compute","text":"Option Cost Notes Google Colab Free Limited GPU time, good for experiments GCP Free Credits Free $300 credits for new accounts Colab Pro+ Paid More GPU time, better GPUs RunPod Paid Flexible, good for longer runs Lightning AI Paid Easy setup, good UX <ul> <li>*first two will suffice for most assignments and labs, for final projects you might need paid options depending on your project scope</li> </ul>"},{"location":"slides/01_orientation/notes/#evaluation-criteria","title":"Evaluation Criteria","text":"Component Weight Description Quizzes 20% 4\u20135 short in-class quizzes Assignments 30% 2 assignments covering key modules Final Project or Presentation 50% Project : Build and deploy an end-to-end LLM application (agents or RAG/systems), or improve an existing approach (e.g., inference optimization or fine-tuning). Presentation : Cover a novel concept not covered in the class (rsearch paper or case study)"},{"location":"slides/01_orientation/notes/#final-project-or-presentation-details","title":"Final Project or Presentation Details","text":"<p>Project Option: - Build an end-to-end LLM application - Options: Fine-tuned model, RAG system, agent application - Deliverables: documentation, demo and  - Evaluation: Functionality, </p> <p>Presentation Option: - Topic: Novel concept not covered in class - Deliverables: Slides or summary document - Evaluation: Depth of understanding, Presentation quality</p>"},{"location":"slides/01_orientation/notes/#grading-policy","title":"Grading Policy","text":"<p>TODO (@yknegi): Add grading policy details ABC </p>"},{"location":"slides/01_orientation/notes/#course-roadmap","title":"Course Roadmap","text":"<p>TODO (@yknegi): remove course roadmap if not needed | Module | Topic | Key Concepts | |:---|:---|:---| | 1 | LLM Foundations | Transformers, GPT-2 style, Modern Architectures, Mixture of Experts, Open-Source LLMs | | 2 | GPUs | GPU Architecture, Multi-GPU Parallelism, Hardware Stack | | 3 | Optimizing Inference | Sampling, Memory Optimization (KV Caching), Quantization, Inference Engines (vLLM, SGLang) | | 4 | Fine-Tuning | Full Fine-Tuning, PEFT (LoRA, QLoRA), Instruction Tuning, Alignment (DPO, RLHF), Distillation, Reasoning | | 5 | RAG and Agents | RAG Fundamentals, Agentic RAG, ReAct, Tools, MCP, Multi-Agent Systems | | 6 | Evaluation | Frameworks, MMLU, LMSYS Arena, LLM-as-a-Judge, Error Analysis | | 7 | Multimodal | Multimodal Architectures, Visual Instruction Fine-tuning | | 8 | Edge Deployment | Edge Architectures, llama.cpp, Optimization, and Deployment |</p>"},{"location":"slides/01_orientation/notes/#making-the-best-of-this-course","title":"Making the Best of This Course","text":"<ul> <li>Participate actively: Ask questions, engage in discussions</li> <li>Practice hands-on: Run the code, experiment with parameters</li> <li>Stay connected: Use forum for async discussions</li> </ul>"},{"location":"slides/01_orientation/notes/#qa","title":"Q&amp;A","text":"<p>Any questions?</p>"},{"location":"slides/02_llm_basics/notes/","title":"Large Language Models (LLMs) - Introduction","text":""},{"location":"slides/02_llm_basics/notes/#what-is-a-large-language-model","title":"What is a Large Language Model?","text":"<p>A Large Language Model is a deep neural network trained on extensive datasets containing text from books, articles, websites, and other sources sometimes encompassing large portions of the entire publicly available text on the internet.</p> <p>LLMs have remarkable capabilities in understanding, generating, and interpreting human language, code, and even multimodal data (text, images, audio).</p> <p></p> <p>A language model predicts the next word in a sequence given the preceding words. The models are trained to predict the next word in a sentence, given the preceding words. This training process allows them to learn grammar, facts about the world, reasoning abilities, and even some level of common sense.</p>"},{"location":"slides/02_llm_basics/notes/#mathematical-definition","title":"Mathematical Definition","text":"<p>Mathematically, a language model estimates the probability distribution over a sequence of tokens. The joint probability of a sequence $W = (w_1, w_2, \\dots, w_T)$ is factorized using the chain rule of probability:</p> <p>$$ P(w_1, w_2, \\dots, w_T) = \\prod_{t=1}^{T} P(w_t \\mid w_1, \\dots, w_{t-1}) $$</p> <p>The core objective is to learn the conditional probability of the next token $w_t$ given the history $w_{&lt;t}$:</p> <p>$$ P(w_t \\mid w_{&lt;t}) $$</p> <p>Where: - $w_t$ is the token to be predicted at step $t$. - $w_{&lt;t} = (w_1, \\dots, w_{t-1})$ is the context of preceding tokens. - $P(w_t \\mid w_{&lt;t})$ is the probability of the next word $w_t$ conditioned on the sequence of previous words.</p> <p> [Image source][1]</p> <p>The \"large\" aspect refers to the model's size, which is typically measured in terms of: - Parameter count: Typically &gt;1B parameters (e.g., GPT-3: 175B, Llama 3: 8B-405B) [2] - Training data: Trillions of tokens (e.g., Llama 3 trained on 15T tokens) [3] - Compute: Thousands of GPU-hours for training</p> <p>These models can have billions or even trillions of parameters, enabling them to capture complex patterns in language.</p> <p></p>"},{"location":"slides/02_llm_basics/notes/#some-applications-of-large-language-models","title":"Some Applications of Large Language Models","text":"<ol> <li> <p>Chatbots and Virtual Assistants: LLMs power conversational agents that can understand and respond to user queries in a human-like manner.</p> </li> <li> <p>Natural Language Processing (NLP): LLMs are widely used in NLP tasks such as sentiment analysis, named entity recognition, and text classification.</p> </li> <li> <p>Content Generation: LLMs can generate coherent and contextually relevant text, making them useful for content creation, summarization, and translation.</p> </li> <li> <p>Code Generation: LLMs can assist in generating code snippets, automating repetitive coding tasks, and even debugging code.</p> </li> <li> <p>Education and Tutoring: LLMs can provide personalized learning experiences, answer questions, and assist with homework.</p> </li> <li> <p>Creative Writing: LLMs can collaborate with authors by generating ideas, suggesting plot twists, and even writing poetry or stories.</p> </li> <li> <p>Research Assistance: LLMs can help researchers by summarizing papers, generating hypotheses, and even conducting literature reviews.</p> </li> </ol> <p>These applications demonstrate the versatility and potential of Large Language Models in transforming how we interact with technology and information.</p>"},{"location":"slides/02_llm_basics/notes/#why-build-your-own-llm","title":"Why build your own LLM?","text":"<ol> <li>Domain-specific models - can outperform general models like ChatGPT, Claude, etc.    e.g., Models trained for law, medical question answering, etc.</li> <li>Cost-effectiveness - cheaper to run your own LLM than to use cloud-based services.</li> <li>Data Privacy - you have control over the data used to train the model, prevent sensitive data being sent to model providers</li> <li>Customized Deployment - you can deploy the model on your own infrastructure or edge devices</li> <li>Autonomy - you can control the model's behavior, update it and fix it</li> </ol>"},{"location":"slides/02_llm_basics/notes/#building-and-using-llms","title":"Building and Using LLMs","text":"<p>Building and using LLMs involves several steps, including data collection, preprocessing, model selection, training, and evaluation.</p> <p></p> <p>1. Data Collection and Preprocessing: Data collection is the first step in building an LLM. It involves gathering a large and diverse dataset of text data from various sources, such as books, articles, websites, and social media platforms. Typically several terabytes or petabytes of data are required to train a large LLM.</p> <p>2. Pretraining: Pretraining involves training a large language model on a large dataset of text data. The model is trained to predict the next token in a sequence of tokens, given the preceding tokens. This training process allows the model to learn the patterns and relationships in the data, which can then be used for a variety of downstream tasks.</p> <p>3. Fine-tuning: Fine-tuning adapts a pretrained model for specific tasks and aligns it to human preferences.</p> <p>4. Inference: Inference involves using the model to generate text based on a given input. The model is used to generate text based on a given input, which can be used for a variety of downstream tasks.</p> <p>In our course, we'll not cover data collection and pretraining from scratch, but will focus on fine-tuning, inference optimization, and building applications using LLMs. Pre-training and data engineering require significant resources and are beyond the scope of this course. We shall use existing pretrained models from open-source repositories like HuggingFace and focus on engineering aspects of LLMs.</p>"},{"location":"slides/02_llm_basics/notes/#code-example-next-token-prediction-with-huggingface","title":"Code Example: Next-Token Prediction with HuggingFace","text":"<pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\ninputs = tokenizer(\"The capital of France is\", return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_new_tokens=5)\nprint(tokenizer.decode(outputs[0]))\n# Output: \"The capital of France is Paris, and the\"\n</code></pre>"},{"location":"slides/02_llm_basics/notes/#demo","title":"Demo","text":"<ul> <li>Text Generation</li> <li>Vision-Language Model</li> <li>Audio Generation</li> </ul>"},{"location":"slides/02_llm_basics/notes/#exercise","title":"Exercise","text":"<ul> <li>Login to Google Colab</li> <li>Run shared notebooks</li> <li>Visit HuggingFace, explore trending models, spaces and datasets. Use this HuggingFace Tutorial as reference.</li> </ul>"},{"location":"slides/02_llm_basics/notes/#references","title":"References","text":"<ol> <li>The Illustrated GPT-2 - Jay Alammar</li> <li>Language Models are Few-Shot Learners (GPT-3) - Brown et al., 2020</li> <li>Llama 3 Technical Report - Meta AI, 2024</li> <li>Hands-On Large Language Models by Jay Alammar, Maarten Grootendorst</li> </ol>"},{"location":"slides/02_llm_basics/slides/","title":"Slides","text":""},{"location":"slides/02_llm_basics/slides/#large-language-models-a-hands-on-approach","title":"Large Language Models : A Hands on Approach","text":""},{"location":"slides/02_llm_basics/slides/#llm-basics","title":"LLM Basics","text":""},{"location":"slides/02_llm_basics/slides/#what-is-a-large-language-model-llm","title":"What is a Large Language Model (LLM)?","text":"<p>A deep neural network trained on extensive text datasets from books, articles, websites\u2014sometimes encompassing large portions of the entire publicly available internet.</p> <p></p>"},{"location":"slides/02_llm_basics/slides/#llm-capabilities","title":"LLM Capabilities","text":"<ul> <li>Understanding and generating human language</li> <li>Code generation and debugging</li> <li>Multimodal data processing (text, images, audio)</li> <li>Reasoning and common sense understanding</li> </ul>"},{"location":"slides/02_llm_basics/slides/#language-modeling","title":"Language Modeling","text":"<p>A language model predicts the next word given preceding words.</p> <p></p> <p>[Image source][1]</p>"},{"location":"slides/02_llm_basics/slides/#mathematical-definition","title":"Mathematical Definition","text":"<p>Joint probability of a sequence using chain rule:</p> <p>$$ P(w_1, w_2, \\dots, w_T) = \\prod_{t=1}^{T} P(w_t \\mid w_1, \\dots, w_{t-1}) $$</p> <p>Core objective: learn the conditional probability:</p> <p>$$ P(w_t \\mid w_{&lt;t}) $$</p> <p>where - $w_t$ \u2014 token to be predicted at step $t$ - $w_{&lt;t} = (w_1, \\dots, w_{t-1})$ \u2014 context of preceding tokens - $P(w_t \\mid w_{&lt;t})$ \u2014 probability of next word conditioned on previous words</p>"},{"location":"slides/02_llm_basics/slides/#what-makes-llms-large","title":"What Makes LLMs \"Large\"?","text":"Aspect Scale Parameters &gt;1B (GPT-3: 175B, Llama 3: 8B-405B) [2] Training Data Trillions of tokens (Llama 3: 15T) [3] Compute Thousands of GPU-hours"},{"location":"slides/02_llm_basics/slides/#applications-of-llms","title":"Applications of LLMs","text":"<ol> <li>Chatbots &amp; Virtual Assistants - conversational agents</li> <li>NLP Tasks \u2014 sentiment analysis, NER, classification</li> <li>Content Generation \u2014 summarization, translation</li> <li>Code Generation \u2014 snippets, debugging, automation</li> <li>Education &amp; Tutoring \u2014 personalized learning</li> <li>Creative Writing \u2014 ideas, plot twists, poetry</li> <li>Research Assistance \u2014 paper summarization, literature reviews</li> </ol>"},{"location":"slides/02_llm_basics/slides/#why-build-your-own-llm","title":"Why Build Your Own LLM?","text":"<ol> <li>Domain-specific models \u2014 outperform general models (law, medical)</li> <li>Cost-effectiveness \u2014 cheaper than cloud APIs at scale</li> <li>Data Privacy \u2014 control over sensitive data</li> <li>Custom Deployment \u2014 on-premise or edge devices</li> <li>Autonomy \u2014 control behavior, updates, fixes</li> </ol>"},{"location":"slides/02_llm_basics/slides/#building-llms-overview","title":"Building LLMs: Overview","text":"<ol> <li>Data Collection and Preprocessing</li> <li>Pretraining </li> <li>Fine-tuning</li> <li>Inference</li> </ol>"},{"location":"slides/02_llm_basics/slides/#course-focus","title":"Course Focus","text":"<p>We will focus on: - Fine-tuning existing models - Inference optimization - Building applications with LLMs</p> <p>Using state of the art open source models available openly.</p>"},{"location":"slides/02_llm_basics/slides/#demo","title":"Demo","text":"<ul> <li>Text Generation</li> <li>Vision-Language Model</li> </ul>"},{"location":"slides/02_llm_basics/slides/#warm-up-exercise","title":"Warm Up Exercise","text":"<ol> <li>Login to Google Colab</li> <li>Run shared notebooks</li> <li>Explore HuggingFace:</li> <li>Trending models</li> <li>Spaces</li> <li>Datasets</li> </ol> <p>Reference: HuggingFace Tutorial</p>"},{"location":"slides/02_llm_basics/slides/#references","title":"References","text":"<ol> <li>The Illustrated GPT-2 - Jay Alammar</li> <li>GPT-3 Paper - Brown et al., 2020</li> <li>Llama 3 Report - Meta AI, 2024</li> </ol>"},{"location":"slides/02_llm_basics/slides/#thank-you","title":"Thank You","text":"<p>Questions?</p>"},{"location":"slides/03_tokenization/slides/","title":"Slides","text":""},{"location":"slides/03_tokenization/slides/#large-language-models-a-hands-on-approach","title":"Large Language Models: A Hands on Approach","text":""},{"location":"slides/03_tokenization/slides/#tokenization","title":"Tokenization","text":""},{"location":"slides/03_tokenization/slides/#topics-covered","title":"Topics Covered","text":"<ul> <li>Preprocessing text data for LLMs</li> <li>Tokenization techniques</li> <li>Byte Pair Encoding (BPE)</li> <li>Converting tokens into vectors</li> </ul>"},{"location":"slides/03_tokenization/slides/#models-of-the-week","title":"Models of the Week","text":""},{"location":"slides/03_tokenization/slides/#translategemma","title":"TranslateGemma","text":"<ul> <li>SOTA Open weights multilingual translation model</li> <li>Multimodal capabilities: text + images for context</li> <li>4B, 12B and 27B versions</li> </ul>"},{"location":"slides/03_tokenization/slides/#medgemma","title":"MedGemma","text":"<ul> <li>SOTA Open weights medical imaging and document understanding model</li> <li>CT/MRI/Histopathology Processing, Medical Document Understanding, Multi-domain Classification</li> </ul>"},{"location":"slides/03_tokenization/slides/#glm-47-flash","title":"GLM-4.7-Flash","text":"<ul> <li>Coding, Tool use and Reasoning abilities</li> <li>30B-A3B MoE model</li> </ul>"},{"location":"slides/03_tokenization/slides/#motivation","title":"Motivation","text":"<p>Why does tokenization matter?</p> <ul> <li>Cost: Billing is per token, not per word</li> <li>Context limits: Tokenization decides what fits vs what gets truncated</li> <li>Reasoning failures: Try \"Say Nameeee\" vs \"Say Name eee\" in DeepSeek</li> <li>Multilingual bias: Some languages need more tokens for same meaning</li> </ul>"},{"location":"slides/03_tokenization/slides/#data-preprocessing-pipeline","title":"Data Preprocessing Pipeline","text":"<ul> <li>Cannot feed raw text directly into LLMs. </li> <li>Need numerical representations.</li> </ul>"},{"location":"slides/03_tokenization/slides/#graph-lr-araw-text-btokenization-b-ctoken-ids-c-dembedding-layer-d-einput-embeddings-e-fllm","title":"<pre><code>graph LR\n    A[Raw Text] --&gt; B[Tokenization]\n    B --&gt; C[Token IDs]\n    C --&gt; D[Embedding Layer]\n    D --&gt; E[Input Embeddings]\n    E --&gt; F[LLM]\n</code></pre>","text":""},{"location":"slides/03_tokenization/slides/#the-full-pipeline","title":"The Full Pipeline","text":"<p>Unified Architecture</p> <p></p>"},{"location":"slides/03_tokenization/slides/#what-is-tokenization","title":"What Is Tokenization?","text":"<p>Tokenization breaks text into smaller units called tokens.</p> <p>Tokens can be words, subwords, or characters.</p> <p>Example: \"The transformer architecture is revolutionary.\"</p> Approach Tokens Count Words <code>[\"The\", \"transformer\", \"architecture\", \"is\", \"revolutionary\", \".\"]</code> 6 Characters <code>[\"T\", \"h\", \"e\", \" \", \"t\", \"r\", \"a\", ...]</code> 45 Subwords <code>[\"The\", \" transform\", \"er\", \" architecture\", \" is\", \" revolution\", \"ary\", \".\"]</code> 8"},{"location":"slides/03_tokenization/slides/#real-world-example-gpt-2-tokenizer","title":"Real World Example: GPT-2 Tokenizer","text":"<p>Try it yourself</p> <p></p>"},{"location":"slides/03_tokenization/slides/#tokenization-pipeline","title":"Tokenization Pipeline","text":""},{"location":"slides/03_tokenization/slides/#tokenization-spectrum","title":"Tokenization Spectrum","text":"<pre><code>Bytes \u2192 Characters \u2192 Subwords \u2192 Words\n  \u2191                              \u2191\n 256 tokens                     Millions of tokens\n Long sequences                 Short sequences\n</code></pre> <p>Trade-off between vocabulary size and sequence length.</p>"},{"location":"slides/03_tokenization/slides/#word-level-tokenization","title":"Word-Level Tokenization","text":"<p>Split text into words based on spaces and punctuation.</p> <pre><code>import re\ntext = \"Hello, world. This, is a test.\"\nresult = re.split(r'(\\s)', text)\n# ['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n</code></pre> <p></p>"},{"location":"slides/03_tokenization/slides/#word-level-pros-and-cons","title":"Word-Level: Pros and Cons","text":"<p>Advantages: - Short sequences (one token per word) - Linguistically intuitive - Fast attention (fewer tokens)</p> <p>Disadvantages: - Huge vocabulary (English needs 100K+ words) - OOV problem: Unknown words \u2192 <code>[UNK]</code> - Morphological blindness: \"run\", \"runs\", \"running\" are unrelated - Language-specific: Some languages don't use spaces</p>"},{"location":"slides/03_tokenization/slides/#character-level-tokenization","title":"Character-Level Tokenization","text":"<p>Split text into individual characters.</p> <p>Advantages: - No OOV tokens (any text can be encoded) - Tiny vocabulary (~100-300 tokens) - Handles typos and neologisms</p> <p>Disadvantages: - Very long sequences (5-10x longer) - Slow attention (more tokens) - Poor semantics (characters lack meaning) - Harder to learn word structure</p>"},{"location":"slides/03_tokenization/slides/#subword-tokenization","title":"Subword Tokenization","text":"<p>The sweet spot: split rare words, keep common words whole.</p> Word Subword Tokens Interpretation the <code>[\"the\"]</code> Common \u2192 single token transformer <code>[\"trans\", \"former\"]</code> Split into known pieces unhappiness <code>[\"un\", \"happi\", \"ness\"]</code> Morphemes preserved GPT-4 <code>[\"G\", \"PT\", \"-\", \"4\"]</code> Unknown \u2192 character fallback"},{"location":"slides/03_tokenization/slides/#popular-subword-algorithms","title":"Popular Subword Algorithms","text":"<ol> <li>Byte Pair Encoding (BPE) - Frequency-based merging</li> <li>WordPiece - Probability-based merging (BERT)</li> <li>SentencePiece - Unigram language model (LLaMA)</li> </ol>"},{"location":"slides/03_tokenization/slides/#byte-pair-encoding-bpe","title":"Byte Pair Encoding (BPE)","text":"<p>Sennrich et al. (2016)</p> <p>Core idea: Iteratively merge the most frequent pair of tokens.</p> <ol> <li>Start with vocabulary of individual characters (or bytes)</li> <li>Count all adjacent pairs in the corpus</li> <li>Merge the most frequent pair into a new token</li> <li>Repeat until reaching desired vocabulary size</li> </ol>"},{"location":"slides/03_tokenization/slides/#bpe-walkthrough","title":"BPE Walkthrough","text":"<p>Input: \"low lower lowest\"</p> <p>Step 1 - Initialize tokens: <pre><code>['l', 'o', 'w', ' ', 'l', 'o', 'w', 'e', 'r', ' ', 'l', 'o', 'w', 'e', 's', 't']\n</code></pre></p> <p>Step 2 - Count pairs: <pre><code>('l', 'o'): 3    ('o', 'w'): 3    ('w', ' '): 3\n(' ', 'l'): 2    ('e', 'r'): 1    ('e', 's'): 1\n</code></pre></p> <p>--</p>"},{"location":"slides/03_tokenization/slides/#bpe-walkthrough-continued","title":"BPE Walkthrough (continued)","text":"<p>Step 3 - Merge most frequent pair <code>('l', 'o')</code> \u2192 <code>'lo'</code> <pre><code>['lo', 'w', ' ', 'lo', 'w', 'e', 'r', ' ', 'lo', 'w', 'e', 's', 't']\n</code></pre></p> <p>Step 4 - Count pairs again: <pre><code>('lo', 'w'): 3    ('w', ' '): 3    (' ', 'lo'): 2\n</code></pre></p> <p>Step 5 - Merge <code>('lo', 'w')</code> \u2192 <code>'low'</code> <pre><code>['low', ' ', 'low', 'e', 'r', ' ', 'low', 'e', 's', 't']\n</code></pre></p> <p>--</p>"},{"location":"slides/03_tokenization/slides/#bpe-result","title":"BPE Result","text":"<p>Final tokenization: <pre><code>\"low lower lowest\" \u2192 [\"low\", \" \", \"low\", \"er\", \" \", \"low\", \"est\"]\n</code></pre></p> <p>Can encode any word: <pre><code>\"lowestness\" \u2192 [\"low\", \"est\", \"ness\"]\n</code></pre></p> <p></p>"},{"location":"slides/03_tokenization/slides/#bpe-pros-and-cons","title":"BPE: Pros and Cons","text":"<p>Advantages: - Balances vocabulary size and sequence length - Handles OOV words by breaking into subwords - Captures morphological structure</p> <p>Disadvantages: - Greedy algorithm (may not find optimal tokenization) - Training corpus dependent</p>"},{"location":"slides/03_tokenization/slides/#byte-level-bpe-gpt-2-style","title":"Byte-Level BPE (GPT-2 Style)","text":"<p>GPT-2 uses byte-level BPE:</p> <ul> <li>Start with 256 byte tokens (not Unicode characters)</li> <li>All text is UTF-8 encoded first</li> <li>Merges operate on bytes, not characters</li> <li>Avoids merges beyond word boundaries</li> </ul> <p>Advantages: - Handles any language without special tokenization - Works with emojis, rare scripts, binary data - No <code>[UNK]</code> tokens ever needed</p>"},{"location":"slides/03_tokenization/slides/#multimodal-tokenization","title":"Multimodal Tokenization","text":"<p>Modern models tokenize more than text:</p> <ul> <li>Text: Subword tokenization (BPE, WordPiece)</li> <li>Images: Patch-based (e.g., ViT) - split into 16x16 patches</li> <li>Video: Frame + patch tokenization</li> <li>Audio: Spectrogram frames</li> </ul> <pre><code>graph LR\n    T[Text Input] --&gt; TT[Text Tokenizer]\n    I[Image Input] --&gt; IT[Image Tokenizer]\n    A[Audio Input] --&gt; AT[Audio Tokenizer]\n    V[Video Input] --&gt; VT[Video Tokenizer]\n\n    TT --&gt; U[Unified Token Sequence]\n    IT --&gt; U\n    AT --&gt; U\n    VT --&gt; U\n\n    U --&gt; M[Transformer Model]\n</code></pre>"},{"location":"slides/03_tokenization/slides/#vocabulary-size-trade-offs","title":"Vocabulary Size Trade-offs","text":"<p>Smaller Vocabulary: - Faster training and inference - Lower memory usage - Longer sequences</p> <p>Larger Vocabulary: - Shorter sequences - Better semantic representation - Higher memory usage</p>"},{"location":"slides/03_tokenization/slides/#vocabulary-impact-on-parameters","title":"Vocabulary Impact on Parameters","text":"<p>$$ \\text{Embedding Matrix} = V \\times E $$</p> <p>$$ \\text{Output Layer} = H \\times V $$</p> <p>Where $V$ = vocabulary size, $E$ = embedding dimension, $H$ = hidden dimension</p>"},{"location":"slides/03_tokenization/slides/#model-vocabulary-comparison","title":"Model Vocabulary Comparison","text":"Model Vocabulary Size Tokenizer GPT-2 50,257 BPE GPT-4 ~100,000 BPE variant BERT 30,522 WordPiece LLaMA 32,000 SentencePiece Claude ~100,000 BPE variant"},{"location":"slides/03_tokenization/slides/#references","title":"References","text":"<ol> <li>Let's Build the GPT Tokenizer - Andrej Karpathy</li> <li>The Smol Training Playbook</li> <li>Neural Machine Translation of Rare Words with Subword Units - Sennrich et al., 2016</li> </ol>"},{"location":"slides/03_tokenization/slides/#thank-you","title":"Thank You","text":"<p>Questions?</p>"},{"location":"slides/reveal/css/theme/","title":"Index","text":""},{"location":"slides/reveal/css/theme/#dependencies","title":"Dependencies","text":"<p>Themes are written using Sass to keep things modular and reduce the need for repeated selectors across files. Make sure that you have the reveal.js development environment installed before proceeding: https://revealjs.com/installation/#full-setup</p>"},{"location":"slides/reveal/css/theme/#creating-a-theme","title":"Creating a Theme","text":"<p>To create your own theme, start by duplicating a <code>.scss</code> file in /css/theme/source. It will be automatically compiled from Sass to CSS (see the gulpfile) when you run <code>npm run build -- css-themes</code>.</p> <p>Each theme file does four things in the following order:</p> <ol> <li> <p>Include /css/theme/template/mixins.scss Shared utility functions.</p> </li> <li> <p>Include /css/theme/template/settings.scss Declares a set of custom variables that the template file (step 4) expects. Can be overridden in step 3.</p> </li> <li> <p>Override This is where you override the default theme. Either by specifying variables (see settings.scss for reference) or by adding any selectors and styles you please.</p> </li> <li> <p>Include /css/theme/template/theme.scss The template theme file which will generate final CSS output based on the currently defined variables.</p> </li> </ol>"},{"location":"slides/reveal/examples/markdown/","title":"Markdown Demo","text":""},{"location":"slides/reveal/examples/markdown/#external-11","title":"External 1.1","text":"<p>Content 1.1</p> <p>Note: This will only appear in the speaker notes window.</p>"},{"location":"slides/reveal/examples/markdown/#external-12","title":"External 1.2","text":"<p>Content 1.2</p>"},{"location":"slides/reveal/examples/markdown/#external-2","title":"External 2","text":"<p>Content 2.1</p>"},{"location":"slides/reveal/examples/markdown/#external-31","title":"External 3.1","text":"<p>Content 3.1</p>"},{"location":"slides/reveal/examples/markdown/#external-32","title":"External 3.2","text":"<p>Content 3.2</p>"},{"location":"slides/reveal/examples/markdown/#external-33-image","title":"External 3.3 (Image)","text":""},{"location":"slides/reveal/examples/markdown/#external-34-math","title":"External 3.4 (Math)","text":"<p><code>\\[ J(\\theta_0,\\theta_1) = \\sum_{i=0} \\]</code></p>"},{"location":"slides/reveal/test/simple/","title":"Simple","text":""},{"location":"slides/reveal/test/simple/#slide-11","title":"Slide 1.1","text":"<pre><code>var a = 1;\n</code></pre>"},{"location":"slides/reveal/test/simple/#slide-12","title":"Slide 1.2","text":""},{"location":"slides/reveal/test/simple/#slide-2","title":"Slide 2","text":""}]}