{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Large Language Models : A Hands on Approach","text":"<p>Jan - May, 2026 @ Center for Continuing Education, Indian Institute of Science</p>"},{"location":"#logistics","title":"Logistics","text":"<ul> <li>Duration: 18 weeks (Jan - May 2026)</li> <li>Format: Online, Tue and Thu, 7:00 - 8:30 PM IST</li> <li>Contact: Yoginder Negi, Senior Scientific Officer (yoginder@iisc.ac.in)</li> </ul>"},{"location":"#registration","title":"Registration","text":"<p>To register please visit CCE Webpage for the course.</p>"},{"location":"#course-description","title":"Course Description","text":"<p>LLMs have become mainstay of NLP and are transforming every domain, from software development, research, and business intelligence to education. However, deploying them efficiently remains a specialized engineering challenge.</p> <p>This course provides an engineering-focused exploration of Large Language Models (LLMs). Participants will go from understanding transformer architectures and GPU internals to mastering fine-tuning, inference optimization, and large-scale deployment across GPUs, clusters, and edge devices. Through a theory-to-practice approach, including case studies, hands-on labs, and projects, learners will cover key topics such as model architecture, fine-tuning techniques, inference optimization, serving strategies, and applications in retrieval-augmented generation (RAG) and agentic systems.</p>"},{"location":"#learning-outcomes","title":"Learning Outcomes","text":"<p>By the end of this course, participants will be able to:</p> <ul> <li>Understand LLM Architecture: Master transformer architectures, attention mechanisms, and modern LLM variants (GPT-OSS, Qwen, Gemma, etc.).</li> <li>Optimize Inference: Implement efficient inference strategies including quantization, KV caching, and serving via inference engines like vLLM.</li> <li>Fine-tune Models: Apply various fine-tuning techniques including instruction tuning, PEFT techniques like LoRA, QLoRA etc and preference alignment.</li> <li>Build RAG Systems: Design and implement Retrieval-Augmented Generation pipelines.</li> <li>Develop AI Agents: Create tool-using agents with the ReAct framework.</li> <li>Deploy at Scale: Set up production-ready LLM serving infrastructure with cost optimization.</li> <li>Multimodal Models: Work with vision-language models and speech.</li> <li>Evaluation: Understand evaluation strategies for LLMs and RAG systems.</li> </ul>"},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Proficiency in Python and familiarity with any deep learning framework (PyTorch preferred).  </li> <li>Basic understanding of neural networks.</li> <li>Optional but recommended: experience with GPU computing.</li> </ul>"},{"location":"#instructors","title":"Instructors","text":"Yoginder Negi - Senior Scientific Officer, SERC, IISc. Bhuthesh R - Senior Data Scientist, Molecule AI Pvt Ltd."},{"location":"#course-navigation","title":"Course Navigation","text":"<ul> <li> <p>Weekly Schedule</p> </li> <li> <p>Assignments and Labs</p> </li> </ul>"},{"location":"assignments/","title":"Assignments and Labs","text":"<p>TBD</p>"},{"location":"course-details/","title":"Course Details","text":""},{"location":"course-details/#course-overview","title":"Course Overview","text":"<p>This is a comprehensive hands-on course on Large Language Models (LLMs) designed specifically for industry professionals. The course covers the complete spectrum of LLM engineering, from foundational concepts to advanced deployment and optimization strategies.</p>"},{"location":"course-details/#course-objectives","title":"Course Objectives","text":"<p>By the end of this course, participants will be able to:</p> <ul> <li>Understand LLM Architecture: Master transformer architectures, attention mechanisms, and modern LLM variants (GPT, BERT, Mixtral, etc.)</li> <li>Optimize Inference: Implement efficient inference strategies including quantization, KV caching, and multi-GPU serving</li> <li>Fine-tune Models: Apply various fine-tuning techniques including LoRA, QLoRA, instruction tuning, and preference alignment</li> <li>Build RAG Systems: Design and implement Retrieval-Augmented Generation pipelines with vector databases and evaluation</li> <li>Develop AI Agents: Create tool-using agents with ReAct frameworks and multi-agent orchestration</li> <li>Deploy at Scale: Set up production-ready LLM serving infrastructure with cost optimization</li> <li>Handle Multimodal AI: Work with vision-language models and speech integration</li> <li>Ensure Security: Implement security measures against prompt injection and other LLM-specific threats</li> </ul>"},{"location":"course-details/#course-structure","title":"Course Structure","text":"<p>The course is organized into 18 weeks covering 8 major themes:</p>"},{"location":"course-details/#1-llm-foundations-weeks-1-3","title":"1. LLM Foundations (Weeks 1-3)","text":"<ul> <li>Transformer architecture deep dive</li> <li>Tokenization and pretraining objectives</li> <li>Modern architectures (GPT, Qwen, Gemma)</li> <li>Scaling laws and emergent properties</li> </ul>"},{"location":"course-details/#2-gpu-infrastructure-week-4","title":"2. GPU &amp; Infrastructure (Week 4)","text":"<ul> <li>GPU architecture and CUDA programming</li> <li>Multi-GPU and multi-node parallelism</li> <li>Hardware selection and cluster building</li> </ul>"},{"location":"course-details/#3-inference-optimization-weeks-5-7","title":"3. Inference Optimization (Weeks 5-7)","text":"<ul> <li>Inference bottlenecks and profiling</li> <li>Quantization techniques (INT8, INT4, GPTQ, AWQ)</li> <li>Inference engines (vLLM, TensorRT-LLM)</li> <li>Multi-GPU serving strategies</li> </ul>"},{"location":"course-details/#4-fine-tuning-weeks-8-10","title":"4. Fine-tuning (Weeks 8-10)","text":"<ul> <li>Parameter-efficient fine-tuning (LoRA, QLoRA)</li> <li>Instruction tuning and data curation</li> <li>Preference alignment (RLHF, DPO)</li> <li>Reasoning and chain-of-thought training</li> </ul>"},{"location":"course-details/#5-rag-systems-week-11","title":"5. RAG Systems (Week 11)","text":"<ul> <li>Vector databases and hybrid search</li> <li>RAG evaluation and optimization</li> <li>Graph RAG and advanced retrieval</li> </ul>"},{"location":"course-details/#6-ai-agents-weeks-12-13","title":"6. AI Agents (Weeks 12-13)","text":"<ul> <li>ReAct framework and tool calling</li> <li>Agent fine-tuning and evaluation</li> <li>Multi-agent orchestration</li> </ul>"},{"location":"course-details/#7-advanced-topics-weeks-14-17","title":"7. Advanced Topics (Weeks 14-17)","text":"<ul> <li>Model evaluation and monitoring</li> <li>Multimodal models (vision, audio)</li> <li>Edge deployment and tiny models</li> <li>Security and privacy engineering</li> <li>Emerging architectures (Mamba, hybrid models)</li> </ul>"},{"location":"course-details/#8-student-presentations-week-18","title":"8. Student Presentations (Week 18)","text":"<ul> <li>Project showcases and peer learning</li> </ul>"},{"location":"course-details/#course-timings","title":"Course Timings","text":"<ul> <li>Duration: 18 weeks</li> <li>Format: Weekly sessions with hands-on labs</li> <li>Target Audience: Industry professionals and advanced practitioners</li> <li>Prerequisites: Python programming, basic ML knowledge</li> <li>Level: Advanced/Professional</li> </ul>"},{"location":"course-details/#hands-on-approach","title":"Hands-on Approach","text":"<p>Each week includes:</p> <ul> <li>Theory Sessions: Core concepts and architecture deep dives</li> <li>Practical Labs: Implementation exercises and code walkthroughs</li> <li>Case Studies: Real-world examples and industry applications</li> <li>Projects: Progressive building of LLM applications</li> </ul>"},{"location":"course-details/#key-tools-technologies","title":"Key Tools &amp; Technologies","text":"<ul> <li>Frameworks: PyTorch, Hugging Face Transformers, vLLM</li> <li>Infrastructure: CUDA, NCCL, DeepSpeed, FSDP</li> <li>Deployment: Docker, Kubernetes, cloud platforms</li> <li>Monitoring: Prometheus, Grafana, PyTorch Profiler</li> <li>Development: Python, Jupyter, Git, MkDocs</li> </ul>"},{"location":"course-details/#assessment","title":"Assessment","text":"<ul> <li>Weekly hands-on assignments (60%)</li> <li>Mid-term project (20%)</li> <li>Final project presentation (20%)</li> </ul>"},{"location":"course-details/#prerequisites","title":"Prerequisites","text":"<ul> <li>Programming: Proficient in Python</li> <li>Machine Learning: Understanding of neural networks, backpropagation</li> <li>Mathematics: Linear algebra, calculus, statistics</li> <li>Hardware: Basic understanding of GPU computing (preferred)</li> <li>Tools: Familiarity with Git, command line, Jupyter notebooks</li> </ul>"},{"location":"schedule/","title":"Course Schedule","text":""},{"location":"schedule/#weekwise-schedule","title":"Weekwise Schedule","text":"<p>Tentative and subject to change</p> Theme Week Topics LLM Foundations I 1.1 Orientation, Transformer architecture 1.2 Transformer Architecture - GPT 1 and 2 2.1 Tokenization, Pretraining objectives 2.2 Mixture of Experts LLM Foundation II 3.1 Case studies: State-of-the-art open-source LLM architectures 3.2 Scaling Laws, Emergent properties GPU Basics 4.1 GPU architecture deep dive 4.2 Parallelism: Multi GPU, Multi Node 5.1 On-Prem Hardware Stack Deep Dive Inference 5.2 Inference Strategies 6.1 Inference Math and Bottlenecks 6.2 Efficient Attention &amp; KV Caching Efficient Inference &amp; Quantization 7.1 Quantization Fundamentals 7.2 Inference Engines and Multi GPU Fine-Tuning Fundamentals 8.1 Full Fine-Tuning vs. PEFT \u2014 When to Use Each 8.2 Instruction Tuning 9.1 Alignment (RLHF, DPO etc) 9.2 More RL Reasoning 10.1 Reasoning &amp; Chain-of-Thought 10.2 CoT, Tree-of-Thought, Self-Consistency \u2014 Prompt Engineering as Code RAG 11.1 RAG Fundamentals - Context-engineering, embeddings, search and rerankers 11.2 Evaluating RAG Agents 12.1 ReAct Framework: Thought \u2192 Action \u2192 Observation Tool Use &amp; Function Calling 12.2 MCP introduction 12.3 Agentic RAG, Multi Agent Orchestration, Multimodal Agents Agent Finetuning 13.1 Fine Tuning for Tool calling 13.2 Agent Evaluation &amp; Safety Evaluation 14.1 Evaluation 14.2 Observability &amp; Monitoring Multimodal Models 15.1 Multi Modal Architecture: Image, Audio and Video models, Running Locally 15.2 Fine tuning multimodal models LLMs on the Edge 16.1 Edge-Optimized LLM Architectures, case studies 16.2 Edge Optimization techniques Security &amp; Privacy Engineering 17.1 Threat Model: Prompt Injection, Jailbreaking, Data Leakage Frontiers 17.2 Emerging Topics: Mamba, Qwen Next, Hybrid architectures Presentations 18.1 Student Presentations I 18.2 Student Presentations II"},{"location":"schedule/#materials","title":"Materials","text":"<ul> <li>Lecture slides and notes will be shared here as class progresses.</li> </ul>"},{"location":"slides/01_orientation/00-intro/","title":"00 intro","text":""},{"location":"slides/01_orientation/00-intro/#large-language-models-a-hands-on-approach","title":"Large Language Models : A Hands on Approach","text":""},{"location":"slides/01_orientation/00-intro/#course-orientation","title":"Course Orientation","text":""},{"location":"slides/01_orientation/00-intro/#course-overview","title":"Course Overview","text":"<ul> <li>Duration: ~18 Weeks</li> <li>Format: 2 sessions per week (Tue &amp; Thu, 7:00\u20138:30 PM IST)</li> <li>Style: Lectures, demos and labs</li> <li>Assessment: Quizzes, assignments, and final project/presentation</li> <li>Credits: 3 + 1 (Credits added in ABC)</li> </ul>"},{"location":"slides/01_orientation/00-intro/#why-this-course","title":"Why This Course?","text":"<p>LLMs have changed how we build intelligent applications.</p> <p>This course aims to bridge the gap between:</p> <p>Using LLMs via APIs \u2192 Engineering LLM systems</p>"},{"location":"slides/01_orientation/00-intro/#learning-outcomes","title":"Learning Outcomes","text":"<ul> <li>Understand LLM architectures</li> <li>Fine-tune models for specific domains and tasks</li> <li>Optimize inference for cost and latency</li> <li>Build robust LLM-powered applications (RAG, agents, tool use)</li> <li>Evaluate, debug, and improve model performance</li> <li>Leverage open-source ecosystems for LLM engineering</li> </ul>"},{"location":"slides/01_orientation/00-intro/#what-this-course-will-not-cover","title":"What This Course Will NOT Cover","text":"<ul> <li>Mathematical foundations</li> <li>Pretraining LLMs from scratch</li> <li>State of the art in research</li> <li>Data engineering pipelines</li> <li>Ethics and societal impacts</li> </ul>"},{"location":"slides/01_orientation/00-intro/#prerequisites","title":"Prerequisites","text":"<ul> <li>Programming: Python proficiency</li> <li>ML Basics: Understanding of neural networks and deep learning</li> <li>Helpful: Familiarity with PyTorch, basic NLP concepts</li> </ul>"},{"location":"slides/01_orientation/00-intro/#tools-and-resources","title":"Tools and Resources","text":""},{"location":"slides/01_orientation/00-intro/#software-stack","title":"Software Stack","text":"<ul> <li>Python</li> <li>PyTorch</li> <li>HuggingFace Ecosystem(models, libraries and datasets)</li> </ul>"},{"location":"slides/01_orientation/00-intro/#compute-stack","title":"Compute Stack","text":"Option Cost Notes Google Colab Free Limited GPU, good for experiments GCP Free Credits Free $300 for new accounts Colab Pro+ Paid More GPU time, better GPUs RunPod Paid Flexible, longer runs Lightning AI Paid Easy setup, good UX <p>Free options suffice for most of our course needs</p>"},{"location":"slides/01_orientation/00-intro/#evaluation-criteria","title":"Evaluation Criteria","text":"Component Weight Notes Quizzes 25% 5 short in-class quizzes Assignments 30% 2 assignments (Mid Feb and Mid Mar) Final Project/Presentation 40% Mar 3rd week onwards Participation 5% In-class and forum engagement <p>--</p>"},{"location":"slides/01_orientation/00-intro/#abc","title":"ABC","text":"<ul> <li>Academic Bank of Credits</li> <li>Credits will be added to your ABC account after successful course completion</li> <li>More information will be shared during the course</li> </ul>"},{"location":"slides/01_orientation/00-intro/#final-project-option","title":"Final Project Option","text":"<p>Individual or group project (max 2).</p> <p>Build an end-to-end LLM application:</p> <ul> <li>Model fine-tuning</li> <li>Inference optimization</li> <li>LLM Application (RAG, Agents)</li> <li>Open source contribution</li> <li>Other ideas welcome too</li> </ul> <p>Deliverables: Documentation + Demo / Presentation</p>"},{"location":"slides/01_orientation/00-intro/#presentation-option","title":"Presentation Option","text":"<ul> <li>Topic: Novel concept not covered in class</li> <li>Format: Research paper or case study</li> <li>Deliverables: Slides or summary document</li> <li>Evaluation: Depth of understanding, presentation quality</li> <li>Duration: 10 minutes, 8-10 slides</li> </ul>"},{"location":"slides/01_orientation/00-intro/#course-roadmap-12","title":"Course Roadmap (1/2)","text":"Module Topic 1 LLM Foundations - Transformers, GPT-2, Modern Architectures, MoE, OSS Models 2 GPUs - Architecture and Programming,  Multi-GPU Parallelism, Hardware Stack 3 Inference - Sampling, KV Caching, Quantization, Speculative Decoding, Model Serving 4 Fine-Tuning - SFT, PEFT (LoRA, QLoRA etc),RLHF (DPO, GRPO etc), Distillation 5 Reasoning - Chain-of-Thought (CoT), Test Time Scaling, Finetuning"},{"location":"slides/01_orientation/00-intro/#course-roadmap-22","title":"Course Roadmap (2/2)","text":"Module Topic 6 RAG &amp; Agents - RAG Fundamentals, ReAct, Tools, Protocols, Agents, Finetuning 7 Evaluation - Moderl Evaluations, Benchmarks, LLM-as-a-Judge 8 Multimodal - Multimodal Architectures, Finetuning 9 Edge Deployment - Edge architectures, Optimization and Deployment"},{"location":"slides/01_orientation/00-intro/#making-the-best-of-this-course","title":"Making the Best of This Course","text":"<ul> <li>Participate actively: Ask questions in class, engage in discussions</li> <li>Practice hands-on: Run the code, experiment with parameters</li> <li>Use AI tools: Leverage AI assistants for coding and learning</li> <li>Stay connected: Use forum for async discussions</li> </ul>"},{"location":"slides/01_orientation/00-intro/#qa","title":"Q&amp;A","text":""},{"location":"slides/01_orientation/00-intro/#any-questions","title":"Any questions?","text":""},{"location":"slides/01_orientation/notes/","title":"Notes","text":""},{"location":"slides/01_orientation/notes/#course-overview-and-structure","title":"Course Overview and Structure","text":"<ul> <li>Duration: ~18 Weeks</li> <li>Format: 2 sessions per week, Tuesday and Thursday (7:00 PM to 8:30 PM IST)</li> <li>Style: Mix of lectures, demos, labs, and projects</li> <li>Assessment: Assignments, quizzes, and final project</li> </ul>"},{"location":"slides/01_orientation/notes/#why-this-course","title":"Why this course?","text":"<p>LLMs have changed how we build intelligent applications. This course bridges the gap between using LLMs via APIs and engineering LLM systems you can deploy and operate in production.</p>"},{"location":"slides/01_orientation/notes/#learning-outcomes","title":"Learning Outcomes","text":"<p>You will learn how to: - Understand LLM architectures - Fine-tune models for specific domains and tasks - Optimize inference for cost and latency - Build robust LLM-powered applications (RAG, agents, tool use) - Evaluate, debug, and improve model performance systematically - Make use open-source ecosystems for LLM engineering</p>"},{"location":"slides/01_orientation/notes/#what-this-course-will-not-cover","title":"What This Course Will NOT Cover","text":"<ul> <li>Mathematical foundations</li> <li>Pretraining LLMs from scratch </li> <li>State of the art in research</li> <li>Ethics and societal impacts</li> <li>Data engineering pipelines</li> </ul>"},{"location":"slides/01_orientation/notes/#prerequisites","title":"Prerequisites","text":"<ul> <li>Programming: Python proficiency</li> <li>ML Basics: Understanding of neural networks and deep learning concepts</li> <li>Helpful: Familiarity with PyTorch, basic NLP concepts</li> </ul>"},{"location":"slides/01_orientation/notes/#tools-and-resources","title":"Tools and Resources","text":""},{"location":"slides/01_orientation/notes/#software","title":"Software","text":"<ul> <li>Python</li> <li>PyTorch</li> <li>HuggingFace (transformers, datasets, peft, trl)</li> </ul>"},{"location":"slides/01_orientation/notes/#hardware-compute","title":"Hardware / Compute","text":"Option Cost Notes Google Colab Free Limited GPU time, good for experiments GCP Free Credits Free $300 credits for new accounts Colab Pro+ Paid More GPU time, better GPUs RunPod Paid Flexible, good for longer runs Lightning AI Paid Easy setup, good UX <ul> <li>*first two will suffice for most assignments and labs, for final projects you might need paid options depending on your project scope</li> </ul>"},{"location":"slides/01_orientation/notes/#evaluation-criteria","title":"Evaluation Criteria","text":"Component Weight Description Quizzes 20% 4\u20135 short in-class quizzes Assignments 30% 2 assignments covering key modules Final Project or Presentation 50% Project : Build and deploy an end-to-end LLM application (agents or RAG/systems), or improve an existing approach (e.g., inference optimization or fine-tuning). Presentation : Cover a novel concept not covered in the class (rsearch paper or case study)"},{"location":"slides/01_orientation/notes/#final-project-or-presentation-details","title":"Final Project or Presentation Details","text":"<p>Project Option: - Build an end-to-end LLM application - Options: Fine-tuned model, RAG system, agent application - Deliverables: documentation, demo and  - Evaluation: Functionality, </p> <p>Presentation Option: - Topic: Novel concept not covered in class - Deliverables: Slides or summary document - Evaluation: Depth of understanding, Presentation quality</p>"},{"location":"slides/01_orientation/notes/#grading-policy","title":"Grading Policy","text":"<p>TODO (@yknegi): Add grading policy details ABC </p>"},{"location":"slides/01_orientation/notes/#course-roadmap","title":"Course Roadmap","text":"<p>TODO (@yknegi): remove course roadmap if not needed | Module | Topic | Key Concepts | |:---|:---|:---| | 1 | LLM Foundations | Transformers, GPT-2 style, Modern Architectures, Mixture of Experts, Open-Source LLMs | | 2 | GPUs | GPU Architecture, Multi-GPU Parallelism, Hardware Stack | | 3 | Optimizing Inference | Sampling, Memory Optimization (KV Caching), Quantization, Inference Engines (vLLM, SGLang) | | 4 | Fine-Tuning | Full Fine-Tuning, PEFT (LoRA, QLoRA), Instruction Tuning, Alignment (DPO, RLHF), Distillation, Reasoning | | 5 | RAG and Agents | RAG Fundamentals, Agentic RAG, ReAct, Tools, MCP, Multi-Agent Systems | | 6 | Evaluation | Frameworks, MMLU, LMSYS Arena, LLM-as-a-Judge, Error Analysis | | 7 | Multimodal | Multimodal Architectures, Visual Instruction Fine-tuning | | 8 | Edge Deployment | Edge Architectures, llama.cpp, Optimization, and Deployment |</p>"},{"location":"slides/01_orientation/notes/#making-the-best-of-this-course","title":"Making the Best of This Course","text":"<ul> <li>Participate actively: Ask questions, engage in discussions</li> <li>Practice hands-on: Run the code, experiment with parameters</li> <li>Stay connected: Use forum for async discussions</li> </ul>"},{"location":"slides/01_orientation/notes/#qa","title":"Q&amp;A","text":"<p>Any questions?</p>"},{"location":"slides/02_llm_basics/notes/","title":"Large Language Models (LLMs) - Introduction","text":""},{"location":"slides/02_llm_basics/notes/#what-is-a-large-language-model","title":"What is a Large Language Model?","text":"<p>A Large Language Model is a deep neural network trained on extensive datasets containing text from books, articles, websites, and other sources sometimes encompassing large portions of the entire publicly available text on the internet.</p> <p>LLMs have remarkable capabilities in understanding, generating, and interpreting human language, code, and even multimodal data (text, images, audio).</p> <p></p> <p>A language model predicts the next word in a sequence given the preceding words. The models are trained to predict the next word in a sentence, given the preceding words. This training process allows them to learn grammar, facts about the world, reasoning abilities, and even some level of common sense.</p>"},{"location":"slides/02_llm_basics/notes/#mathematical-definition","title":"Mathematical Definition","text":"<p>Mathematically, a language model estimates the probability distribution over a sequence of tokens. The joint probability of a sequence $W = (w_1, w_2, \\dots, w_T)$ is factorized using the chain rule of probability:</p> <p>$$ P(w_1, w_2, \\dots, w_T) = \\prod_{t=1}^{T} P(w_t \\mid w_1, \\dots, w_{t-1}) $$</p> <p>The core objective is to learn the conditional probability of the next token $w_t$ given the history $w_{&lt;t}$:</p> <p>$$ P(w_t \\mid w_{&lt;t}) $$</p> <p>Where: - $w_t$ is the token to be predicted at step $t$. - $w_{&lt;t} = (w_1, \\dots, w_{t-1})$ is the context of preceding tokens. - $P(w_t \\mid w_{&lt;t})$ is the probability of the next word $w_t$ conditioned on the sequence of previous words.</p> <p> [Image source][1]</p> <p>The \"large\" aspect refers to the model's size, which is typically measured in terms of: - Parameter count: Typically &gt;1B parameters (e.g., GPT-3: 175B, Llama 3: 8B-405B) [2] - Training data: Trillions of tokens (e.g., Llama 3 trained on 15T tokens) [3] - Compute: Thousands of GPU-hours for training</p> <p>These models can have billions or even trillions of parameters, enabling them to capture complex patterns in language.</p> <p></p>"},{"location":"slides/02_llm_basics/notes/#some-applications-of-large-language-models","title":"Some Applications of Large Language Models","text":"<ol> <li> <p>Chatbots and Virtual Assistants: LLMs power conversational agents that can understand and respond to user queries in a human-like manner.</p> </li> <li> <p>Natural Language Processing (NLP): LLMs are widely used in NLP tasks such as sentiment analysis, named entity recognition, and text classification.</p> </li> <li> <p>Content Generation: LLMs can generate coherent and contextually relevant text, making them useful for content creation, summarization, and translation.</p> </li> <li> <p>Code Generation: LLMs can assist in generating code snippets, automating repetitive coding tasks, and even debugging code.</p> </li> <li> <p>Education and Tutoring: LLMs can provide personalized learning experiences, answer questions, and assist with homework.</p> </li> <li> <p>Creative Writing: LLMs can collaborate with authors by generating ideas, suggesting plot twists, and even writing poetry or stories.</p> </li> <li> <p>Research Assistance: LLMs can help researchers by summarizing papers, generating hypotheses, and even conducting literature reviews.</p> </li> </ol> <p>These applications demonstrate the versatility and potential of Large Language Models in transforming how we interact with technology and information.</p>"},{"location":"slides/02_llm_basics/notes/#why-build-your-own-llm","title":"Why build your own LLM?","text":"<ol> <li>Domain-specific models - can outperform general models like ChatGPT, Claude, etc.    e.g., Models trained for law, medical question answering, etc.</li> <li>Cost-effectiveness - cheaper to run your own LLM than to use cloud-based services.</li> <li>Data Privacy - you have control over the data used to train the model, prevent sensitive data being sent to model providers</li> <li>Customized Deployment - you can deploy the model on your own infrastructure or edge devices</li> <li>Autonomy - you can control the model's behavior, update it and fix it</li> </ol>"},{"location":"slides/02_llm_basics/notes/#building-and-using-llms","title":"Building and Using LLMs","text":"<p>Building and using LLMs involves several steps, including data collection, preprocessing, model selection, training, and evaluation.</p> <p></p> <p>1. Data Collection and Preprocessing: Data collection is the first step in building an LLM. It involves gathering a large and diverse dataset of text data from various sources, such as books, articles, websites, and social media platforms. Typically several terabytes or petabytes of data are required to train a large LLM.</p> <p>2. Pretraining: Pretraining involves training a large language model on a large dataset of text data. The model is trained to predict the next token in a sequence of tokens, given the preceding tokens. This training process allows the model to learn the patterns and relationships in the data, which can then be used for a variety of downstream tasks.</p> <p>3. Fine-tuning: Fine-tuning adapts a pretrained model for specific tasks and aligns it to human preferences.</p> <p>4. Inference: Inference involves using the model to generate text based on a given input. The model is used to generate text based on a given input, which can be used for a variety of downstream tasks.</p> <p>In our course, we'll not cover data collection and pretraining from scratch, but will focus on fine-tuning, inference optimization, and building applications using LLMs. Pre-training and data engineering require significant resources and are beyond the scope of this course. We shall use existing pretrained models from open-source repositories like HuggingFace and focus on engineering aspects of LLMs.</p>"},{"location":"slides/02_llm_basics/notes/#code-example-next-token-prediction-with-huggingface","title":"Code Example: Next-Token Prediction with HuggingFace","text":"<pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\ninputs = tokenizer(\"The capital of France is\", return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_new_tokens=5)\nprint(tokenizer.decode(outputs[0]))\n# Output: \"The capital of France is Paris, and the\"\n</code></pre>"},{"location":"slides/02_llm_basics/notes/#demo","title":"Demo","text":"<ul> <li>Text Generation</li> <li>Vision-Language Model</li> <li>Audio Generation</li> </ul>"},{"location":"slides/02_llm_basics/notes/#exercise","title":"Exercise","text":"<ul> <li>Login to Google Colab</li> <li>Run shared notebooks</li> <li>Visit HuggingFace, explore trending models, spaces and datasets. Use this HuggingFace Tutorial as reference.</li> </ul>"},{"location":"slides/02_llm_basics/notes/#references","title":"References","text":"<ol> <li>The Illustrated GPT-2 - Jay Alammar</li> <li>Language Models are Few-Shot Learners (GPT-3) - Brown et al., 2020</li> <li>Llama 3 Technical Report - Meta AI, 2024</li> <li>Hands-On Large Language Models by Jay Alammar, Maarten Grootendorst</li> </ol>"},{"location":"slides/02_llm_basics/slides/","title":"Slides","text":""},{"location":"slides/02_llm_basics/slides/#large-language-models-a-hands-on-approach","title":"Large Language Models : A Hands on Approach","text":""},{"location":"slides/02_llm_basics/slides/#llm-basics","title":"LLM Basics","text":""},{"location":"slides/02_llm_basics/slides/#what-is-a-large-language-model-llm","title":"What is a Large Language Model (LLM)?","text":"<p>A deep neural network trained on extensive text datasets from books, articles, websites\u2014sometimes encompassing large portions of the entire publicly available internet.</p> <p></p>"},{"location":"slides/02_llm_basics/slides/#llm-capabilities","title":"LLM Capabilities","text":"<ul> <li>Understanding and generating human language</li> <li>Code generation and debugging</li> <li>Multimodal data processing (text, images, audio)</li> <li>Reasoning and common sense understanding</li> </ul>"},{"location":"slides/02_llm_basics/slides/#language-modeling","title":"Language Modeling","text":"<p>A language model predicts the next word given preceding words.</p> <p></p> <p>[Image source][1]</p>"},{"location":"slides/02_llm_basics/slides/#mathematical-definition","title":"Mathematical Definition","text":"<p>Joint probability of a sequence using chain rule:</p> <p>$$ P(w_1, w_2, \\dots, w_T) = \\prod_{t=1}^{T} P(w_t \\mid w_1, \\dots, w_{t-1}) $$</p> <p>Core objective: learn the conditional probability:</p> <p>$$ P(w_t \\mid w_{&lt;t}) $$</p> <p>where - $w_t$ \u2014 token to be predicted at step $t$ - $w_{&lt;t} = (w_1, \\dots, w_{t-1})$ \u2014 context of preceding tokens - $P(w_t \\mid w_{&lt;t})$ \u2014 probability of next word conditioned on previous words</p>"},{"location":"slides/02_llm_basics/slides/#what-makes-llms-large","title":"What Makes LLMs \"Large\"?","text":"Aspect Scale Parameters &gt;1B (GPT-3: 175B, Llama 3: 8B-405B) [2] Training Data Trillions of tokens (Llama 3: 15T) [3] Compute Thousands of GPU-hours"},{"location":"slides/02_llm_basics/slides/#applications-of-llms","title":"Applications of LLMs","text":"<ol> <li>Chatbots &amp; Virtual Assistants - conversational agents</li> <li>NLP Tasks \u2014 sentiment analysis, NER, classification</li> <li>Content Generation \u2014 summarization, translation</li> <li>Code Generation \u2014 snippets, debugging, automation</li> <li>Education &amp; Tutoring \u2014 personalized learning</li> <li>Creative Writing \u2014 ideas, plot twists, poetry</li> <li>Research Assistance \u2014 paper summarization, literature reviews</li> </ol>"},{"location":"slides/02_llm_basics/slides/#why-build-your-own-llm","title":"Why Build Your Own LLM?","text":"<ol> <li>Domain-specific models \u2014 outperform general models (law, medical)</li> <li>Cost-effectiveness \u2014 cheaper than cloud APIs at scale</li> <li>Data Privacy \u2014 control over sensitive data</li> <li>Custom Deployment \u2014 on-premise or edge devices</li> <li>Autonomy \u2014 control behavior, updates, fixes</li> </ol>"},{"location":"slides/02_llm_basics/slides/#building-llms-overview","title":"Building LLMs: Overview","text":"<ol> <li>Data Collection and Preprocessing</li> <li>Pretraining </li> <li>Fine-tuning</li> <li>Inference</li> </ol>"},{"location":"slides/02_llm_basics/slides/#course-focus","title":"Course Focus","text":"<p>We will focus on: - Fine-tuning existing models - Inference optimization - Building applications with LLMs</p> <p>Using state of the art open source models available openly.</p>"},{"location":"slides/02_llm_basics/slides/#demo","title":"Demo","text":"<ul> <li>Text Generation</li> <li>Vision-Language Model</li> </ul>"},{"location":"slides/02_llm_basics/slides/#warm-up-exercise","title":"Warm Up Exercise","text":"<ol> <li>Login to Google Colab</li> <li>Run shared notebooks</li> <li>Explore HuggingFace:</li> <li>Trending models</li> <li>Spaces</li> <li>Datasets</li> </ol> <p>Reference: HuggingFace Tutorial</p>"},{"location":"slides/02_llm_basics/slides/#references","title":"References","text":"<ol> <li>The Illustrated GPT-2 - Jay Alammar</li> <li>GPT-3 Paper - Brown et al., 2020</li> <li>Llama 3 Report - Meta AI, 2024</li> </ol>"},{"location":"slides/02_llm_basics/slides/#thank-you","title":"Thank You","text":"<p>Questions?</p>"},{"location":"slides/03_tokenization/slides/","title":"Slides","text":""},{"location":"slides/03_tokenization/slides/#large-language-models-a-hands-on-approach","title":"Large Language Models: A Hands on Approach","text":""},{"location":"slides/03_tokenization/slides/#tokenization","title":"Tokenization","text":""},{"location":"slides/03_tokenization/slides/#topics-covered","title":"Topics Covered","text":"<ul> <li>Preprocessing text data for LLMs</li> <li>Tokenization techniques</li> <li>Byte Pair Encoding (BPE)</li> <li>Converting tokens into vectors</li> </ul>"},{"location":"slides/03_tokenization/slides/#models-of-the-week","title":"Models of the Week","text":""},{"location":"slides/03_tokenization/slides/#translategemma","title":"TranslateGemma","text":"<ul> <li>SOTA Open weights multilingual translation model</li> <li>Multimodal capabilities: text + images for context</li> <li>4B, 12B and 27B versions</li> </ul>"},{"location":"slides/03_tokenization/slides/#medgemma","title":"MedGemma","text":"<ul> <li>SOTA Open weights medical imaging and document understanding model</li> <li>CT/MRI/Histopathology Processing, Medical Document Understanding, Multi-domain Classification</li> </ul>"},{"location":"slides/03_tokenization/slides/#glm-47-flash","title":"GLM-4.7-Flash","text":"<ul> <li>Coding, Tool use and Reasoning abilities</li> <li>30B-A3B MoE model</li> </ul>"},{"location":"slides/03_tokenization/slides/#motivation","title":"Motivation","text":"<p>Why does tokenization matter?</p> <ul> <li>Cost: Billing is per token, not per word</li> <li>Context limits: Tokenization decides what fits vs what gets truncated</li> <li>Reasoning failures: Try \"Say Nameeee\" vs \"Say Name eee\" in DeepSeek</li> <li>Multilingual bias: Some languages need more tokens for same meaning</li> </ul>"},{"location":"slides/03_tokenization/slides/#data-preprocessing-pipeline","title":"Data Preprocessing Pipeline","text":"<ul> <li>Cannot feed raw text directly into LLMs. </li> <li>Need numerical representations.</li> </ul>"},{"location":"slides/03_tokenization/slides/#graph-lr-araw-text-btokenization-b-ctoken-ids-c-dembedding-layer-d-einput-embeddings-e-fllm","title":"<pre><code>graph LR\n    A[Raw Text] --&gt; B[Tokenization]\n    B --&gt; C[Token IDs]\n    C --&gt; D[Embedding Layer]\n    D --&gt; E[Input Embeddings]\n    E --&gt; F[LLM]\n</code></pre>","text":""},{"location":"slides/03_tokenization/slides/#the-full-pipeline","title":"The Full Pipeline","text":"<p>Unified Architecture</p> <p></p>"},{"location":"slides/03_tokenization/slides/#what-is-tokenization","title":"What Is Tokenization?","text":"<p>Tokenization breaks text into smaller units called tokens.</p> <p>Tokens can be words, subwords, or characters.</p> <p>Example: \"The transformer architecture is revolutionary.\"</p> Approach Tokens Count Words <code>[\"The\", \"transformer\", \"architecture\", \"is\", \"revolutionary\", \".\"]</code> 6 Characters <code>[\"T\", \"h\", \"e\", \" \", \"t\", \"r\", \"a\", ...]</code> 45 Subwords <code>[\"The\", \" transform\", \"er\", \" architecture\", \" is\", \" revolution\", \"ary\", \".\"]</code> 8"},{"location":"slides/03_tokenization/slides/#real-world-example-gpt-2-tokenizer","title":"Real World Example: GPT-2 Tokenizer","text":"<p>Try it yourself</p> <p></p>"},{"location":"slides/03_tokenization/slides/#tokenization-pipeline","title":"Tokenization Pipeline","text":""},{"location":"slides/03_tokenization/slides/#tokenization-spectrum","title":"Tokenization Spectrum","text":"<pre><code>Bytes \u2192 Characters \u2192 Subwords \u2192 Words\n  \u2191                              \u2191\n 256 tokens                     Millions of tokens\n Long sequences                 Short sequences\n</code></pre> <p>Trade-off between vocabulary size and sequence length.</p>"},{"location":"slides/03_tokenization/slides/#word-level-tokenization","title":"Word-Level Tokenization","text":"<p>Split text into words based on spaces and punctuation.</p> <pre><code>import re\ntext = \"Hello, world. This, is a test.\"\nresult = re.split(r'(\\s)', text)\n# ['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n</code></pre> <p></p>"},{"location":"slides/03_tokenization/slides/#word-level-pros-and-cons","title":"Word-Level: Pros and Cons","text":"<p>Advantages: - Short sequences (one token per word) - Linguistically intuitive - Fast attention (fewer tokens)</p> <p>Disadvantages: - Huge vocabulary (English needs 100K+ words) - OOV problem: Unknown words \u2192 <code>[UNK]</code> - Morphological blindness: \"run\", \"runs\", \"running\" are unrelated - Language-specific: Some languages don't use spaces</p>"},{"location":"slides/03_tokenization/slides/#character-level-tokenization","title":"Character-Level Tokenization","text":"<p>Split text into individual characters.</p> <p>Advantages: - No OOV tokens (any text can be encoded) - Tiny vocabulary (~100-300 tokens) - Handles typos and neologisms</p> <p>Disadvantages: - Very long sequences (5-10x longer) - Slow attention (more tokens) - Poor semantics (characters lack meaning) - Harder to learn word structure</p>"},{"location":"slides/03_tokenization/slides/#subword-tokenization","title":"Subword Tokenization","text":"<p>The sweet spot: split rare words, keep common words whole.</p> Word Subword Tokens Interpretation the <code>[\"the\"]</code> Common \u2192 single token transformer <code>[\"trans\", \"former\"]</code> Split into known pieces unhappiness <code>[\"un\", \"happi\", \"ness\"]</code> Morphemes preserved GPT-4 <code>[\"G\", \"PT\", \"-\", \"4\"]</code> Unknown \u2192 character fallback"},{"location":"slides/03_tokenization/slides/#popular-subword-algorithms","title":"Popular Subword Algorithms","text":"<ol> <li>Byte Pair Encoding (BPE) - Frequency-based merging</li> <li>WordPiece - Probability-based merging (BERT)</li> <li>SentencePiece - Unigram language model (LLaMA)</li> </ol>"},{"location":"slides/03_tokenization/slides/#byte-pair-encoding-bpe","title":"Byte Pair Encoding (BPE)","text":"<p>Sennrich et al. (2016)</p> <p>Core idea: Iteratively merge the most frequent pair of tokens.</p> <ol> <li>Start with vocabulary of individual characters (or bytes)</li> <li>Count all adjacent pairs in the corpus</li> <li>Merge the most frequent pair into a new token</li> <li>Repeat until reaching desired vocabulary size</li> </ol>"},{"location":"slides/03_tokenization/slides/#bpe-walkthrough","title":"BPE Walkthrough","text":"<p>Input: \"low lower lowest\"</p> <p>Step 1 - Initialize tokens: <pre><code>['l', 'o', 'w', ' ', 'l', 'o', 'w', 'e', 'r', ' ', 'l', 'o', 'w', 'e', 's', 't']\n</code></pre></p> <p>Step 2 - Count pairs: <pre><code>('l', 'o'): 3    ('o', 'w'): 3    ('w', ' '): 3\n(' ', 'l'): 2    ('e', 'r'): 1    ('e', 's'): 1\n</code></pre></p> <p>--</p>"},{"location":"slides/03_tokenization/slides/#bpe-walkthrough-continued","title":"BPE Walkthrough (continued)","text":"<p>Step 3 - Merge most frequent pair <code>('l', 'o')</code> \u2192 <code>'lo'</code> <pre><code>['lo', 'w', ' ', 'lo', 'w', 'e', 'r', ' ', 'lo', 'w', 'e', 's', 't']\n</code></pre></p> <p>Step 4 - Count pairs again: <pre><code>('lo', 'w'): 3    ('w', ' '): 3    (' ', 'lo'): 2\n</code></pre></p> <p>Step 5 - Merge <code>('lo', 'w')</code> \u2192 <code>'low'</code> <pre><code>['low', ' ', 'low', 'e', 'r', ' ', 'low', 'e', 's', 't']\n</code></pre></p> <p>--</p>"},{"location":"slides/03_tokenization/slides/#bpe-result","title":"BPE Result","text":"<p>Final tokenization: <pre><code>\"low lower lowest\" \u2192 [\"low\", \" \", \"low\", \"er\", \" \", \"low\", \"est\"]\n</code></pre></p> <p>Can encode any word: <pre><code>\"lowestness\" \u2192 [\"low\", \"est\", \"ness\"]\n</code></pre></p> <p></p>"},{"location":"slides/03_tokenization/slides/#bpe-pros-and-cons","title":"BPE: Pros and Cons","text":"<p>Advantages: - Balances vocabulary size and sequence length - Handles OOV words by breaking into subwords - Captures morphological structure</p> <p>Disadvantages: - Greedy algorithm (may not find optimal tokenization) - Training corpus dependent</p>"},{"location":"slides/03_tokenization/slides/#byte-level-bpe-gpt-2-style","title":"Byte-Level BPE (GPT-2 Style)","text":"<p>GPT-2 uses byte-level BPE:</p> <ul> <li>Start with 256 byte tokens (not Unicode characters)</li> <li>All text is UTF-8 encoded first</li> <li>Merges operate on bytes, not characters</li> <li>Avoids merges beyond word boundaries</li> </ul> <p>Advantages: - Handles any language without special tokenization - Works with emojis, rare scripts, binary data - No <code>[UNK]</code> tokens ever needed</p>"},{"location":"slides/03_tokenization/slides/#multimodal-tokenization","title":"Multimodal Tokenization","text":"<p>Modern models tokenize more than text:</p> <ul> <li>Text: Subword tokenization (BPE, WordPiece)</li> <li>Images: Patch-based (e.g., ViT) - split into 16x16 patches</li> <li>Video: Frame + patch tokenization</li> <li>Audio: Spectrogram frames</li> </ul> <pre><code>graph LR\n    T[Text Input] --&gt; TT[Text Tokenizer]\n    I[Image Input] --&gt; IT[Image Tokenizer]\n    A[Audio Input] --&gt; AT[Audio Tokenizer]\n    V[Video Input] --&gt; VT[Video Tokenizer]\n\n    TT --&gt; U[Unified Token Sequence]\n    IT --&gt; U\n    AT --&gt; U\n    VT --&gt; U\n\n    U --&gt; M[Transformer Model]\n</code></pre>"},{"location":"slides/03_tokenization/slides/#vocabulary-size-trade-offs","title":"Vocabulary Size Trade-offs","text":"<p>Smaller Vocabulary: - Faster training and inference - Lower memory usage - Longer sequences</p> <p>Larger Vocabulary: - Shorter sequences - Better semantic representation - Higher memory usage</p>"},{"location":"slides/03_tokenization/slides/#vocabulary-impact-on-parameters","title":"Vocabulary Impact on Parameters","text":"<p>$$ \\text{Embedding Matrix} = V \\times E $$</p> <p>$$ \\text{Output Layer} = H \\times V $$</p> <p>Where $V$ = vocabulary size, $E$ = embedding dimension, $H$ = hidden dimension</p>"},{"location":"slides/03_tokenization/slides/#model-vocabulary-comparison","title":"Model Vocabulary Comparison","text":"Model Vocabulary Size Tokenizer GPT-2 50,257 BPE GPT-4 ~100,000 BPE variant BERT 30,522 WordPiece LLaMA 32,000 SentencePiece Claude ~100,000 BPE variant"},{"location":"slides/03_tokenization/slides/#references","title":"References","text":"<ol> <li>Let's Build the GPT Tokenizer - Andrej Karpathy</li> <li>The Smol Training Playbook</li> <li>Neural Machine Translation of Rare Words with Subword Units - Sennrich et al., 2016</li> </ol>"},{"location":"slides/03_tokenization/slides/#thank-you","title":"Thank You","text":"<p>Questions?</p>"},{"location":"slides/04_transformer_basics/notes/","title":"Notes","text":""},{"location":"slides/04_transformer_basics/notes/#transformer-architecture-introduction","title":"Transformer Architecture : Introduction","text":"<p>Most modern LLMs rely on the transformer architecture, which is a deep neural network architecture introduced in the 2017 paper \u201cAttention Is All You Need\u201d (https://arxiv.org/abs/1706.03762).</p> <p>TODO (cover transformer in depth) Jay Alammar's blog : https://jalammar.github.io/illustrated-transformer/ Annotated transformer paper : http://nlp.seas.harvard.edu/2018/04/03/attention.html Attention is all you need paper : https://arxiv.org/abs/1706.03762</p>"},{"location":"slides/04_transformer_basics/notes/#encoder-architecture","title":"Encoder Architecture","text":"<p>Self-Attention at high level  Say the following sentence is an input sentence we want to translate:</p> <p>\u201dThe animal didn't cross the street because it was too tired\u201d</p> <p>What does \u201cit\u201d in this sentence refer to? Is it referring to the street or to the animal? It\u2019s a simple question to a human, but not as simple to an algorithm.</p> <p>When the model is processing the word \u201cit\u201d, self-attention allows it to associate \u201cit\u201d with \u201canimal\u201d.</p> <p>Self-Attention in detail - The first step in calculating self-attention is to create three vectors from each of the encoder\u2019s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process.      q1 = Wq x embedding1, k1 = Wk x embedding1, v1 = Wv x embedding1</p> <ul> <li> <p>Next, we calculate a score that determines how much focus to place on other parts of the input sentence for each word. We do this by taking the dot product of the Query vector with the Key vector of each word in the sentence. This gives us a score for each word.     score1 = q1 . k1, score2 = q1 . k2, score3 = q1 . k3, ...</p> </li> <li> <p>We then divide each of these scores by the square root of the dimension of the Key vectors     score1 = score1 / sqrt(dk), score2 = score2 / sqrt(dk), ...</p> </li> <li> <p>Next, we apply a softmax function to these scores to obtain the weights on the Value vectors. The softmax function converts the scores into probabilities that sum to 1.     weights = softmax([score1, score2, score3, ...])</p> </li> <li> <p>The fifth step is to multiply each value vector by the softmax score (in preparation to sum them up).     weighted_v1 = weights[0] * v1, weighted_v2 = weights[1] * v2, ...</p> </li> <li>Finally, we sum up the weighted value vectors to get the output vector for this word.    z1 = weighted_v1 + weighted_v2 + weighted_v3 + ...</li> </ul> <p>Multi-Head Attention - Instead of performing a single self-attention calculation, the transformer architecture uses multiple self-attention calculations in parallel, known as multi-head attention. Each head has its own set of learned weight matrices (Wq, Wk, Wv) and produces its own output vector. - The outputs of all the heads are then concatenated and linearly transformed to produce the final output vector for the word. - Multiple heads allow the model to attend to different parts of the input sentence simultaneously, capturing different relationships and patterns in the data. - This is particularly useful for complex tasks like language translation, where different words may have different relationships with each other.</p> <p>Positional Encoding - Since the transformer architecture does not have any inherent notion of word order (unlike RNNs), it uses positional encoding to inject information about the position of each word in the sentence. - Positional encodings are added to the input embeddings at the bottom of the encoder and decoder stacks.</p> <p>The Residual Connection and Layer Normalization - Each sub-layer in the transformer (such as the multi-head attention layer and the feed-forward layer) has a residual connection around it, followed by layer normalization. - This helps to stabilize the training process and allows for deeper networks.</p>"},{"location":"slides/04_transformer_basics/notes/#the-decoder-side-of-the-transformer","title":"The decoder side of the Transformer","text":"<ul> <li>The output of the top encode block is K and V values</li> <li>This is fed into encoder-decoder attention blocks of the decoder blocks along with the target sequence (shifted right by one position) </li> <li>The decoder has masked self-attention layers to prevent positions from attending to subsequent positions. This masking, combined with the fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.</li> </ul> <p>The final linear and softmax layer  - The final output of the decoder stack is fed into a linear layer followed by a softmax layer to produce the final output probabilities for each token in the vocabulary.  - The token with the highest probability is selected as the output token for that position.  - </p>"},{"location":"slides/04_transformer_basics/notes/#transfromer-circuits","title":"Transfromer Circuits","text":"<p>Residual connections :   - communication channels that skip over layers  - no computation, just copy-paste  - help with gradient flow</p> <p>Attention heads :   - Independent attention mechanism from each head  - Attention heads can be thought as information routing circuits  - where to get information from  - what to do with the information   - how to combine information from different sources</p> <p>Readling List - https://jalammar.github.io/illustrated-transformer/ - http://nlp.seas.harvard.edu/2018/04/03/attention.html - https://arxiv.org/abs/1706.03762 - Illustrated GPT-2 : https://jalammar.github.io/illustrated-gpt2/ - Krupa Dave - Everything about Transformers - Ch3 and Ch4 of LLMs from Scratch book - Pytorch overview </p>"},{"location":"slides/04_transformer_basics/slides-2/","title":"Slides 2","text":"# Transformers - II"},{"location":"slides/04_transformer_basics/slides-2/#topics-covered","title":"Topics Covered","text":"<ul> <li>Self-attention mechanism</li> <li>Causal Attention</li> <li>Multi-head attention</li> </ul>"},{"location":"slides/04_transformer_basics/slides-2/#recap","title":"RECAP","text":"<ul> <li>Encoder-Decoder architectures</li> <li>Cross attention mechanism between encoder and decoder</li> <li>Simple attention mechanism</li> </ul>"},{"location":"slides/04_transformer_basics/slides-2/#sequence-modeling","title":"Sequence Modeling","text":"<ul> <li>Process all input into a hidden state, </li> <li>Pass hidden state to decoder</li> <li>Decoder uses hidden state to generate output sequence</li> </ul>"},{"location":"slides/04_transformer_basics/slides-2/#rnns-attention","title":"RNNs + Attention","text":"<ul> <li>Let Decoder access all Encoder hidden states</li> <li>Attend to relevant parts of input sequence when generating each output token [Bahdanau et al., 2015]</li> </ul>"},{"location":"slides/04_transformer_basics/slides-2/#rnns-attention_1","title":"RNNs + Attention","text":"<p>Limitations</p> <ul> <li>Sequential processing limits parallelization</li> <li>Difficulty capturing long-range dependencies</li> </ul> <p>Solution: </p> <ul> <li>Remove recurrence, process all input tokens simultaneously</li> <li>Allow each token in the input to focus on relevant parts of the input</li> </ul>"},{"location":"slides/04_transformer_basics/slides-2/#parallel-processing","title":"Parallel Processing","text":"<ul> <li>Encoder : Process all input tokens simultaneously</li> <li>Decoder : Generate output tokens one by one, attending to encoder states and previous tokens</li> </ul>"},{"location":"slides/04_transformer_basics/slides-2/#self-attention-mechanism","title":"Self-Attention Mechanism","text":"<ul> <li>Compute attention within the same sequence of tokens. Self = Same Sequence</li> <li>Get improved representation by mixing in information from other tokens that seem relevant.</li> </ul>"},{"location":"slides/04_transformer_basics/slides-2/#self-attention-vs-encoderdecoder-attention","title":"Self-Attention vs Encoder\u2013Decoder Attention","text":"<ul> <li> <p>Encoder\u2013Decoder: One sequence attends to a different sequence (e.g., translation: output attends to the input sentence).</p> </li> <li> <p>Self-attention: Sequence attends to itself (tokens attending to other tokens in the same sentence).</p> </li> </ul>"},{"location":"slides/04_transformer_basics/slides-2/#self-attention-intuition","title":"Self-Attention : Intuition","text":"*\"Self-attention is like a group conversation where everyone can hear everyone else simultaneously, rather than passing notes one by one (RNNs)\"*"},{"location":"slides/04_transformer_basics/slides-2/#computing-attention-weights-for-a-single-token","title":"Computing attention weights for a single token","text":"<p>Step 1: - Calculate attention scores dot product of input vectors - Normalize scores to get attention weights (additive normalization)</p> <pre><code>attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n\ndef softmax_naive(x):\n    return torch.exp(x) / torch.exp(x).sum(dim=0)\n\nattn_weights_2_naive = softmax_naive(attn_scores_2)\n\nattn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n</code></pre> <p>--</p> <p>Step 2: - Compute output vector as weighted sum of value vectors</p> <p></p> <pre><code>context_vec_2 = torch.zeros(inputs.shape[1])\nfor i, x_i in enumerate(inputs):\n    context_vec_2 += attn_weights_2[i] * x_i\n</code></pre>"},{"location":"slides/04_transformer_basics/slides-2/#computing-attention-weigths-for-all-tokens","title":"Computing attention weigths for all tokens","text":"<pre><code>attn_scores = torch.zeros(inputs.shape[0], inputs.shape[0])\n\nattn_scores = inputs @ inputs.T\nattn_weights = torch.softmax(attn_scores, dim=-1)\noutput_vectors = attn_weights @ inputs\n</code></pre>"},{"location":"slides/04_transformer_basics/slides-2/#summary-of-self-attention-mechanism","title":"Summary of Self-Attention Mechanism","text":"<ul> <li>Input: sequence of vectors (X) (source)</li> <li>Output: sequence of vectors (Z) (context)</li> <li>Compute attention scores against all input vectors</li> <li>Normalize scores to get attention weights</li> <li>Compute output vectors as weighted sum of input vectors</li> </ul> <pre><code>def self_attention(inputs):\n    # Step 1: Compute attention scores\n    attn_scores = inputs @ inputs.T\n    # Step 2: Normalize scores to get attention weights\n    attn_weights = torch.softmax(attn_scores, dim=-1)\n    # Step 3: Compute output vectors as weighted sum of input vectors\n    output_vectors = attn_weights @ inputs\n    return output_vectors\n</code></pre>   **How to improve this basic self-attention mechanism?**   Learn the weights used to compute attention scores!    ## Self-Attention with Learned Weights"},{"location":"slides/04_transformer_basics/slides-2/#attention-as-information-flow","title":"Attention as information flow","text":"<p>Three steps in attention mechanism: 1. What to attend to : Identify relevant tokens in the sequence =&gt; Identify similar information 2. How strongly to attend: Compute attention weights over those tokens =&gt; Determine importance 3. Contextualize/Output: Weighted sum of token representations =&gt; Copy relevant information</p>"},{"location":"slides/04_transformer_basics/slides-2/#attention-as-information-flow_1","title":"Attention as information flow","text":"<p>Three steps in attention mechanism: 1. What to attend to : Identify relevant tokens in the sequence =&gt; Identify similar information 2. How strongly to attend: Compute attention weights over those tokens =&gt; Determine importance 3. Contextualize/Output: Weighted sum of token representations =&gt; Copy relevant information</p> <ul> <li>We look at same token in three different ways to achieve this.</li> </ul>"},{"location":"slides/04_transformer_basics/slides-2/#attention-as-information-flow_2","title":"Attention as information flow","text":"<p>Three steps in attention mechanism: 1. What to attend to : Identify relevant tokens in the sequence =&gt; Identify similar information 2. How strongly to attend: Compute attention weights over those tokens =&gt; Determine importance 3. Contextualize/Output: Weighted sum of token representations =&gt; Copy relevant information</p> <ul> <li>We look at same token in three different ways to achieve this.</li> <li>The token embeddings alone are not sufficient.</li> </ul>"},{"location":"slides/04_transformer_basics/slides-2/#attention-as-information-flow_3","title":"Attention as information flow","text":"The token embeddings alone are not sufficient."},{"location":"slides/04_transformer_basics/slides-2/#attention-as-information-flow_4","title":"Attention as information flow","text":"The token embeddings alone are not sufficient.  Why"},{"location":"slides/04_transformer_basics/slides-2/#attention-as-information-flow_5","title":"Attention as information flow","text":"The token embeddings alone are not sufficient.  Why"},{"location":"slides/04_transformer_basics/slides-2/#introducing-q-k-v-vectors","title":"Introducing Q, K, V vectors","text":"<p>We transform each input token into three different vectors:</p> <ul> <li>Token embedding: x_i</li> <li>Query (Q): What am I looking for?</li> <li>Key (K): What do I have to offer?</li> <li>Value (V): What information do I carry?</li> </ul> <p></p>"},{"location":"slides/04_transformer_basics/slides-2/#why-three-vectors-q-k-v","title":"Why three vectors (Q, K, V)?","text":"<ul> <li>Query (Q): Current token we are focusing on. <ul> <li>Q should get specialized to ask questions about relevance.</li> </ul> </li> <li>Key (K): All other tokens in the sequence. <ul> <li>K should get specialized to help determine relevance to Q.</li> </ul> </li> <li>Value (V): Represents the actual information we want to copy from the input. <ul> <li>V gets specialized to carry useful content.</li> </ul> </li> </ul> <ul> <li>Q and K exist to define a similarity space, while V exists to define an information space.</li> <li>Queries are optimized to ask questions</li> <li>Values are optimized to carry information</li> </ul> <p>--</p> <p>Example Library System:</p> <p>Think of each token as a book in a library. Every book has three different representations, depending on what you\u2019re doing with it.</p> <p>Query (Q): the question you\u2019re asking right now</p> <p>\"I'm looking for material about animal fatigue\"</p> <p>\"I need something that explains causal negation\"</p> <p>Important: The query is shaped by your current goal, not by what the books contain.</p> <p>That\u2019s exactly what Q does:</p> <p>It encodes what this token needs from context.</p> <p>--</p> <p>Key (K): the book\u2019s index card</p> <p>Each book has an index card (or metadata record).</p> <p>Topics, Keywords, Cross-references, Classification codes</p> <p>Important: The system never reads the whole book to decide relevance. It compares your query against the keys.</p> <p>That\u2019s K: A compact representation optimized for matching, not for content.</p> <p>--</p> <p>Value (V): the actual book content</p> <p>Once relevant books are identified, you don\u2019t copy the index cards.</p> <p>You copy: Paragraphs, Explanations, Facts,</p> <p>That\u2019s the Value: The information you actually want to transfer into your answer. | Component | Description | Example | |-----------|-------------|---------| | Query (Q) | The question you're asking | \"I need info about animal fatigue\" | | Key (K) | Index cards with topics/keywords | Book metadata, classification codes | | Value (V) | Actual content to retrieve | Paragraphs, explanations, facts to copy |</p> <p>--</p> <p>Can we use Q for V? - Using the same vector for both querying and copying can limit expressiveness. - Separate Q, K, V allow the model to learn different representations for querying and copying.</p>"},{"location":"slides/04_transformer_basics/slides-2/#how-to-get-q-k-v-vectors","title":"How to get Q, K, V vectors","text":"<p>We project the token embedding to three learned projection spaces:</p> <p>PROJECTION === \"Matrix Multiplication\"</p> <ul> <li>Query (Q): $q_i = x_i W_q$</li> <li>Key (K): $k_i = x_i W_k$</li> <li>Value (V): $v_i = x_i W_v$</li> </ul> <p></p>"},{"location":"slides/04_transformer_basics/slides-2/#q-k-v-projections","title":"Q, K, V Projections","text":"<p>$W_q$: projects tokens into query space</p> <p>$W_k$: projects tokens into key space</p> <p>$W_v$: projects tokens into value space</p> <p></p>"},{"location":"slides/04_transformer_basics/slides-2/#q-k-v-projections_1","title":"Q, K, V Projections","text":"<p>$Q = X W_q$ =&gt; Turn input tokens into queries</p> <p>$K = X W_k$ =&gt; Turn input tokens into keys</p> <p>$V = X W_v$ =&gt; Turn input tokens into values</p> <p></p>"},{"location":"slides/04_transformer_basics/slides-2/#self-attention-with-learned-weights","title":"Self-Attention with Learned Weights","text":"<ul> <li>Project input vectors to query, key, and value spaces</li> </ul>      $$\\mathbf{queries} = X W_q$$  $$\\mathbf{keys} = X W_k$$  $$\\mathbf{values} = X W_v$$  <ul> <li>Compute Similarity : </li> </ul>   $$\\mathbf{scores} = \\mathbf{queries} \\times \\mathbf{keys}^T$$  $$\\mathbf{attn\\_weights} = \\text{softmax}(\\mathbf{scores})$$   <ul> <li>Contextualization</li> </ul>   $$\\mathbf{context\\_vec} = \\mathbf{attn\\_weights} \\times \\mathbf{values}$$"},{"location":"slides/04_transformer_basics/slides-2/#scaled-dot-product-attention","title":"Scaled Dot-Product Attention","text":"$$ \\mathbf{Attention}(Q, K, V) = \\mathbf{softmax}\\left(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\right)V $$   <p>$$d_k \\text{ is the dimensionality of the key vectors (used for scaling).}$$</p> <p>Why divide by sqrt(dk)? - Prevents large dot product values when dk is large - Helps keep gradients stable during training - Dot product variance is ~dk, scaling by sqrt(dk) normalizes variance to ~1</p>"},{"location":"slides/04_transformer_basics/slides-2/#implementing-self-attention-with-learned-weights-single-token","title":"Implementing Self-Attention with Learned Weights : Single Token","text":"<p>Calculating self-attention step-by-step : Single Token</p> <p></p> <p>--</p>"},{"location":"slides/04_transformer_basics/slides-2/#implementing-self-attention-with-learned-weights-single-token_1","title":"Implementing Self-Attention with Learned Weights : Single Token","text":"<ol> <li> <p>Initial Step <pre><code>W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\nW_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\nW_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n</code></pre></p> </li> <li> <p>Compute Query, Key, Value vectors:</p> </li> </ol> <pre><code>q_2 = x_2 @ W_query  # Query for token 2\nk_2 = x_2 @ W_key    # Key for token 2\nv_2 = x_2 @ W_value  # Value for token 2\n</code></pre> <ol> <li>Compute attention weights by dot product of Query with all Key vectors:</li> </ol> <pre><code>attn_scores_2 = torch.empty(inputs.shape[0])\nattn_scores_2 = q_2.dot(k_2)\nattn_weights_2 = torch.softmax(attn_scores_2 / (k_2.shape[-1]**0.5), dim=-1)\n</code></pre> <ol> <li>Calculate value vectors: <pre><code>context_vec_2 = torch.zeros(inputs.shape[1])\ncontext_vec_2 = attn_weights[2] @ V\n</code></pre></li> </ol>"},{"location":"slides/04_transformer_basics/slides-2/#self-attention-with-learned-weights-all-tokens","title":"Self-Attention with Learned Weights :  All Tokens","text":"<pre><code>class SelfAttention(nn.Module):\n    def __init__(self, d_in, d_out, qkv_bias=False):\n\n        super().__init__()\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n\n    def forward(self, x):\n        keys = self.W_key(x)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n        attn_scores = queries @ keys.T\n        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n        context_vec = attn_weights @ values\n        return context_vec\n</code></pre>"},{"location":"slides/04_transformer_basics/slides-2/#attention-during-training","title":"Attention during Training","text":"<ul> <li>All tokens are available =&gt; [\"cat\", \"sat\", \"on\", \"the\", \"mat\"]</li> <li>For each token we predict the next token <ul> <li>\"cat\" =&gt; \"sat\"</li> <li>\"cat sat\" =&gt; \"on\"</li> <li>\"cat sat on\" =&gt; \"the\"</li> <li>\"cat sat on the\" =&gt; \"mat\"</li> <li>\"cat sat on the mat\" =&gt; \".\"</li> </ul> </li> <li>We need to hide future tokens during training to prevent information leakage.</li> </ul>"},{"location":"slides/04_transformer_basics/slides-2/#attention-during-training_1","title":"Attention during Training","text":"<p>Masking future tokens</p>"},{"location":"slides/04_transformer_basics/slides-2/#causal-attention-masking-in-decoder-only-models","title":"Causal  Attention (Masking) in Decoder-Only Models","text":"<ul> <li>In decoder-only models, we predict next token based on previous tokens</li> <li>Note : During training, we predict all tokens in parallel</li> <li>To prevent information leakage from future tokens, we apply a causal mask to the attention scores</li> <li>At all time steps, each token can only attend to earlier tokens</li> </ul>"},{"location":"slides/04_transformer_basics/slides-2/#masking-in-causal-attention","title":"Masking in Causal Attention","text":"<ul> <li>Masking</li> </ul> <pre><code>sa = SelfAttention(d_in, d_out)\nqueries = sa.W_query(inputs)\nkeys = sa.W_key(inputs)\nattn_scores = queries @ keys.T\n</code></pre> <ul> <li>Softmax Function</li> </ul> <p>$$ \\mathrm{Softmax}(x) = \\frac{\\exp(x)}{\\sum \\exp(x)} $$</p> <ul> <li>Fill with -inf where mask is True</li> </ul> <pre><code>    # Step 1: Create mask shape (L, L)\n    seq_len = attn_scores.shape[-1]\n\n    # Step 2: Lower triangular = positions we CAN attend to\n    causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n\n    # Step 3: Set positions we CAN'T attend to as -inf\n    attn_scores.masked_fill_(causal_mask, float('-inf'))\n</code></pre>"},{"location":"slides/04_transformer_basics/slides-2/#attenion-dropout","title":"Attenion + Dropout","text":"<p>Dropout is a regularization technique that randomly sets some neuron weights to zero during training to prevent overfitting.</p> <p></p>"},{"location":"slides/04_transformer_basics/slides-2/#dropout-in-attention-weights","title":"Dropout in Attention Weights","text":"<ul> <li>By applying dropout to attention weights, we randomly ignore some attention connections during training.</li> <li>Makes the model more robust by preventing it from relying too heavily on specific attention patterns.</li> </ul> <pre><code>dropout = nn.Dropout(p=0.2)\nattn_weights = dropout(attn_weights)\n</code></pre>"},{"location":"slides/04_transformer_basics/slides-2/#putting-it-all-together-self-attention-module-with-masking-and-dropout","title":"Putting it all together: Self-Attention Module with Masking and Dropout","text":"<pre><code>    keys = self.W_key(x)\n    queries = self.W_query(x)\n    values = self.W_value(x)\n\n    attn_scores = queries @ keys.T\n\n    mask = torch.triu(torch.ones(L, L), diagonal=1).bool()  # Upper triangular\n    attn_scores.masked_fill_(mask, float('-inf'))\n\n    attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n\n    attn_weights = self.dropout(attn_weights)\n\n    context_vec = attn_weights @ values\n</code></pre>"},{"location":"slides/04_transformer_basics/slides-2/#multi-head-attention","title":"Multi-Head Attention","text":""},{"location":"slides/04_transformer_basics/slides-2/#stacking-multiple-attention-heads","title":"Stacking multiple attention heads","text":"<ul> <li>Perform multiple self-attention calculations in parallel, with own set of learned weight matrices (Wq, Wk, Wv) and  output vector for each head.</li> <li>Concatenate all to produce one context vector for each token.</li> <li>Multiple heads -&gt; attend to input sentence simultaneously -&gt; different relationships and patterns in the data.</li> </ul>"},{"location":"slides/04_transformer_basics/slides-2/#multi-head-attention-naive-implementation","title":"Multi-Head Attention : Naive Implementation","text":"<pre><code># naive version using loops\n\nclass MultiHeadAttentionWrapper(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n        super().__init__()\n        self.heads = nn.ModuleList([\n            CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n            for _ in range(num_heads)\n        ])\n\n    def forward(self, x):\n        return torch.cat([head(x) for head in self.heads], dim=-1)\n</code></pre>"},{"location":"slides/04_transformer_basics/slides-2/#multi-head-attention-efficient-implementation","title":"Multi-Head Attention : Efficient Implementation","text":"<pre><code>class MultiHeadAttention(nn.Module):\n  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n    super().__init__()\n    self.num_heads = num_heads\n    assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n    self.head_dim = d_out // num_heads\n    self.out_proj = nn.Linear(d_out, d_out)\n\n  def forward(self, X):\n\n    queries = queries.reshape(batches, num_tokens, self.num_heads, self.head_dim) # B x L x num_heads x head_dim\n    keys = keys.reshape(batches, num_tokens, self.num_heads, self.head_dim) # B x L x num_heads x head_dim\n    values = values.reshape(batches, num_tokens, self.num_heads, self.head_dim) # B x L x num_heads x head_dim\n\n    queries = queries.transpose(1, 2) # B x num_heads x L x head_dim\n    keys = keys.transpose(1, 2) # B x num_heads x L x head_dim\n    values = values.transpose(1, 2) # B x num_heads x L x head_dim\n\n    attn_scores = queries @ keys.transpose(2, 3) # (B x num_heads x L x head_dim) @ (B x num_heads x head_dim x L) =&gt; B x num_heads x L x L\n\n    # mask : # L x L =&gt; (1 x 1 x L x L)\n    attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) # B x num_heads x L x L\n\n    attn_weights = torch.softmax(attn_scores / self.head_dim ** 0.5, dim=-1) \n    attn_weights = nn.Dropout(self.dropout)(attn_weights) # B x num_heads x L x L\n\n    context_vec = attn_weights @ values # (B x num_heads x L x L) @ (B x num_heads x L x head_dim)\n\n    context_vec = context_vec.transpose(1, 2) # B x L x num_heads x head_dim\n    context_vec = context_vec.reshape(batches, num_tokens, self.d_out) # B x L x d_out\n\n    return self.out_proj(context_vec) # (B x L x d_out) @ (d_out x d_out) =&gt; B x L x d_out\n</code></pre>"},{"location":"slides/04_transformer_basics/slides-2/#summary","title":"Summary","text":"<p>Journey through Attention:</p> <ol> <li>Simple Attention: Dot products between embeddings (no learning)</li> <li>Self-Attention: Add learnable Q, K, V projections</li> <li>Causal Attention: Mask future tokens for autoregressive generation</li> <li>Multi-Head: Run multiple attention patterns in parallel</li> </ol>"},{"location":"slides/04_transformer_basics/slides-2/#what-we-didnt-cover","title":"What We Didn't Cover","text":"<ul> <li>Positional Encodings: How does attention know word order?</li> <li>Feed-Forward Networks: The other half of each transformer block</li> <li>Layer Normalization: Stabilizing training</li> <li>Residual Connections: Enabling deep networks</li> </ul> <p>\u2192 These will be covered in the next session where we implement a full transformer block and a LLM from scratch!</p>"},{"location":"slides/04_transformer_basics/slides/","title":"Slides","text":"## LLMs : A Hands-on Approach   ### Transformers"},{"location":"slides/04_transformer_basics/slides/#topics-covered","title":"Topics Covered","text":"<ul> <li>Transformer architecture</li> <li>Self-attention mechanism</li> <li>Causal Attention</li> <li>Multi-head attention</li> </ul>"},{"location":"slides/04_transformer_basics/slides/#models-of-the-week","title":"Models of the Week","text":"<p>moonshotai/Kimi-K2.5</p> <ul> <li>SOTA Open Source model</li> <li>1T+ parameters, 30T tokens pretraining data, Vocabulary Size - 160K, Context Length - 256K</li> <li>Agentic, Agent Swarm, Multi-modal capabilities</li> </ul> <p>nvidia/personaplex-7b-v1</p> <ul> <li>7B, Real-time, speech to speech model, full-duplex model </li> <li>Demo</li> </ul> <p>Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice</p> <ul> <li>Text to Speech model with voice design, voice cloning, custom voice</li> </ul>"},{"location":"slides/04_transformer_basics/slides/#recap-tokenization","title":"Recap : Tokenization","text":"<ul> <li>Words, Subwords, Characters level tokenization</li> <li>Subword tokenization is the most commonly used approach in LLMs</li> </ul>"},{"location":"slides/04_transformer_basics/slides/#text-to-token-ids","title":"Text to Token IDs","text":"<pre><code>import tiktoken\ntokenizer = tiktoken.get_encoding(\"gpt2\")\ntext = \"This is an example.\"\ntoken_ids = tokenizer.encode(text)\n# tokens = ['This', ' is', ' an', ' example', '.']\n# token_ids = [40234, 2052, 133, 389, 12]\n</code></pre>"},{"location":"slides/04_transformer_basics/slides/#tokens-to-embeddings","title":"Tokens to Embeddings","text":"<ul> <li>Each token ID maps to a unique vector in the embedding matrix</li> <li>Embedding Matrix : [vocab_size x embedding_dim] </li> <li>Example : GPT-2 Small<ul> <li>vocab_size = 50257</li> <li>embedding_dim = 768</li> </ul> </li> </ul>   ## Transformers"},{"location":"slides/04_transformer_basics/slides/#transformers","title":"Transformers","text":"<ul> <li>All LLMs rely on the Transformer architecture, introduced in the 2017 paper \"Attention Is All You Need\" (https://arxiv.org/abs/1706.03762).</li> <li>No Recurrent or Convolution layers, entirely based on Attention Mechanism</li> </ul>"},{"location":"slides/04_transformer_basics/slides/#two-types-of-transformer-architectures","title":"Two types of Transformer Architectures","text":"<ul> <li>Encoder-Decoder : Sequence to sequence tasks (translation, summarization)</li> <li>Decoder-Only : Language modeling tasks (text generation, completion)</li> <li>Most LLMs (GPT, Llama, etc.) use the Decoder-Only architecture</li> </ul>"},{"location":"slides/04_transformer_basics/slides/#key-components-of-transformer","title":"Key Components of Transformer","text":"<ul> <li>Positional Encoding </li> <li>Multi-Head Attention </li> <li>Residual Connections </li> <li>Feed Forward Networks </li> <li>Layer Normalization </li> </ul>"},{"location":"slides/04_transformer_basics/slides/#language-modeling","title":"Language Modeling","text":"<ul> <li>Given a sequence of tokens, predict the next token</li> <li>Example: A robot may not harm a ___ -&gt; human</li> <li>[PROMPT] -&gt; [MODEL] -&gt; [PREDICTION]</li> </ul>"},{"location":"slides/04_transformer_basics/slides/#decoder-only-language-model","title":"Decoder-only Language Model","text":"<ul> <li>A decoder-only language model is a stack of transformer decoder blocks</li> </ul>"},{"location":"slides/04_transformer_basics/slides/#decoder-only-llm-input-side","title":"Decoder-only LLM : Input Side","text":"<ul> <li>Input tokens are passed through multiple  decoder blocks</li> <li>Embed : Text -&gt; Token IDs -&gt; Embeddings -&gt; Decoder Blocks</li> </ul>"},{"location":"slides/04_transformer_basics/slides/#decoder-only-llm-output-side","title":"Decoder-only LLM : Output Side","text":"<ul> <li>UnEmbed : The final vector is projected to vocabulary size and softmaxed to get token probabilities</li> </ul>"},{"location":"slides/04_transformer_basics/slides/#decoder-block-internals","title":"Decoder Block Internals","text":"<ul> <li>Masked Multi-Head Self-Attention</li> <li>Feed Forward Network (FFN)</li> <li>Residual Connections</li> <li>Layer Normalization</li> </ul>   ## Attention Mechanism"},{"location":"slides/04_transformer_basics/slides/#sequence-modeling-challenges","title":"Sequence Modeling Challenges","text":"<ul> <li>Understanding context and relationships between words</li> <li>Need to keep grammatical structures aligned</li> </ul>"},{"location":"slides/04_transformer_basics/slides/#recurrent-neural-networks-rnns-for-sequence-modeling","title":"Recurrent Neural Networks (RNNs) for Sequence Modeling","text":"<ul> <li>Process all input into a hidden state, </li> <li>Pass hidden state to decoder</li> <li>Decoder uses hidden state to generate output sequence</li> </ul>"},{"location":"slides/04_transformer_basics/slides/#rnns-attention","title":"RNNs + Attention","text":"<ul> <li>Let Decoder access all Encoder hidden states</li> <li>Attend to relevant parts of input sequence when generating each output token [Bahdanau et al., 2015]</li> </ul>"},{"location":"slides/04_transformer_basics/slides/#rnns-attention_1","title":"RNNs + Attention","text":"<p>Limitations</p> <ul> <li>Sequential processing limits parallelization</li> <li>Difficulty capturing long-range dependencies</li> </ul> <p>Solution: </p> <ul> <li>Remove recurrence, process all input tokens simultaneously</li> <li>Allow each token in the input to focus on relevant parts of the input</li> </ul>"},{"location":"slides/04_transformer_basics/slides/#parallel-processing","title":"Parallel Processing","text":"<ul> <li>Encoder : Process all input tokens simultaneously</li> <li>Decoder : Generate output tokens one by one, attending to encoder states and previous tokens</li> </ul>"},{"location":"slides/04_transformer_basics/slides/#self-attention-mechanism","title":"Self-Attention Mechanism","text":"<ul> <li>Compute attention within the same sequence of tokens. Self = Same Sequence</li> <li>Get improved representation by mixing in information from other tokens that seem relevant.</li> </ul>"},{"location":"slides/04_transformer_basics/slides/#self-attention-intuition","title":"Self-Attention : Intuition","text":"*\"Self-attention is like a group conversation where everyone can hear everyone else simultaneously, rather than passing notes one by one (RNNs)\"*"},{"location":"slides/04_transformer_basics/slides/#self-attention-intuition_1","title":"Self-Attention : Intuition","text":"<ul> <li> <p>Each token : \"Who should I pay attention to?\"</p> </li> <li> <p>For every token, the model:</p> <ul> <li>treats that token as the \"current focus\"</li> <li>assigns higher weight to tokens that help interpret it</li> <li>creates an updated vector for the token:  </li> </ul> </li> </ul>"},{"location":"slides/04_transformer_basics/slides/#self-attention-vs-encoderdecoder-attention","title":"Self-Attention vs Encoder\u2013Decoder Attention","text":"<ul> <li> <p>Encoder\u2013Decoder: One sequence attends to a different sequence (e.g., translation: output attends to the input sentence).</p> </li> <li> <p>Self-attention: Sequence attends to itself (tokens attending to other tokens in the same sentence).</p> </li> </ul>"},{"location":"slides/04_transformer_basics/slides/#simple-attention-mechanism","title":"Simple Attention Mechanism","text":"<p>Input  -  Sequence of vectors (X) (source)</p> <p>Output - Sequence of vectors (Z) (context)</p>  $$ X = [x_1, x_2, \\dots, x_n], \\quad x_i \\in \\mathbb{R}^d $$  $$ Z = [z_1, z_2, \\dots, z_n], \\quad z_i \\in \\mathbb{R}^d $$  $$ z_i = \\sum_{j=1}^{n} \\text{attention\\_weight}_{ij} \\. x_j $$"},{"location":"slides/04_transformer_basics/slides/#computing-attention-weights-for-a-single-token","title":"Computing attention weights for a single token","text":"*Your **journey** starts with one step*   <p>query = \"journey\"</p> <p></p> <p>Step 1: </p> <ul> <li>Compute attention scores by dot product of \"journey\" with all tokens</li> </ul> <pre><code>query = inputs[1]\n\nattn_scores_2 = torch.empty(inputs.shape[0])\n\nfor i, x_i in enumerate(inputs):\n    attn_scores_2[i] = torch.dot(x_i, query)\n\nprint(attn_scores_2)\n</code></pre> <p>--</p>"},{"location":"slides/04_transformer_basics/slides/#computing-attention-weights-for-a-single-token_1","title":"Computing attention weights for a single token","text":"<p>Step 2:</p> <ul> <li>Apply normalization to get attention weights (additive normalization)</li> <li>Normalization using softmax is more common in practice, as it ensures all weights are positive and sum to 1.</li> </ul> <pre><code>attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n\ndef softmax_naive(x):\n    return torch.exp(x) / torch.exp(x).sum(dim=0)\n\nattn_weights_2_naive = softmax_naive(attn_scores_2)\n\nattn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n</code></pre> <p>--</p> <p>Step 3: - Compute output vector as weighted sum of value vectors</p> <p></p> <pre><code>context_vec_2 = torch.zeros(inputs.shape[1])\nfor i, x_i in enumerate(inputs):\n    context_vec_2 += attn_weights_2[i] * x_i\n</code></pre>"},{"location":"slides/04_transformer_basics/slides/#computing-attention-weigths-for-all-tokens","title":"Computing attention weigths for all tokens","text":"<ul> <li>Compute attention scores for all tokens ```python [1 | 2-10] attn_scores = torch.zeros(inputs.shape[0], inputs.shape[0])</li> </ul> <p>for i, x_i in enumerate(inputs):     for j, x_j in enumerate(inputs):         attn_scores[i, j] = torch.dot(x_i, x_j)</p> <pre><code>- Normalize scores to get attention weights\n```python\nattn_weights = torch.softmax(attn_scores, dim=-1)\n</code></pre> <ul> <li>Compute output/context    vectors for all tokens <pre><code>output_vectors = torch.zeros_like(inputs)\nfor i in range(inputs.shape[0]):\n    for j in range(inputs.shape[0]):\n        output_vectors[i] += attn_weights[i, j] * inputs[j]\n</code></pre></li> </ul> <p>--</p>"},{"location":"slides/04_transformer_basics/slides/#computing-attention-weigths-for-all-tokens_1","title":"Computing attention weigths for all tokens","text":"<ul> <li>Better implementation using matrix multiplication</li> </ul> <pre><code>attn_scores = torch.zeros(inputs.shape[0], inputs.shape[0])\n\nattn_scores = inputs @ inputs.T\nattn_weights = torch.softmax(attn_scores, dim=-1)\noutput_vectors = attn_weights @ inputs\n</code></pre>"},{"location":"slides/04_transformer_basics/slides/#summary-of-self-attention-mechanism","title":"Summary of Self-Attention Mechanism","text":"<ul> <li>Input: sequence of vectors (X) (source)</li> <li>Output: sequence of vectors (Z) (context)</li> <li>Compute attention scores against all input vectors</li> <li>Normalize scores to get attention weights</li> <li>Compute output vectors as weighted sum of input vectors</li> </ul> <pre><code>def self_attention(inputs):\n    # Step 1: Compute attention scores\n    attn_scores = inputs @ inputs.T\n    # Step 2: Normalize scores to get attention weights\n    attn_weights = torch.softmax(attn_scores, dim=-1)\n    # Step 3: Compute output vectors as weighted sum of input vectors\n    output_vectors = attn_weights @ inputs\n    return output_vectors\n</code></pre>   **How to improve this basic self-attention mechanism?**   Learn the weights used to compute attention scores!"},{"location":"slides/04_transformer_basics/slides/#references","title":"References","text":"<ul> <li>Vaswani et al., Attention Is All You Need (2017) - The original transformer paper</li> <li>Bahdanau et al., Neural Machine Translation by Jointly Learning to Align and Translate (2014) - Introduced attention for seq2seq</li> <li>Jay Alammar, The Illustrated Transformer</li> <li>Jay Alammar, The Illustrated GPT-2</li> <li>The Annotated Transformer - Harvard NLP</li> <li>Sebastian Raschka, Build a Large Language Model from Scratch - Chapters 3-4</li> </ul>"},{"location":"slides/04_transformer_basics/slides/#thank-you","title":"Thank You","text":"<p>Questions?</p>"},{"location":"slides/05_llm_implmentation/notes/","title":"Implementing a LLM","text":"<ul> <li>In previous classes we have covered tokenization, embeddings, and the transformer architecture in detail.</li> <li> <p>We have also implemented the multi head self-attention mechanism, the crucial component of transformers.</p> </li> <li> <p>In this class we will put everything together and implement a small LLM from scratch using PyTorch.</p> </li> <li> <p>The implmentation closely follows the GPT-2 architecture.</p> </li> </ul>"},{"location":"slides/05_llm_implmentation/notes/#gpt-2-model","title":"GPT - 2 Model","text":"<p>GPT-2 model is built using the transformer decoder blocks. The model is just a stack of transformer decoder blocks, with an embedding layer at the input and a linear + softmax layer at the output.</p> <p>How many layers of transformer blocks?  - GPT-2 Small: 12 layers  - GPT-2 Medium: 24 layers  - GPT-2 Large: 36 layers  - GPT-2 XL: 48 layers</p> <p>Generation Viz 1:</p> <p></p> <p>Generation Viz 2: </p>"},{"location":"slides/05_llm_implmentation/notes/#looking-inside-the-gpt-2-model","title":"Looking inside the GPT-2 model","text":"<ul> <li>A stack of transformer blocks</li> <li>Each block has multi-head self-attention, feed forward network, layer normalization, and residual connections</li> <li>Input embeddings + positional encodings at the bottom, linear + softmax layer at the top</li> </ul> <p>Sample Text generation</p> <p> - Initally the model only has one input token, so that path would be the only active one.  - The token is processed successively through all the layers, then a vector is produced along that path. That vector can be scored against the model\u2019s vocabulary (all the words the model knows, 50,000 words in the case of GPT-2) and the most likely next token can be selected. - next step, we add the output from the first step to our input sequence, and have the model make its next prediction: - Each layer of GPT-2 has retained its own interpretation of the first token and will use it in processing the second token (we\u2019ll get into more detail about this in the following section about self-attention). GPT-2 does not re-interpret the first token in light of the second token.</p> <p>Input Encoding - We need to give the model input in the form of token IDs. - for each token ID, we look up its corresponding embedding vector from the embedding matrix.</p> <p></p> <ul> <li>We take mbeddings and add positional encodings to them to give the model information about the position of each token in the sequence.</li> </ul> <p></p> <p>After combination:  </p> <p>In summary, the input to GPT-2 is a sequence of token IDs, which are converted to embedding vectors and combined with positional encodings to form the final input representation for the model.</p> <p>Passing token through the transformer blocks - The first block can now process the token by first passing it through the self-attention process, then passing it through its neural network layer.  - Once the first transformer block processes the token, it sends its resulting vector up the stack to be processed by the next block.  - The process is identical in each block, but each block has its own weights in both self-attention and the neural network sublayers. </p> <p>Self-Attention in GPT-2 - Self attention allows each token to pay importance to other tokens in the sequence when processing itself. - Example 1: \u201dThe animal didn't cross the street because it was too tired\u201d . When the model is processing the word \u201cit\u201d, self-attention allows it to associate \u201cit\u201d with \u201canimal\u201d. - Example 2 : \"A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.\u201d Here, self-attention helps the model understand that \"such orders\" refers to \"the orders given it by human beings\".</p> <p></p> <p>Self-Attention Process - For each token, we create Query (Q), Key (K), and Value (V) vectors - Query: The query is a representation of the current word used to score against all the other words (using their keys). We only care about the query of the token we\u2019re currently processing. - Key: Key vectors are like labels for all the words in the segment. They\u2019re what we match against in our search for relevant words. - Value: Value vectors are actual word representations, once we\u2019ve scored how relevant each word is, these are the values we add up to represent the current word.</p> <p> - We compute attention scores by taking the dot product of the Query vector of the current token with the Key vectors of all tokens. - We scale the scores by dividing by the square root of the dimension of the Key vectors. - We apply softmax to the scores to get attention weights. - We multiply each Value vector by its corresponding attention weight. - We sum the weighted Value vectors to get the output vector for the current token.</p> <p></p> <p>Model Output  - After the input token has passed through all the transformer blocks, we get a final output vector.   </p> <ul> <li>This vector is then passed through a linear layer followed by a softmax layer to produce the final output probabilities for each token in the vocabulary.</li> </ul> <p></p> <p></p> <p>Our imlementations: vocabulary size: 50257 (same as GPT-2) embedding dimension: 768 number of transformer blocks: 12 number of attention heads: 12 maximum sequence length: 1024</p>"},{"location":"slides/05_llm_implmentation/notes/#transformer-block","title":"Transformer Block","text":""},{"location":"slides/05_llm_implmentation/notes/#layer-normalization","title":"Layer Normalization","text":""},{"location":"slides/05_llm_implmentation/notes/#feed-forward-network-ffn","title":"Feed Forward Network (FFN)","text":""},{"location":"slides/05_llm_implmentation/notes/#residual-connections-shortcuts","title":"Residual Connections / shortcuts","text":""},{"location":"slides/05_llm_implmentation/notes/#illustrated-gpt-2-architecture","title":"Illustrated GPT-2 Architecture","text":""},{"location":"slides/05_llm_implmentation/notes/#generating-text","title":"Generating Text","text":"<p>as we can see, the transformer Block maintains the input dimensions in its output, indicating that the transformer architecturfe proc esses sequences of data without altering their shape through out the network</p>"},{"location":"slides/05_llm_implmentation/notes/#pretraining-the-llm","title":"Pretraining the LLM","text":""},{"location":"slides/05_llm_implmentation/notes/#tidbits","title":"Tidbits","text":"<ul> <li> <p>show cross entropy loss calculation for next token prediction</p> <ul> <li>on random token predction on the vocabulary</li> <li>show as model trains </li> <li>show on trained weights</li> </ul> </li> <li></li> </ul>"},{"location":"slides/05_llm_implmentation/slides-2/","title":"Slides 2","text":"# LLMs : A Hands-on Approach   ### GPT - 2  : Implementation and Training"},{"location":"slides/05_llm_implmentation/slides-2/#topics-covered","title":"Topics Covered","text":"<ul> <li> <p>GPT-2 Architecture Review</p> <ul> <li>Layer Normalization</li> <li>Self-Attention in GPT-2</li> <li>Feed-Forward Network (FFN)</li> <li>Residual Connections</li> </ul> </li> <li> <p>Training GPT-2</p> </li> <li> <p>Text generation</p> </li> <li>Greedy decoding loop</li> <li>Training loop</li> </ul>"},{"location":"slides/05_llm_implmentation/slides-2/#gpt-2-architecture","title":"GPT-2 Architecture","text":"<ul> <li>GPT model stacks multiple transformer decoder blocks</li> <li>Each block has:</li> <li>Masked Multi-Head Self-Attention layer</li> <li>Feed-Forward Neural Network (FFN)</li> <li>Layer Normalization and Residual Connections</li> <li>Final output layer</li> </ul>"},{"location":"slides/05_llm_implmentation/slides-2/#gpt-2-architecture_1","title":"GPT-2 Architecture","text":"<ul> <li>12 Transformer blocks</li> <li>768-dimensional hidden states</li> <li>12 attention heads</li> <li>Vocabulary size: 50,257 tokens</li> </ul>    ![](images/gpt2-final.png)     ## Text Generation"},{"location":"slides/05_llm_implmentation/slides-2/#autoregressive-text-generation","title":"Autoregressive Text Generation","text":"<p>LLMs generate text one token at a time through an iterative process:</p> <ol> <li>Start with an input context (e.g., \"Hello, I am\")</li> <li>Model predicts the next token probability distribution</li> <li>Select the next token (highest probability = greedy decoding)</li> <li>Append token to context</li> <li>Repeat until desired length</li> </ol> <p></p> <p>Key Insight: The model consumes its own previous outputs as future inputs - this is the autoregressive property.</p>"},{"location":"slides/05_llm_implmentation/slides-2/#logits-to-token-selection","title":"Logits to Token Selection","text":"<p>The GPT output is logits, not probabilities.</p> <ul> <li>Logits \u2208 \u211d^V</li> <li>Softmax converts logits -&gt; probability distribution</li> <li>Argmax selects most likely token</li> </ul> <pre><code>probas = torch.softmax(logits, dim=-1)\ntoken_id = torch.argmax(probas)\n</code></pre> <p></p>"},{"location":"slides/05_llm_implmentation/slides-2/#understanding-the-generation-flow","title":"Understanding the Generation Flow","text":"<p>Input \u2192 Token IDs \u2192 Model \u2192 Logits \u2192 Softmax \u2192 Token Selection \u2192 Output</p> Step Operation Shape Transformation 1 Tokenize input \"Hello, I am\" \u2192 [15496, 11, 314, 716] 2 Model forward (1, 4) \u2192 (1, 4, 50257) 3 Extract last logits (1, 4, 50257) \u2192 (1, 50257) 4 Softmax logits \u2192 probabilities 5 Argmax select token ID 257 \u2192 \"a\" 6 Concatenate extend sequence for next iteration <p></p>"},{"location":"slides/05_llm_implmentation/slides-2/#greedy-decoding-loop","title":"Greedy Decoding Loop","text":"<p>Greedy decoding selects the highest-probability token at each step.</p> <p>Properties:</p> <ul> <li>Deterministic</li> <li>Fast</li> <li>Often repetitive / dull</li> </ul> <p></p> <p>Greedy decoding = always take the most confident step.</p> <ul> <li>Does Greedy Decoding produce best text?</li> <li>Why not generate whole sentences?</li> <li>How different from encoder and decoder models?</li> </ul>"},{"location":"slides/05_llm_implmentation/slides-2/#why-untrained-models-generate-gibberish","title":"Why Untrained Models Generate Gibberish","text":"<p>Before training:</p> <ul> <li>Weights are random</li> <li>Logits are random</li> <li>Token probabilities are near-uniform</li> </ul> <p>With vocab size = 50,257:</p> <p>Initial probability \u2248 1 / 50,257 \u2248 0.00002</p> <p>Thus generated text is effectively random noise.</p>     ## Training the GPT"},{"location":"slides/05_llm_implmentation/slides-2/#training-overview","title":"Training Overview","text":"<ul> <li>Training Loop</li> <li>Loss Optimization</li> <li>Data Loading</li> <li>Model Evaluation</li> <li>Using Pretrained weights</li> </ul>"},{"location":"slides/05_llm_implmentation/slides-2/#evaluating-generative-text-models","title":"Evaluating Generative Text Models","text":""},{"location":"slides/05_llm_implmentation/slides-2/#why-evaluation-matters","title":"Why Evaluation Matters","text":"<p>Before training, we need metrics to:</p> <ul> <li>Measure model performance quantitatively</li> <li>Track training progress</li> <li>Detect overfitting</li> <li>Compare different models</li> </ul> <p>Key Challenge: How do we numerically assess the quality of generated text?</p> <p>Solution: Use the model's own probability estimates vs true probabilties for the \"correct\" next tokens.</p>"},{"location":"slides/05_llm_implmentation/slides-2/#cross-entropy-loss-for-language-modeling","title":"Cross Entropy Loss for Language Modeling","text":"<p>The training objective: Maximize the probability of the correct next token</p> <p>Given:</p> <ul> <li>Inputs: Token IDs the model sees</li> <li>Targets: Token IDs the model should predict (inputs shifted by 1)</li> </ul> <pre><code>inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n                       [40,    1107, 588]])    #  \"I really like\"]\n\ntargets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n                        [1107, 588, 11311]])  #  \" really like chocolate\"]\n</code></pre> <p>Cross-entropy loss measures how well the model's predicted probabilities match the true next tokens.</p> <p>Mathematical Formula:</p> <p>$$ \\text{Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log P(x_i^{\\text{target}}) $$</p> <p>Where:</p> <ul> <li>$N$ = total number of tokens</li> <li>$P(x_i^{\\text{target}})$ = predicted probability for the correct token at position $i$</li> <li>The negative sign converts it to a loss (we want to maximize probability = minimize negative log probability)</li> </ul>"},{"location":"slides/05_llm_implmentation/slides-2/#calculating-cross-entropy-loss","title":"Calculating Cross Entropy Loss","text":""},{"location":"slides/05_llm_implmentation/slides-2/#understanding-cross-entropy-loss","title":"Understanding Cross Entropy Loss","text":"<p>Step-by-step computation</p> <ol> <li>Get logits from model: shape <code>(batch, seq_len, vocab_size)</code></li> <li>Apply softmax to get probabilities</li> <li>Extract target probabilities: probability assigned to correct tokens</li> <li>Apply logarithm: log probabilities are more numerically stable</li> <li>Average over all tokens</li> <li>Negate: we minimize negative log-likelihood</li> </ol> <pre><code># PyTorch does all 6 steps in one function:\nloss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n</code></pre> <p>Initial loss (untrained model): ~10.99</p> <p>Target loss (well-trained): approaches ~0</p>"},{"location":"slides/05_llm_implmentation/slides-2/#perplexity-interpretable-metric","title":"Perplexity: Interpretable Metric","text":"<p>Perplexity = <code>exp(cross_entropy_loss)</code></p> <p>Interpretation: The effective vocabulary size the model is \"uncertain\" about at each step.</p> <p>Interpretation:</p> <ul> <li>Effective number of equally likely tokens</li> <li>Lower is better</li> </ul> <pre><code>perplexity = torch.exp(loss)\n# For initial loss of 10.79: perplexity \u2248 48,725\n# Meaning: model is unsure among ~48,725 tokens\n</code></pre> Loss Perplexity Interpretation 10.79 48,725 Random guessing (vocab size: 50,257) 5.0 148 Moderate uncertainty 2.0 7.4 Low uncertainty 0.5 1.65 Very confident"},{"location":"slides/05_llm_implmentation/slides-2/#training-vs-validation-loss","title":"Training vs Validation Loss","text":"<p>During training, split data into training and validation sets</p> <pre><code>train_ratio = 0.90\nsplit_idx = int(train_ratio * len(text_data))\ntrain_data = text_data[:split_idx]\nval_data = text_data[split_idx:]\n</code></pre> <p>What to watch for:</p> <ul> <li>Training loss decreases, Validation loss decreases -&gt; Model is learning and generalizing</li> <li>Training loss decreases, Validation loss increases -&gt; OVERFITTING!</li> </ul>"},{"location":"slides/05_llm_implmentation/slides-2/#processing-data-for-training","title":"Processing Data for Training","text":"<ul> <li>Inputs and targets are created by shifting token IDs by one position</li> <li>Given a text corpus</li> <li>split into training and validation sets</li> <li>for each set encode it to token IDs</li> <li>create input-target pairs</li> </ul> <p>In Pytorch</p> <ul> <li>Dataset Encodes all text to token IDs</li> <li>DataLoader handles batching and shuffling:</li> </ul>"},{"location":"slides/05_llm_implmentation/slides-2/#the-training-loop","title":"The Training Loop","text":"<p>Training updates model weights to minimize loss through backpropagation and gradient descent.</p> <p></p>"},{"location":"slides/05_llm_implmentation/slides-2/#core-code-training-function","title":"Core Code: Training Function","text":"<pre><code>    for epoch in range(num_epochs):\n        model.train()  # Enable dropout\n\n        for input_batch, target_batch in train_loader:\n            optimizer.zero_grad()  # Reset gradients\n\n            loss = calc_loss_batch(input_batch, target_batch, model, device)\n            loss.backward()        # Calculate gradients\n            optimizer.step()       # Update weights\n</code></pre>"},{"location":"slides/05_llm_implmentation/slides-2/#loading-and-saving-model-weights","title":"Loading and Saving Model Weights","text":"<p>Persistence Matters</p> <p>We must save trained models to:</p> <ul> <li>Avoid retraining</li> <li>Share models with others</li> <li>Resume training later</li> <li>Deploy to production</li> </ul>"},{"location":"slides/05_llm_implmentation/slides-2/#core-code-save-and-load","title":"Core Code: Save and Load","text":"<pre><code># ============ SAVE ============\n\ntorch.save(model.state_dict(), \"model.pth\")\n\n# Save model + optimizer (for resuming training)\ntorch.save({\n    \"model_state_dict\": model.state_dict(),\n    \"optimizer_state_dict\": optimizer.state_dict(),\n}, \"model_and_optimizer.pth\")\n\n\n# ============ LOAD ============\n# Load weights into fresh model\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.load_state_dict(torch.load(\"model.pth\", map_location=device))\nmodel.eval()  # Set to evaluation mode\n\n# Resume training\ncheckpoint = torch.load(\"model_and_optimizer.pth\", map_location=device)\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.load_state_dict(checkpoint[\"model_state_dict\"])\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\noptimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\nmodel.train()  # Set to training mode\n</code></pre>"},{"location":"slides/05_llm_implmentation/slides-2/#key-points","title":"Key Points","text":"<p><code>state_dict</code>: Dictionary mapping layer names to parameter tensors</p> <p><code>map_location</code>: Ensures model loads on correct device (CPU/GPU)</p> <p><code>model.eval()</code> vs <code>model.train()</code>:</p> <ul> <li><code>eval()</code>: Disables dropout, batch norm uses running stats</li> <li><code>train()</code>: Enables dropout, batch norm uses batch stats</li> </ul> <p>Why save optimizer state?</p> <ul> <li>AdamW stores momentum and adaptive learning rate history per parameter</li> <li>Without it, optimizer resets \u2192 suboptimal convergence</li> <li>Essential for resuming training</li> </ul>"},{"location":"slides/05_llm_implmentation/slides-2/#llm-loss-surfaces","title":"LLM Loss Surfaces","text":"<p>LLM training optimizes a high-dimensional non-convex loss surface defined by:</p> <p>L(\u03b8) = \u2212E[log p_\u03b8(token\u209c\u208a\u2081 | context\u209c)]</p> <p>Key properties:</p> <ul> <li>Billions of parameters</li> <li>Extremely overparameterized</li> <li>Many equivalent minima</li> <li>Flat basins dominate</li> </ul>"},{"location":"slides/05_llm_implmentation/slides-2/#summary-key-takeaways","title":"Summary: Key Takeaways","text":""},{"location":"slides/05_llm_implmentation/slides-2/#text-generation-section-47","title":"Text Generation (Section 4.7)","text":"<ul> <li>Autoregressive: one token at a time</li> <li>Greedy decoding: always pick highest probability</li> <li>Context window limits how far back model \"remembers\"</li> </ul>"},{"location":"slides/05_llm_implmentation/slides-2/#evaluation-section-51","title":"Evaluation (Section 5.1)","text":"<ul> <li>Cross entropy loss measures prediction quality</li> <li>Perplexity = effective vocabulary uncertainty</li> <li>Training/validation split detects overfitting</li> </ul>"},{"location":"slides/05_llm_implmentation/slides-2/#training-section-52","title":"Training (Section 5.2)","text":"<ul> <li>Standard PyTorch loop: zero_grad \u2192 forward \u2192 backward \u2192 step</li> <li>AdamW optimizer with weight decay</li> <li>Monitor both losses to detect overfitting</li> </ul>"},{"location":"slides/05_llm_implmentation/slides-2/#model-persistence-section-54","title":"Model Persistence (Section 5.4)","text":"<ul> <li>Save <code>state_dict</code> for efficient storage</li> <li>Save optimizer state to resume training</li> <li>Use <code>model.eval()</code> for consistent inference</li> </ul>"},{"location":"slides/05_llm_implmentation/slides/","title":"Slides","text":"# LLMs : A Hands-on Approach   ### GPT - 2  : Model Architecture and Implementation"},{"location":"slides/05_llm_implmentation/slides/#topics-covered","title":"Topics Covered","text":"<ul> <li>Transformer Architecture Review<ul> <li>Self-Attention Mechanism Recap</li> <li>Causal Masking in Decoder-Only Models</li> <li>Multi-Head Attention</li> </ul> </li> <li>GPT-2 Architecture<ul> <li>Layer Normalization</li> <li>Self-Attention in GPT-2</li> <li>Feed-Forward Network (FFN)</li> <li>Residual Connections</li> </ul> </li> <li>Implementing GPT-2 from Scratch</li> </ul>"},{"location":"slides/05_llm_implmentation/slides/#models-of-the-week","title":"Models of the Week","text":"<p>stepfun-ai/Step-3.5-Flash</p> <ul> <li>SOTA Open Source model</li> <li>199B parameters, Vocabulary Size - 128K, Context Length - 256K</li> <li>45 Transformer layers</li> <li>Attention<ul> <li>num_attention_heads: 64</li> <li>head_dim: 128</li> </ul> </li> <li>Coding and Agentic use</li> </ul> <p>arcee-ai/Trinity-Large-Preview</p> <ul> <li>~398B parameters, Vocabulary Size - 200192, Context Length - 8192, 512K</li> <li>60 Transformer layers</li> <li>Attention<ul> <li>num_attention_heads: 48</li> <li>head_dim: 128</li> </ul> </li> </ul>"},{"location":"slides/05_llm_implmentation/slides/#recap-self-attention-mechanism","title":"Recap : Self-Attention Mechanism","text":"<ul> <li>The token embeddings alone are not sufficient.</li> <li>We want dynamic, context-dependent representations of each token.</li> <li>Self-attention allows each token to attend to all other tokens in the sequence to gather relevant context.</li> </ul>"},{"location":"slides/05_llm_implmentation/slides/#recap-self-attention-mechanism_1","title":"Recap : Self-Attention Mechanism","text":"<p>We project the token embedding to three learned projection spaces:</p> <p>PROJECTION === \"Matrix Multiplication\"</p> <ul> <li>Query (Q): $q_i = x_i W_q$</li> <li>Key (K): $k_i = x_i W_k$</li> <li>Value (V): $v_i = x_i W_v$</li> </ul> <p></p>"},{"location":"slides/05_llm_implmentation/slides/#introducing-q-k-v-vectors","title":"Introducing Q, K, V vectors","text":"<p>We transform each input token into three different vectors:</p> <ul> <li>Token embedding: x_i</li> <li>Query (Q): What am I looking for?</li> <li>Key (K): What do I have to offer?</li> <li>Value (V): What information do I carry?</li> </ul> <p></p>"},{"location":"slides/05_llm_implmentation/slides/#scaled-dot-product-attention","title":"Scaled Dot-Product Attention","text":"$$ \\mathbf{Attention}(Q, K, V) = \\mathbf{softmax}\\left(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\right)V $$   <ul> <li>$$d_k \\text{ is the dimensionality of the key vectors (used for scaling).}$$</li> <li>without $\\sqrt{d_k}$, dot products grow with dimension \u2192 softmax saturates \u2192 tiny gradients.</li> </ul>"},{"location":"slides/05_llm_implmentation/slides/#causal-attention-masking-in-decoder-only-models","title":"Causal  Attention (Masking) in Decoder-Only Models","text":"<ul> <li>In decoder-only models, we predict next token based on previous tokens</li> <li>Note : During training, we predict all tokens in parallel</li> <li>To prevent information leakage from future tokens, we apply a causal mask to the attention scores</li> <li>At all time steps, each token can only attend to earlier tokens and itself</li> </ul>"},{"location":"slides/05_llm_implmentation/slides/#multi-head-attention","title":"Multi-Head Attention","text":""},{"location":"slides/05_llm_implmentation/slides/#stacking-multiple-attention-heads","title":"Stacking multiple attention heads","text":"<ul> <li>Perform multiple self-attention calculations in parallel</li> <li>Independent set of learned weight matrices (Wq, Wk, Wv) and  output vector for each head.</li> <li>Concatenate all to produce one context vector for each token.</li> <li>Multiple heads -&gt; attend to input sentence simultaneously -&gt; different relationships and patterns in the data.</li> </ul> <p>$ MultiHead(Q,K,V) = Concat(head_1,...,head_h)W_O $ </p> <p>where $ head_i = Attention(QW_{q_i}, KW_{k_i}, VW_{v_i}) $</p> <p></p>   # GPT-2 Model  ### Architecture and Implementation"},{"location":"slides/05_llm_implmentation/slides/#generative-pre-trained-transformer-gpt","title":"Generative Pre-trained Transformer (GPT)","text":"<ul> <li>Decoder only Transformer architecture</li> <li>Pre-trained on large corpus of text data (40GB of internet text) using self-supervised next-token prediction.</li> <li>Demonstrated  that general purpose models for textual understanding are incredibly effective without using any task-specific architectures or modifications</li> </ul>"},{"location":"slides/05_llm_implmentation/slides/#language-modeling-objective","title":"Language Modeling Objective","text":"<ul> <li> <p>Predict the next token in a sequence given all previous tokens</p> </li> <li> <p>P(x) = \u220f P(x_i | x_1, ..., x_{i-1})</p> </li> </ul> <p></p>"},{"location":"slides/05_llm_implmentation/slides/#auto-regressive-text-generation","title":"Auto-regressive Text Generation","text":"<ul> <li>Given a prompt, GPT-2 generates text one token at a time</li> <li>At each step, the model predicts the next token based on all previous tokens</li> <li>The predicted token is appended to the input sequence for the next prediction</li> </ul>"},{"location":"slides/05_llm_implmentation/slides/#gpt-2-architecture","title":"GPT-2 Architecture","text":"<ul> <li>GPT model stacks multiple transformer decoder blocks</li> <li>Each block has:</li> <li>Masked Multi-Head Self-Attention layer</li> <li>Feed-Forward Neural Network (FFN)</li> <li>Layer Normalization and Residual Connections</li> <li>Final output layer</li> </ul> <p>Ref : Radford et al., 2019 \"Language Models are Unsupervised Multitask Learners\" </p>"},{"location":"slides/05_llm_implmentation/slides/#detailed-gpt-2-architecture","title":"Detailed GPT-2 Architecture","text":"<pre><code>GPT_CONFIG_124M = {\n\"vocab_size\": 50257, # 50000 BPE merges + 256 byte tokens + 1 special token\n\"context_length\": 1024, # max length of input sequences\n\"emb_dim\": 768,\n\"n_heads\": 12,\n\"n_layers\": 12,\n\"drop_rate\": 0.1,\n\"qkv_bias\": False\n}\n</code></pre>"},{"location":"slides/05_llm_implmentation/slides/#inputs-to-the-gpt-2-model","title":"Inputs to the GPT-2 Model","text":"<ul> <li>Input Text -&gt; Tokenization -&gt; Token IDs</li> <li>Input Encodings:</li> <li>Token Embeddings: Represent the meaning of each token</li> <li>Positional Encodings: Represent the position of each token in the sequence</li> <li>Combined to form the input to the first transformer block</li> </ul>"},{"location":"slides/05_llm_implmentation/slides/#inputs-to-the-gpt-2-model_1","title":"Inputs to the GPT-2 Model","text":"<ul> <li>Transformers have no inherent notion of sequence order, Without positional info, 'dog bites man' = 'man bites dog' to the model</li> <li>GPT-2 uses LEARNED positional embeddings (vs sinusoidal in original Transformer)</li> <li>Both embeddings are simply added element-wise (not concatenated)</li> </ul>"},{"location":"slides/05_llm_implmentation/slides/#layer-normalization","title":"Layer Normalization","text":"<ul> <li>Gradient explosion/vanishing issues in deep networks</li> <li>Normalizes (centers) inputs across features for each token to have zero mean and unit variance</li> <li>Stabilizes training and improves convergence</li> <li>In GPT-2, applied before self-attention and feed-forward layers (Pre-LN)</li> </ul>"},{"location":"slides/05_llm_implmentation/slides/#layer-normalization_1","title":"Layer Normalization","text":"<ul> <li>Normalization</li> </ul>      $$     \\hat x = \\frac{x - \\mu}{\\sigma + \\epsilon}      $$   <ul> <li>LayerNorm formula:</li> </ul>   $$ \\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sigma + \\epsilon} \\cdot \\gamma + \\beta $$   <p>Where: - x: input vector for a token - \u03bc: mean of the elements in x - \u03c3: standard deviation of the elements in x - \u03b5: small constant for numerical stability - \u03b3, \u03b2: learnable parameters for scaling and shifting</p>"},{"location":"slides/05_llm_implmentation/slides/#self-attention-in-gpt-2","title":"Self-Attention in GPT-2","text":"<ul> <li>Multi-Head Self-Attention mechanism</li> <li>Attention heads : 12</li> <li>Head dimension : 64</li> <li>Context length : 1024 tokens</li> <li>Context Vector Size per token : 768 (12 heads * 64 dim)</li> </ul>"},{"location":"slides/05_llm_implmentation/slides/#self-attention-in-gpt-2_1","title":"Self-Attention in GPT-2","text":"<ul> <li> <p>Each Transformer Block contains </p> <ul> <li>A Multi-Head Self-Attention layer</li> <li>Each head computes attention using Q, K, V</li> <li>Causal mask applied to prevent attending to future tokens and dropout for regularization</li> </ul> <p></p> </li> </ul>"},{"location":"slides/05_llm_implmentation/slides/#feed-forward-network-ffn","title":"Feed-Forward  Network (FFN)","text":"<ul> <li>A Feed-Forward Network (FFN) is applied independently to each token's representation after the self-attention layer.</li> <li>Affine transformation followed by a non-linear activation function.</li> </ul>"},{"location":"slides/05_llm_implmentation/slides/#feed-forward-network-ffn_1","title":"Feed-Forward  Network (FFN)","text":"<ul> <li>A Feed-Forward Network (FFN) is applied independently to each token's representation after the self-attention layer.</li> <li>Affine transformation followed by a non-linear activation function.</li> </ul>  A Linear Layer is defined as: $ \\text{Linear}(x) = xW^T + b $"},{"location":"slides/05_llm_implmentation/slides/#feed-forward-network-ffn-in-gpt-2","title":"Feed-Forward  Network (FFN) in GPT-2","text":"<ul> <li>Each transformer block contains a Feed-Forward Network (FFN)</li> <li>FFN has two linear layers with GELU activation in between</li> </ul>  $$ \\text{FFN}(x) = \\text{Linear}_2(\\text{GELU}(\\text{Linear}_1(x))) $$"},{"location":"slides/05_llm_implmentation/slides/#gelu-activation-function","title":"GELU Activation Function","text":"<ul> <li>Gaussian Error Linear Unit (GELU) is  used in GPT-2's FFN</li> <li>Smooth approximation of ReLU, allows small negative values to pass through</li> </ul>  $\\mathbf{GELU(x) = 0.5 \\cdot x \\cdot (1 + \\tanh[\\sqrt{2/\\pi}(x + 0.044715 \\cdot x^3)])}$"},{"location":"slides/05_llm_implmentation/slides/#ffn-in-gpt-2","title":"FFN in GPT-2","text":"<ul> <li>Hidden layer size is 4 times the input/output size (3072 for GPT-2 small)</li> <li>Applies non-linear transformation to each token's representation independently</li> </ul> <ul> <li>Why 4x?</li> <li> <p>Empirically found to work well</p> </li> <li> <p>FFN layers contain most of the model's parameters!</p> </li> <li>For GPT-2 small: FFN has 768 -&gt; 3072 -&gt; 768, that's 768 * 3072 * 2 \u2248 4.7M params per block</li> <li>12 blocks \u00d7 4.7 M \u2248 56M params just in FFNs (almost half the model)</li> </ul>"},{"location":"slides/05_llm_implmentation/slides/#residual-connections","title":"Residual Connections","text":"<ul> <li>Help mitigate vanishing gradient problems in deep networks</li> <li>Add the input of a layer to its output before passing to the next layer</li> <li>Gradients get progressively smaller as they backpropagate through layers</li> <li>Preserve information from earlier layers, helping training stability</li> </ul>"},{"location":"slides/05_llm_implmentation/slides/#connecting-it-all-transformer-block-in-gpt-2","title":"Connecting it all : Transformer Block in GPT-2","text":"<ul> <li> <p>We have implemented the key components of a transformer block used in GPT-2:</p> <ul> <li>\u2713 Layer Normalization</li> <li>\u2713 Multi-Head Self-Attention with Causal Masking</li> <li>\u2713 Residual Connection</li> <li>\u2713 Feed-Forward Network (FFN)</li> </ul> </li> <li> <p>12 such blocks are stacked in GPT-2 small </p> </li> </ul>"},{"location":"slides/05_llm_implmentation/slides/#transformer-block-in-gpt-2","title":"Transformer Block in GPT-2","text":"<pre><code>def forward(self, x):\n    shortcut = x\n    x = self.norm1(x)\n    x = self.att(x)\n    x = self.dropout(x)\n    x = x + shortcut\n\n    shortcut = x\n    x = self.norm2(x)\n    x = self.ff(x)\n    x = self.dropout(x)\n    x = x + shortcut\n\n    return x\n</code></pre>"},{"location":"slides/05_llm_implmentation/slides/#gpt-2-model-implementation","title":"GPT-2 Model Implementation","text":"<ul> <li>We have all the components to implement GPT-2 from scratch</li> <li>Token Embeddings + Positional Encodings</li> <li>Stack of Transformer Blocks</li> <li>Final Layer Normalization and Output Layer</li> </ul>"},{"location":"slides/05_llm_implmentation/slides/#gpt-2-model-implementation_1","title":"GPT-2 Model Implementation","text":"<p>Model Initialization</p> <p><pre><code>## GPT-2 Model Initialization pseudo-code\ntok_emb = nn.Embedding(vocab_size, emb_dim)\npos_emb = nn.Embedding(context_length, emb_dim)\ndrop_emb = nn.Dropout(drop_rate)\ntrf_blocks = [TransformerBlock(cfg) for _ in range(n_layers)] # \nfinal_norm = LayerNorm(emb_dim)\nout_head = nn.Linear(emb_dim, vocab_size, bias=False)\n</code></pre> Forward Pass <pre><code>x = tok_embeds + pos_embeds\nx = self.drop_emb(x)\nx = self.trf_blocks(x)\nx = self.final_norm(x)\nlogits = self.out_head(x)\n</code></pre></p>"},{"location":"slides/05_llm_implmentation/slides/#gpt-2-parameter-count","title":"GPT-2 Parameter Count","text":"<ul> <li>GPT-2 small has ~124M parameters</li> <li>Major contributors:</li> <li>Token Embeddings: ~38M</li> <li>Transformer Blocks: ~85M<ul> <li>Self-Attention layers: ~23M</li> <li>Feed-Forward Networks: ~56M</li> </ul> </li> <li>Output Layer: ~38M</li> <li>Most parameters are in embeddings and FFN layers</li> </ul>"},{"location":"slides/05_llm_implmentation/slides/#next-steps","title":"Next Steps","text":"<ul> <li>Implement training loop with cross-entropy loss</li> <li>Integrate tokenizer for text input/output</li> <li>Load pretrained weights from HuggingFace for GPT-2</li> <li>Experiment with text generation</li> </ul>"},{"location":"slides/05_llm_implmentation/slides/#references","title":"References","text":"<ol> <li>Decoder-Only Transformers: The Workhorse of Generative LLMs</li> <li>The Illustrated GPT-2 (Visualizing Transformer Language Models)</li> <li>GPT-2 Paper</li> <li>Attention Is All You Need Paper</li> <li>Build LLMs from Scratch, Sebastian Raschka, Manning Publications, 2025</li> </ol>"},{"location":"slides/06_modern_architectures/notes/","title":"Modern Architectures","text":""},{"location":"slides/06_modern_architectures/notes/#outline-and-goals","title":"Outline and goals","text":"<ul> <li>Quick recap of the \u2018standard\u2019 transformer (what you implement)</li> <li>What do most of the large LMs have in common?</li> <li>What are common variations to the architecture / training process?</li> </ul> <p>Today\u2019s theme: the best way to learn is hands-on experience the second best way is to try to learn from others\u2019 experience</p>"},{"location":"slides/06_modern_architectures/notes/#starting-point-the-original-transformer","title":"Starting point: the \u2018original\u2019 transformer","text":"<p>Review: choices in the standard transformer</p> <p>Position embedding: sines and cosines $$ PE_{(pos,2i)} = sin(pos/10000^{2i/d_{\\text{model}}}) $$ $$ PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{\\text{model}}}) $$</p> <p>FFN: ReLU $$ \\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2 $$</p> <p>Norm type: post-norm, LayerNorm</p> <p>[Diagram showing the standard transformer architecture with Attention, Feed Forward, Add &amp; Norm blocks]</p>"},{"location":"slides/06_modern_architectures/notes/#what-we-implemented-gpt-2-decoder-only-transformer","title":"What we implemented \u2013 GPT- 2 (Decoder-only transformer)","text":"<p>Differences from original transformer: *   LayerNorm is in front of the block (pre-norm) *   Learned absolute position embeddings (not sinusoidal) *   FF layers use GeLU activation, not ReLU *   Bias terms included in linear layers and LayerNorm</p> <p>[Diagram showing GPT-2 architecture with Pre-Norm, Learned Position Embeddings, and GeLU]</p> <p>What do current models use?</p>"},{"location":"slides/06_modern_architectures/notes/#how-should-we-think-about-architectures","title":"How should we think about architectures?","text":"<p>Lots of architecture. We'll cover few of them in the next class. </p>"},{"location":"slides/06_modern_architectures/notes/#lets-look-at-the-data-on-dense-architectures","title":"Let\u2019s look at the data (on dense architectures)","text":"<p>Learn from the many other models (and papers) out there</p> <p>[Large table screenshot listing various models (GPT, T5, LLaMA, Mistral, etc.) and their specifications]</p> <p>We will talk through many major architecture and hyperparameter variants.</p> <ul> <li>What do all these models have in common?</li> <li>What parts vary?</li> <li>What can we learn from this?</li> </ul>"},{"location":"slides/06_modern_architectures/notes/#what-are-we-going-to-cover","title":"What are we going to cover?","text":"<p>Common architecture variations *   Activations, FFN *   Attention variants *   Position embeddings</p> <p>Hyperparameters that (do or don\u2019t) matter *   What is <code>ff_dim</code>? Do <code>multi_head</code> dims always sum to <code>model_dim</code>? *   How many vocab elements?</p>"},{"location":"slides/06_modern_architectures/notes/#architecture-variations","title":"Architecture variations..","text":"<p>Let\u2019s think about the core architecture piece</p> <p>[Table highlighting Norm, Position embedding, and Activations columns]</p>"},{"location":"slides/06_modern_architectures/notes/#pre-vs-post-norm","title":"Pre-vs-post norm","text":"<p>[Diagram comparing Post-LN Transformer vs Pre-LN Transformer]</p> <p>Post-LN Transformer</p> <p>$x_{l,i}^{post,1} = \\text{MultiHeadAtt}(x_{l,i}^{post}, [x_{l,1}^{post}, \\dots, x_{l,n}^{post}])$</p> <p>$x_{l,i}^{post,2} = x_{l,i}^{post} + x_{l,i}^{post,1}$</p> <p>$x_{l,i}^{post,3} = \\text{LayerNorm}(x_{l,i}^{post,2})$</p> <p>$x_{l,i}^{post,4} = \\text{ReLU}(x_{l,i}^{post,3}W^{1,l} + b^{1,l})W^{2,l} + b^{2,l}$</p> <p>$x_{l,i}^{post,5} = x_{l,i}^{post,3} + x_{l,i}^{post,4}$</p> <p>$x_{l+1,i}^{post} = \\text{LayerNorm}(x_{l,i}^{post,5})$</p> <p>Pre-LN Transformer</p> <p>$x_{l,i}^{pre,1} = \\text{LayerNorm}(x_{l,i}^{pre})$</p> <p>$x_{l,i}^{pre,2} = \\text{MultiHeadAtt}(x_{l,i}^{pre,1}, [x_{l,1}^{pre,1}, \\dots, x_{l,n}^{pre,1}])$</p> <p>$x_{l,i}^{pre,3} = x_{l,i}^{pre} + x_{l,i}^{pre,2}$</p> <p>$x_{l,i}^{pre,4} = \\text{LayerNorm}(x_{l,i}^{pre,3})$</p> <p>$x_{l,i}^{pre,5} = \\text{ReLU}(x_{l,i}^{pre,4}W^{1,l} + b^{1,l})W^{2,l} + b^{2,l}$</p> <p>$x_{l+1,i}^{pre} = x_{l,i}^{pre,3} + x_{l,i}^{pre,5}$</p> <p>Final LayerNorm: </p> <p>$x_{Final,i}^{pre} \\leftarrow \\text{LayerNorm}(x_{L+1,i}^{pre})$</p> <p>Set up LayerNorm so that it doesn\u2019t affect the main residual signal path (on the left)</p> <p>Almost all modern LMs use pre-norm</p>"},{"location":"slides/06_modern_architectures/notes/#new-things-double-norm","title":"New things \u2013 \u2018double\u2019 norm.","text":"<p>If putting LayerNorms in residual streams is bad.. Why not post-norm outside the stream?</p> <p>[Diagram showing LayerNorm added after the addition step, but outside the main residual path]</p> <p>Recent models: Grok, Gemma 2. Olmo 2 only does non-residual post norm</p>"},{"location":"slides/06_modern_architectures/notes/#layernorm-vs-rmsnorm","title":"LayerNorm vs RMSNorm","text":"<p>Original transformer: LayerNorm \u2013 normalizes the mean and variance across $d_{\\text{model}}$ $$ y = \\frac{x - E[x]}{\\sqrt{\\text{Var}[x] + \\epsilon}} * \\gamma + \\beta $$ Notable models: GPT3/2/1, OPT, GPT-J, BLOOM</p> <p>Many modern LMs: RMSNorm \u2013 does not subtract mean or add a bias term $$ y = \\frac{x}{\\sqrt{||x||_2^2 + \\epsilon}} * \\gamma $$ Notable models: LLaMA-family, PaLM, Chinchilla, T5</p>"},{"location":"slides/06_modern_architectures/notes/#why-rmsnorm","title":"Why RMSNorm?","text":"<p>Modern explanation \u2013 it\u2019s faster (and just as good). *   Fewer operations (no mean calculation) *   Fewer parameters (no bias term to store)</p> <p>$$ y = \\frac{x - E[x]}{\\sqrt{\\text{Var}[x] + \\epsilon}} * \\gamma + \\beta $$</p> <p>Does this explanation make sense?</p> Operator class % flop $\\Delta$ Tensor contraction 99.80 $\\square$ Stat. normalization 0.17 $\\bigcirc$ Element-wise 0.03 <p>Matrix multiplies are the vast majority of FLOPs (and memory) [Ivanov et al 2023]</p>"},{"location":"slides/06_modern_architectures/notes/#why-rmsnorm-2","title":"Why RMSNorm (2)","text":"<p>Important lesson: FLOPS are not runtime! (we will discuss this in far more detail later)</p> <p>[Ivanov et al 2023]</p> Operator class % flop % Runtime $\\Delta$ Tensor contraction 99.80 61.0 $\\square$ Stat. normalization 0.17 25.5 $\\bigcirc$ Element-wise 0.03 13.5 <p>[Diagram showing arithmetic intensity] Left top (\"43G\") is FLOPS Right top (\"153\") is the FLOP-to-memory ratio</p> <p>RMSNorm can still matter due to the importance of data movement</p>"},{"location":"slides/06_modern_architectures/notes/#more-generally-dropping-bias-terms","title":"More generally: dropping bias terms","text":"<p>Most modern transformers don\u2019t have bias terms.</p> <p>Original Transformer: $$ \\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2 $$</p> <p>Most implementations (if they\u2019re not gated): $$ \\text{FFN}(x) = \\sigma(xW_1)W_2 $$</p> <p>Reasons: memory (similar to RMSnorm) and optimization stability</p>"},{"location":"slides/06_modern_architectures/notes/#layernorm-recap","title":"LayerNorm: recap","text":"<ul> <li> <p>Basically everyone does pre-norm.</p> <ul> <li>Intuition \u2013 keep the good parts of residual connections</li> <li>Observations \u2013 nicer gradient propagation, fewer spike</li> <li>Some people add a second norm outside the residual stream (NOT post-norm)</li> </ul> </li> <li> <p>Most people do RMSnorm</p> <ul> <li>In practice, works as well as LayerNorm</li> <li>But, has fewer parameters to move around, which saves on wallclock time</li> <li>People more generally drop bias terms since the compute/param tradeoffs are not great.</li> </ul> </li> </ul>"},{"location":"slides/06_modern_architectures/notes/#activations","title":"Activations","text":"<p>A whole zoo of activations ..</p> <p>ReLU, GeLU, Swish, ELU, GLU, GeGLU, ReGLU, SeLU, SwiGLU, LiGLU</p> <p>What are these things? What do people use? Does it matter?</p>"},{"location":"slides/06_modern_architectures/notes/#a-few-of-the-common-activations","title":"A few of the common activations","text":"<p>ReLU $$ FF(x) = \\max(0, xW_1) W_2 $$ [Graph of ReLU] Notable models: Original transformer, T5, Gopher, Chinchilla, OPT</p> <p>GeLU $$ FF(x) = \\text{GELU}(xW_1)W_2 $$ $$ GELU(x) := x\\Phi(x) $$ [Graph of GeLU] Notable models: GPT1/2/3, GPTJ, GPT-Neox, BLOOM</p> <p>SwiGLU / GeGLU (next slide..) Notable models: Llama, PaLM, T5 v1.1, most models post 2023</p>"},{"location":"slides/06_modern_architectures/notes/#gated-activations-glu","title":"Gated activations (*GLU)","text":"<p>GLUs modify the \u2018first part\u2019 of a FF layer $$ FF(x) = \\max(0, xW_1) W_2 $$</p> <p>Instead of a linear + ReLU, augment the above with an (entrywise) linear term $$ \\max(0, xW_1) \\rightarrow \\max(0, xW_1) \\otimes (xV) $$</p> <p>This gives the gated variant (ReGLU) \u2013 note that we have an extra parameter (V) $$ \\text{FF}_{\\text{ReGLU}}(x) = (\\max(0, xW_1) \\otimes xV) W_2 $$</p>"},{"location":"slides/06_modern_architectures/notes/#gated-variants-of-standard-ff-layers","title":"Gated variants of standard FF layers","text":"<p>GeGLU $$ \\text{FFN}_{\\text{GEGLU}}(x, W, V, W_2) = (\\text{GELU}(xW) \\otimes xV)W_2 $$ Notable models: T5 v1.1, mT5, LaMDA, Phi3, Gemma 2, Gemma 3</p> <p>SwiGLU (swish is $x * \\text{sigmoid}(x)$) $$ \\text{FFN}_{\\text{SwiGLU}}(x, W, V, W_2) = (\\text{Swish}_1(xW) \\otimes xV)W_2 $$ Notable models: LLaMa 1/2/3, PaLM, Mistral, OlMo, most models post 2023</p> <p>Note: Gated models use smaller dimensions for the $d_{ff}$ by 2/3</p>"},{"location":"slides/06_modern_architectures/notes/#serial-vs-parallel-layers","title":"Serial vs Parallel layers","text":"<p>Normal transformer blocks are serial \u2013 they compute attention, then the MLP</p> <p>[Diagram of Serial Transformer Block] Add -&gt; Dropout -&gt; Feed-Forward -&gt; Norm -&gt; Add -&gt; Dropout -&gt; Attention -&gt; Norm</p> <p>Could we parallelize the transformer block?</p>"},{"location":"slides/06_modern_architectures/notes/#parallel-layers","title":"Parallel layers","text":"<p>A few models (GPTJ, PaLM, GPT-NeoX) do parallel layers. Originally in GPT-J</p> <p>Parallel Layers \u2013 We use a \"parallel\" formulation in each Transformer block (Wang &amp; Komatsuzaki, 2021), rather than the standard \"serialized\" formulation. Specifically, the standard formulation can be written as: $$ y = x + \\text{MLP}(\\text{LayerNorm}(x + \\text{Attention}(\\text{LayerNorm}(x))) $$ Whereas the parallel formulation can be written as: $$ y = x + \\text{MLP}(\\text{LayerNorm}(x)) + \\text{Attention}(\\text{LayerNorm}(x)) $$ The parallel formulation results in roughly 15% faster training speed at large scales, since the MLP and Attention input matrix multiplications can be fused. Ablation experiments showed a small quality degradation at 8B scale but no quality degradation at 62B scale, so we extrapolated that the effect of parallel layers should be quality neutral at the 540B scale.</p> <p>If implemented right, LayerNorm can be shared, and matrix multiplies can be fused</p> <p>Recent Models: Cohere Command A, Falcon 2 11B, Command R+</p>"},{"location":"slides/06_modern_architectures/notes/#summary-architectures","title":"Summary: architectures","text":"<p>Pre-vs-post norm: *   Everyone does pre-norm (except OPT350M), likely with good reason.</p> <p>Layer vs RMSnorm: *   RMSnorm has clear compute wins, sometimes even performance</p> <p>Gating: *   GLUs seem generally better, though differences are small</p> <p>Serial vs parallel layers: *   No extremely serious ablations, but has a compute win.</p> <p>[Table summary of architectures visible on the right]</p>"},{"location":"slides/06_modern_architectures/notes/#many-variations-in-position-embeddings","title":"Many variations in position embeddings","text":"<p>Sine embeddings: add sines and cosines that enable localization $$ Embed(x, i) = v_x + PE_{pos} $$ $$ PE_{(pos,2i)} = sin(pos/10000^{2i/d_{\\text{model}}}) $$ $$ PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{\\text{model}}}) $$ Notable models: Original transformer</p> <p>Absolute embeddings: add a position vector to the embedding $$ Embed(x, i) = v_x + u_i $$ Notable models: GPT1/2/3, OPT</p> <p>Relative embeddings: add a vector to the attention computation $$ e_{ij} = \\frac{x_i W^Q (x_j W^K + a_{ij}^K)^T}{\\sqrt{d_z}} $$ Notable models: T5, Gopher, Chinchilla</p> <p>Rope embeddings (next slides..) Notable models: GPTJ, PaLM, LLaMA Most 2024+ models</p>"},{"location":"slides/06_modern_architectures/notes/#rope-rotary-position-embeddings","title":"RoPE: rotary position embeddings","text":"<p>High level thought process: a relative position embedding should be some $f(x, i)$ s.t. $$ \\langle f(x, i), f(y, j) \\rangle = g(x, y, i - j) $$ That is, the attention function only gets to depend on the relative position (i-j). How do existing embeddings not fulfill this goal?</p> <ul> <li>Sine: Has various cross-terms that are not relative     $$ \\langle Embed(x, i), Embed(y, i) \\rangle = \\langle v_x, v_y \\rangle + \\langle PE_i, v_y \\rangle ... $$</li> <li>Absolute: obviously not relative</li> <li>Relative embeddings:     $$ e_{ij} = \\frac{x_i W^Q (x_j W^K + a_{ij}^K)^T}{\\sqrt{d_z}} $$     is not an inner product</li> </ul>"},{"location":"slides/06_modern_architectures/notes/#rope-rotary-position-embeddings_1","title":"RoPE: rotary position embeddings","text":"<p>How can we solve this problem? *   We want our embeddings to be invariant to absolute position *   We know that inner products are invariant to arbitrary rotation.</p> <p>[Diagram illustrating vectors rotating] Position independent embedding -&gt; Rotate \"we\" by '0 positions', \"know\" by '1 position' -&gt; Rotate \"we\" by '2 positions', \"know\" by '3 positions'. The relative angle between vectors remains constant.</p>"},{"location":"slides/06_modern_architectures/notes/#rope-rotary-position-embeddings_2","title":"RoPE: rotary position embeddings","text":"<p>There are many rotations, which one do you pick?</p> <p>[Diagram showing rotation in 2D pairs] Just pair up the coordinates and rotate them in 2d (motivation: complex numbers)</p> <p>[Su et al 2021]</p>"},{"location":"slides/06_modern_architectures/notes/#the-actual-rope-math","title":"The actual RoPE math","text":"<p>Multiply with sines and cosines</p> <p>$$ f_{{q,k}}(x_m, m) = \\boldsymbol{R}{\\Theta,m}^d \\boldsymbol{W}{{q,k}} x_m \\quad (14) $$</p> <p>$$ \\boldsymbol{R}{\\Theta,m}^d = \\begin{pmatrix} \\cos m\\theta_1 &amp; -\\sin m\\theta_1 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 \\ \\sin m\\theta_1 &amp; \\cos m\\theta_1 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; \\cos m\\theta_2 &amp; -\\sin m\\theta_2 &amp; \\cdots &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; \\sin m\\theta_2 &amp; \\cos m\\theta_2 &amp; \\cdots &amp; 0 &amp; 0 \\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; \\cos m\\theta{d/2} &amp; -\\sin m\\theta_{d/2} \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; \\sin m\\theta_{d/2} &amp; \\cos m\\theta_{d/2} \\end{pmatrix} \\quad (15) $$</p> <p>Difference with sine embeddings \u2013 not additive, no cross terms</p>"},{"location":"slides/06_modern_architectures/notes/#implementation-and-code-for-rope","title":"Implementation and code for RoPE","text":"<p><pre><code>query_states = self.q_proj(hidden_states)\nkey_states = self.k_proj(hidden_states)\nvalue_states = self.v_proj(hidden_states)\n\n# Flash attention requires the input to have the shape\n# batch_size x seq_length x head_dim x hidden_dim\n# therefore we just need to keep the original shape\nquery_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\nkey_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\nvalue_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\ncos, sin = self.rotary_emb(value_states, position_ids)\nquery_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n...\n</code></pre> Same stuff as the usual multi-head self attention below</p> <p>Note: embedding at each attention operation to enforce position invariance</p>"},{"location":"slides/06_modern_architectures/notes/#hyperparameters","title":"Hyperparameters","text":"<p>Transformer hyperparameter questions you might have had in 224n..</p> <ul> <li>How much bigger should the feedforward size be compared to hidden size?</li> <li>How many heads, and should num_heads always divide hidden size?</li> <li>What should my vocab size be?</li> </ul> <p>And other model setting questions *   Do people even regularize these huge LMs? *   How do people scale these models - very deep or very wide?</p> <p>Feedforward \u2013 model dimension ratio.</p> <p>$$ \\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2 $$</p> <p>There are two dimensions that are relevant \u2013 the feedforward dim ($d_{ff}$) and model dim ($d_{model}$). What should their relationship be?</p> <p>$$ \\boldsymbol{d_{ff} = 4 d_{model}} $$</p> <p>This is almost always true. There\u2019s just a few exceptions.</p>"},{"location":"slides/06_modern_architectures/notes/#surprising-consensus-hyperparameter-2","title":"Surprising (?) consensus hyperparameter 2","text":"<p>Head-dim*num-heads to model-dim ratio. As a reminder, slide from 224n.</p> <p>Multi-head self-attention is computationally efficient *   Even though we compute $h$ many attention heads, it's not really more costly.     *   We compute $XQ \\in \\mathbb{R}^{n \\times d}$, and then reshape to $\\mathbb{R}^{n \\times h \\times d/h}$. (Likewise for $XK, XV$.)</p>"},{"location":"slides/06_modern_architectures/notes/#the-total-cost-is-still-on2-d-same-as-single-head-attention-with-dimension-d","title":"&gt; *   The total cost is still $O(n^2 d)$, same as single-head attention with dimension $d$.","text":""},{"location":"slides/06_modern_architectures/notes/#what-are-typical-vocabulary-sizes","title":"What are typical vocabulary sizes?","text":"<p>Monolingual models \u2013 30-50k vocab</p> Model Token count Original transformer 37000 GPT 40257 GPT2/3 50257 T5/T5v1.1 32128 LLaMA 32000 <p>Multilingual / production systems 100-250k</p> Model Token count mT5 250000 PaLM 256000 GPT4 100276 Command A 255000 DeepSeek 100000 Qwen 15B 152064 Yi 64000 <p>Monolingual vocabs don\u2019t need to be huge, but multilingual ones do</p>"},{"location":"slides/06_modern_architectures/notes/#dropout-and-other-regularization","title":"Dropout and other regularization","text":"<p>Do we need regularization during pretraining?</p> <p>Arguments against: *   There is a lot of data (trillions of tokens), more than parameters. *   SGD only does a single pass on a corpus (hard to memorize)</p> <p>This is all quite reasonable.. but what do people do in practice?</p>"},{"location":"slides/06_modern_architectures/notes/#summary-hyperparameters","title":"Summary: hyperparameters","text":"<p>Feedforward *   Factor-of-4 rule of thumb (8/3 for GLUs) is standard (with some evidence)</p> <p>Head dim *   Head dim*Num head = D model is standard \u2013 but low to no validation</p>"},{"location":"slides/06_modern_architectures/notes/#attention-heads","title":"Attention heads","text":"<p>GQA / MQA : Saving inference costs by reducing the number of heads</p> <p>Sparse or sliding window attention (GPT4/Mistral): restricting the attention pattern to reduce compute cost</p>"},{"location":"slides/06_modern_architectures/notes/#gqamqa-reducing-attention-head-cost","title":"GQA/MQA \u2013 Reducing attention head cost","text":"<p>Let\u2019s think about the compute involved for attention</p> <p>[Diagram showing attention calculation $XQ K^T X^T$]</p> <p>$$ \\text{softmax} \\left( X Q K^T X^T \\right) XV = P \\cdot V = \\text{output} \\in \\mathbb{R}^{n \\times d} $$</p> <p>Total arithmetic operations ($bnd^2$), total memory accesses ($bnd + bhn^2 + d^2$)</p> <p>Arithmetic intensity is high $O \\left( \\left(\\frac{1}{k} + \\frac{1}{bn} \\right)^{-1} \\right)$ - we can keep our GPUs running</p>"},{"location":"slides/06_modern_architectures/notes/#gqamqa-reducing-attention-head-cost_1","title":"GQA/MQA \u2013 Reducing attention head cost","text":"<p>What about the incremental case when we generate text?</p> <p>Key difference: can\u2019t parallelize the generation process \u2013 needs to be step by step</p> <p>In this case \u2013 we need to incrementaly re-compute/update attention via the \u2018KV cache\u2019</p> <p>[Diagram showing KV Caching process] [Animation from https://medium.com/@joaolages/kv-caching-explained-276520203249]</p>"},{"location":"slides/06_modern_architectures/notes/#gqamqa-reducing-attention-head-cost_2","title":"GQA/MQA \u2013 Reducing attention head cost","text":"<p>What\u2019s the incremental arithmetic intensity?</p> <p>Total arithmetic operations ($bnd^2$), total memory accesses ($bn^2d + nd^2$)</p> <p>Arithmetic intensity is not good $O \\left( \\left(\\frac{n}{d} + \\frac{1}{b} \\right)^{-1} \\right)$ - need large batches + short seq length (n) or big model dimensions (d)</p> <p>Is there some way around this? The n/d term is difficult to reduce.</p>"},{"location":"slides/06_modern_architectures/notes/#mqa-just-have-fewer-key-dimensions","title":"MQA \u2013 just have fewer key dimensions.","text":"<p>Key idea \u2013 have multiple queries, but just one dimension for keys and values</p> <p>[Diagram showing Multi-Query Attention with shared Keys and Values]</p> <p>We have much fewer items to move in and out of memory (KV Cache)</p> <p>Total memory access ($bnd + bn^2k + nd^2$), Arithmetic intensity $O \\left( \\left(\\frac{1}{d} + \\frac{n}{dh} + \\frac{1}{b} \\right)^{-1} \\right)$</p> <p>[figure from https://blog.fireworks.ai/multi-query-attention-is-all-you-need-db072e758055]</p>"},{"location":"slides/06_modern_architectures/notes/#recent-extension-gqa","title":"Recent extension \u2013 GQA","text":"<p>Don\u2019t go all the way to one dimension of KV \u2013 have fewer dims</p> <p>[Diagram comparing Multi-head, Grouped-query, and Multi-query attention]</p> <p>Simple knob to control expressiveness (key-query ratio) and inference efficiency</p>"},{"location":"slides/06_modern_architectures/notes/#does-mqa-hurt-sometimes","title":"Does MQA hurt? Sometimes..","text":"<p>Small PPL hit w/ MQA [Shazeer 2019]</p> Attention $h$ $d_k, d_v$ $d_{ff}$ dev-PPL multi-head 8 128 8192 29.9 multi-query 8 128 9088 30.2 multi-head 1 128 9984 31.2 multi-head 2 64 9984 31.1 multi-head 4 32 9984 31.0 multi-head 8 16 9984 30.9 <p>Low/no hit w/ GQA [Ainslie 2023] [Graphs showing Performance vs Time per sample]</p>"},{"location":"slides/06_modern_architectures/notes/#sparse-sliding-window-attention","title":"Sparse / sliding window attention","text":"<p>Attending to the entire context can be expensive (quadratic).</p> <p>Build sparse / structured attention that trades off expressiveness vs runtime (GPT3)</p> <p>[Diagrams showing attention matrices: (a) Transformer (full), (b) Sparse Transformer (strided), (c) Sparse Transformer (fixed)]</p> <p>[Child et al 2019]</p>"},{"location":"slides/06_modern_architectures/notes/#sliding-window-attention","title":"Sliding window attention","text":"<p>Another variation on this idea \u2013 sliding window attention</p> <p>[Diagram comparing Vanilla Attention vs Sliding Window Attention]</p> <p>Just use the main part of the strided pattern \u2013 let depth extend effective context (Mistral)</p>"},{"location":"slides/06_modern_architectures/notes/#current-standard-trick-interleave-full-and-lr-attention","title":"Current standard trick \u2013 interleave \u2018full\u2019 and \u2018LR\u2019 attention","text":"<p>From Cohere Command A \u2013 Every $4^{\\text{th}}$ layer is a full attention</p> <p>[Diagram showing Command A Transformer Block sequence: SWA -&gt; SWA -&gt; SWA -&gt; Full]</p> <p>Long-range info via NoPE, short-range info via RoPE + SWA.</p> <p>Other models \u2013 LLaMA 4, Gemma does SWA+Full RoPE.</p>"},{"location":"slides/06_modern_architectures/notes/#recap-conclusion-etc","title":"Recap, conclusion, etc.","text":"<p>Many aspects (arch, hparams) of transformers are in common across the big LMs</p> <p>[Large table summarizing model parameters]</p> <p>Major differences? Position embeddings, activations, tokenization</p>"},{"location":"slides/06_modern_architectures/notes/#-","title":"---","text":""},{"location":"slides/06_modern_architectures/notes/#llm-architecture-evolution-2023-2025","title":"LLM Architecture Evolution: 2023 \u2192 2025","text":"<p>Big Picture: From scaling dense Transformers \u2192 engineering-driven efficiency</p> <ul> <li>MoE maturity</li> <li>Long-context stability</li> <li>Active parameters \u2260 total parameters</li> </ul>"},{"location":"slides/06_modern_architectures/notes/#2023-scaling-era-baseline-transformers","title":"2023: Scaling Era (Baseline Transformers)","text":"<p>Representative models: GPT-4, LLaMA-2</p> Component State Architecture Dense Transformers dominateSequential Attention \u2192 MLP blocks Normalization Pre-Norm becomes standard (Post-Norm effectively dead)Mix of LayerNorm and early RMSNorm FFN / Activations Transition to SwiGLU underway but not universal Position Encoding RoPE standard, limited long-context stability Context 8k\u201332k typical, 128k is exceptional Training BF16/FP16FLOPs scale \u2248 \"just make it bigger\" <p>Key limitation: Inefficient scaling: cost \u221d parameters</p>"},{"location":"slides/06_modern_architectures/notes/#2024-efficiency-structure-take-over","title":"2024: Efficiency &amp; Structure Take Over","text":"<p>Representative models: DeepSeek-V3, Mistral-Small-3, Grok-2.5</p> Component State Architecture MoE returns seriously (not research toys anymore)Hybrid attention (GQA, sliding window) Normalization RMSNorm wins (LayerNorm mostly gone) FFN / Activations SwiGLU is defaultShared experts appear in MoE Position Encoding NTK-scaled / extended RoPE Context 32k\u2013128k becomes normal Training Still BF16, but FLOPs efficiency matters <p>Key shift: Active parameters \u2260 total parameters. Cost/performance optimization mindset</p>"},{"location":"slides/06_modern_architectures/notes/#2025-engineering-first-llms","title":"2025: Engineering-First LLMs","text":"<p>Representative models: DeepSeek-R1, Llama-4, Qwen-3, Gemini-3</p> Component State Architecture MoE is mature (routing, shared experts, stability solved)Dense models still exist, but only when justified Block Design Experimental parallel Attention + MLP appearsKernel-fusion-friendly layouts Normalization RMSNorm + Pre-Norm is universal Position Encoding Long-context-safe RoPE variants are mandatory Context 128k is baselineFrontier models: 200k \u2192 1M tokens Training FP8 adoption beginsFLOPs/token is the real metric"},{"location":"slides/06_modern_architectures/slides-1/","title":"Slides 1","text":"# LLMs : A Hands-on Approach   ### Modern Architectures"},{"location":"slides/06_modern_architectures/slides-1/#topics-covered","title":"Topics Covered","text":"<ul> <li>GPT-2 Review<ul> <li>Training Loop</li> </ul> </li> <li>Modern LLM Architectures<ul> <li>Norm Types</li> <li>Activation Functions</li> <li>Positional Encodings</li> <li>Attention Variants</li> <li>Hyperparameters</li> </ul> </li> </ul>"},{"location":"slides/06_modern_architectures/slides-1/#recap-gpt-2-training-loop","title":"Recap : GPT-2 Training Loop","text":"<p>During Training we update model weights to minimize loss through backpropagation and gradient descent.</p> <p></p> <p>Training Loop in code</p>"},{"location":"slides/06_modern_architectures/slides-1/#for-epoch-in-rangenum_epochs-modeltrain-enable-dropout-for-input_batch-target_batch-in-train_loader-optimizerzero_grad-reset-gradients-loss-calc_loss_batchinput_batch-target_batch-model-device-lossbackward-calculate-gradients-optimizerstep-update-weights","title":"<pre><code>    for epoch in range(num_epochs):\n        model.train()  # Enable dropout\n\n        for input_batch, target_batch in train_loader:\n            optimizer.zero_grad()  # Reset gradients\n\n            loss = calc_loss_batch(input_batch, target_batch, model, device)\n            loss.backward()        # Calculate gradients\n            optimizer.step()       # Update weights\n</code></pre>","text":""},{"location":"slides/06_modern_architectures/slides-1/#loading-and-saving-model-weights","title":"Loading and Saving Model Weights","text":"<p>We must save trained models to:</p> <ul> <li>Avoid retraining</li> <li>Share models with others</li> <li>Resume training later</li> <li>Deploy to production</li> </ul> <pre><code># ============ SAVE ============\n\ntorch.save(model.state_dict(), \"model.pth\")\n\n# Save model + optimizer (for resuming training)\ntorch.save({\n    \"model_state_dict\": model.state_dict(),\n    \"optimizer_state_dict\": optimizer.state_dict(),\n}, \"model_and_optimizer.pth\")\n\n\n# ============ LOAD ============\n# Load weights into fresh model\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.load_state_dict(torch.load(\"model.pth\", map_location=device))\nmodel.eval()  # Set to evaluation mode\n\n# Resume training\ncheckpoint = torch.load(\"model_and_optimizer.pth\", map_location=device)\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.load_state_dict(checkpoint[\"model_state_dict\"])\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\noptimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\nmodel.train()  # Set to training mode\n</code></pre>"},{"location":"slides/06_modern_architectures/slides-1/#llm-loss-surfaces","title":"LLM Loss Surfaces","text":"<p>LLM training optimizes a high-dimensional non-convex loss surface defined by:</p> <p>$$ \\mathcal{L}(\\theta) = -\\frac{1}{N} \\sum_{i=1}^{N} \\log P_\\theta(x_i^{\\text{target}}) $$</p> <p>Key properties:</p> <ul> <li>Billions of parameters</li> <li>Extremely overparameterized</li> <li>Many equivalent minima</li> <li>Flat basins dominate</li> </ul> <p>More details in :  - Unveiling the Basin-Like Loss Landscape in Large Language Models  - Visualizing the Loss Landscape of Neural Nets</p>   ## Modern Architectures"},{"location":"slides/06_modern_architectures/slides-1/#gpt-2-architecture","title":"GPT-2 Architecture","text":"<p>Position embedding: learned, absolute</p> <p>FFN: GELU</p> <p>$$ \\text{FFN}(x) = GELU(xW_1 + b_1)W_2 + b_2 $$</p> <p>Norm type: Pre-Norm, LayerNorm</p> <p></p>"},{"location":"slides/06_modern_architectures/slides-1/#current-models","title":"Current Models","text":""},{"location":"slides/06_modern_architectures/slides-1/#llama-2-llama-3-and-qwen-3-architectures","title":"Llama 2, LlaMA 3 and Qwen 3 Architectures","text":"<p>Position embedding: RoPE (rotary position embeddings)</p> <p>FFN: *GLU variant (SwiGLU for LLaMA, GeGLU for Qwen)</p> <p>$$ \\textbf{SwiGLU}(x) = \\text{Swish}(xW) \\otimes (xV)W_2 $$</p> <p>Norm type: Post-Norm, RMSNorm</p> <p></p>"},{"location":"slides/06_modern_architectures/slides-1/#pre-norm-vs-post-norm","title":"Pre-Norm vs Post-Norm","text":"<p>Almost all models post-2020 use pre-norm.</p> <p></p> <p>Original Transformer : Post Norm</p> <p><code>x \u2192 Attention(x) \u2192 Add \u2192 LayerNorm \u2192 FFN \u2192 Add \u2192 LayerNorm</code></p> <p>GPT 2 : Pre-Norm</p> <p><code>x \u2192 LayerNorm \u2192 Attention \u2192 Add \u2192 LayerNorm \u2192 FFN \u2192 Add</code></p>"},{"location":"slides/06_modern_architectures/slides-1/#pre-norm-vs-post-norm_1","title":"Pre-Norm vs Post-Norm","text":"<p> <p>Why pre-norm wins:</p> <ul> <li>Better gradient flow throrugh residual connections. </li> <li>Practical evidence: almost all modern LLMs use pre-norm</li> </ul> <p>Note : Double norm also used in some models, but not as common as pre-norm. It applies LayerNorm both before and after the sub-layer.</p> <p>Question : <code>BERT was trained with post-norm and it was huge success. But most models use pre-norm. Why?</code></p>"},{"location":"slides/06_modern_architectures/slides-1/#layernorm-vs-rmsnorm","title":"LayerNorm vs RMSNorm","text":"Strong consensus toward RMSNorm    **LayerNorm** (original):   Normalize by subtracting mean and dividing by std dev, then scale ($\\gamma$) and shift ($\\beta$):  $$y = \\frac{x - E[x]}{\\sqrt{Var[x] + \\epsilon}} \\cdot \\gamma + \\beta$$  *Models* : GPT-1/2/3, OPT, GPT-J, BLOOM     **RMSNorm** (modern):   Drop the mean subtraction and bias term:  $$y = \\frac{x}{\\sqrt{||x||_2^2 + \\epsilon}} \\cdot \\gamma$$  *Models* : LLaMA family, DeepSeek V3, Qwen3 etc   <p>Why RMSNorm</p> <ul> <li>Fewer operations: RMSNorm requires fewer computations (no mean subtraction, no bias term) which reduces both FLOPs and memory bandwidth.</li> </ul>"},{"location":"slides/06_modern_architectures/slides-1/#dropping-bias-terms-in-ffn-and-layernorm","title":"Dropping bias Terms in FFN and LayerNorm","text":"<p>Most modern transformers have no bias terms in linear layers or LayerNorm.</p> <p>Original: $FFN(x) = \\max(0, xW_1 + b_1)W_2 + b_2$</p> <p>Modern: $FFN(x) = \\sigma(xW_1)W_2$ SiLU activation is used instead of ReLU, but the key point is that bias terms are removed.</p> <p>Reasons:</p> <ol> <li>Same memory/data movement argument as RMSNorm -- fewer parameters to load</li> <li>Optimization stability -- empirically, dropping bias terms stabilizes training of very large networks</li> </ol> <p>LayerNorm Recap</p> <ul> <li>Most models use RMSNorm</li> <li>Almost all models use pre-norm</li> </ul>"},{"location":"slides/06_modern_architectures/slides-1/#activations-gated-linear-units-strong-trend-toward-swiglugeglu","title":"Activations &amp; Gated Linear Units (Strong trend toward SwiGLU/GeGLU)","text":"<p>Evolution of activations:</p> Activation Formula Notable Models ReLU $FF(x) = \\max(0, xW_1)W_2$ Original transformer, T5, Gopher, OPT GeLU $FF(x) = GELU(xW_1)W_2$ where $GELU(x) = x\\Phi(x)$ GPT-1/2/3, GPT-J, BLOOM SwiGLU $FF(x) = (Swish(xW) \\otimes xV)W_2$ LLaMA 1/2/3, PaLM, Mistral, most post-2023 GeGLU $FF(x) = (GELU(xW) \\otimes xV)W_2$ T5 v1.1, mT5, Phi3, Gemma 2/3 <p>where <code>Swish(x) = x * sigmoid(x)</code> and $\\otimes$ is elementwise multiplication.</p>"},{"location":"slides/06_modern_architectures/slides-1/#gated-linear-units-glu","title":"Gated Linear Units (GLU)","text":"<p>What do GLUs do?</p> <ul> <li>GLUs add a gating mechanism</li> <li>Hidden representation element-wise multiplied by a gate $xV$ (learned linear projection)</li> <li>$xV$ controls information flow through the MLP</li> </ul> <p>$$\\text{Standard:} \\quad \\sigma(xW_1) \\rightarrow \\sigma(xW_1) \\otimes (xV) \\quad \\text{(gated)}$$</p> <p></p>"},{"location":"slides/06_modern_architectures/slides-1/#gated-linear-units-glu_1","title":"Gated Linear Units (GLU)","text":"<p>More number of parameters?</p> <p>The extra parameter V means GLU models have 3 weight matrices (W, V, W2) instead of 2. </p> <p>How to keep parameter count the same? (memory is the real bottleneck, not compute)</p>"},{"location":"slides/06_modern_architectures/slides-1/#gated-linear-units-glu_2","title":"Gated Linear Units (GLU)","text":"<p>More number of parameters?</p> <p>The extra parameter V means GLU models have 3 weight matrices (W, V, W2) instead of 2. </p> <p>How to keep parameter count the same? (memory is the real bottleneck, not compute)</p> <p>Scale the FF Params </p>"},{"location":"slides/06_modern_architectures/slides-1/#gated-linear-units-glu_3","title":"Gated Linear Units (GLU)","text":"<p>More number of parameters?</p> <p>The extra parameter V means GLU models have 3 weight matrices (W, V, W2) instead of 2. </p> <p>How to keep parameter count the same? (memory is the real bottleneck, not compute)</p> <p>Scale the FF Params  - Standard MLP     -  $W_1 \\in \\mathbb{R}^{d \\times d_{ff}}$ + $W_2 \\in \\mathbb{R}^{d_{ff} \\times d}$ = $2 \\cdot d \\cdot d_{ff}$ params     - Total FFN params = $2 \\cdot d \\cdot 4 d$ = $8 \\cdot d^2$.</p>"},{"location":"slides/06_modern_architectures/slides-1/#gated-linear-units-glu_4","title":"Gated Linear Units (GLU)","text":"<p>More number of parameters?</p> <p>The extra parameter V means GLU models have 3 weight matrices (W, V, W2) instead of 2. </p> <p>How to keep parameter count the same? (memory is the real bottleneck, not compute)</p> <p>Scale the FF Params  - Standard MLP     -  $W_1 \\in \\mathbb{R}^{d \\times d_{ff}}$ + $W_2 \\in \\mathbb{R}^{d_{ff} \\times d}$ = $2 \\cdot d \\cdot d_{ff}$ params     - Total FFN params = $2 \\cdot d \\cdot 4 d$ = $8 \\cdot d^2$.</p> <ul> <li>Gated MLP: <ul> <li>$W \\in \\mathbb{R}^{d \\times d_{ff}}$ + $V \\in \\mathbb{R}^{d \\times d_{ff}}$ + $W_2 \\in \\mathbb{R}^{d_{ff} \\times d}$ = $3 \\cdot d \\cdot d_{ff}$ params. </li> </ul> </li> </ul>"},{"location":"slides/06_modern_architectures/slides-1/#gated-linear-units-glu_5","title":"Gated Linear Units (GLU)","text":"<p>More number of parameters?</p> <p>The extra parameter V means GLU models have 3 weight matrices (W, V, W2) instead of 2. </p> <p>How to keep parameter count the same? (memory is the real bottleneck, not compute)</p> <p>Scale the FF Params  - Standard MLP     -  $W_1 \\in \\mathbb{R}^{d \\times d_{ff}}$ + $W_2 \\in \\mathbb{R}^{d_{ff} \\times d}$ = $2 \\cdot d \\cdot d_{ff}$ params     - Total FFN params = $2 \\cdot d \\cdot 4 d$ = $8 \\cdot d^2$.</p> <ul> <li> <p>Gated MLP: </p> <ul> <li>$W \\in \\mathbb{R}^{d \\times d_{ff}}$ + $V \\in \\mathbb{R}^{d \\times d_{ff}}$ + $W_2 \\in \\mathbb{R}^{d_{ff} \\times d}$ = $3 \\cdot d \\cdot d_{ff}$ params. </li> </ul> </li> <li> <p>To match: </p> <ul> <li>set $d_{ff}^{gated} = \\frac{2}{3} d_{ff}^{standard} = \\frac{2}{3} \\cdot 4d = \\frac{8}{3}d$. </li> <li>Total FFN params = $3 \\cdot d \\cdot \\frac{8}{3}d = 8 \\cdot d^2$.</li> </ul> </li> </ul> <p>Scaling Factors:  - Standard MLP: $d_{ff} = 4 \\cdot d$ - Gated MLP: $d_{ff} = \\frac{8}{3} \\cdot d \\approx 2.67 \\cdot d$</p>"},{"location":"slides/06_modern_architectures/slides-1/#serial-vs-parallel-layers","title":"Serial vs Parallel Layers","text":"<p>Normal transformer blocks are serial \u2013 they compute attention, then the MLP</p> <p>Standard transformer block can be written as:</p> <p>$$  y = x + \\text{MLP}(\\text{LayerNorm}(x + \\text{Attention}(\\text{LayerNorm}(x)))  $$</p> <p>Whereas the parallel formulation can be written as:</p> <p>$$  y = x + \\text{MLP}(\\text{LayerNorm}(x)) + \\text{Attention}(\\text{LayerNorm}(x))  $$</p> <p></p> <p>image source</p>"},{"location":"slides/06_modern_architectures/slides-1/#further-reading","title":"Further Reading","text":"<p>The Big LLM Architecture Comparison</p>"},{"location":"slides/06_modern_architectures/slides-2/","title":"Slides 2","text":"# LLMs : A Hands-on Approach   ### Modern Architectures"},{"location":"slides/06_modern_architectures/slides-2/#topics-covered","title":"Topics Covered","text":"<ul> <li>Modern LLM Architectures<ul> <li>Positional Encodings</li> <li>Attention Variants</li> </ul> </li> </ul>"},{"location":"slides/06_modern_architectures/slides-2/#recap-current-models","title":"Recap : Current Models","text":""},{"location":"slides/06_modern_architectures/slides-2/#pre-norm-vs-post-norm","title":"Pre-Norm vs Post-Norm","text":"<p>Almost all models post-2020 use pre-norm.</p> <p></p> <p>Original Transformer : Post Norm</p> <p><code>x \u2192 Attention(x) \u2192 Add \u2192 LayerNorm \u2192 FFN \u2192 Add \u2192 LayerNorm</code></p> <p>GPT 2 : Pre-Norm</p> <p><code>x \u2192 LayerNorm \u2192 Attention \u2192 Add \u2192 LayerNorm \u2192 FFN \u2192 Add</code></p>"},{"location":"slides/06_modern_architectures/slides-2/#layernorm-vs-rmsnorm","title":"LayerNorm vs RMSNorm","text":"Strong consensus toward RMSNorm    **LayerNorm** (original):   Normalize by subtracting mean and dividing by std dev, then scale ($\\gamma$) and shift ($\\beta$):  $$y = \\frac{x - E[x]}{\\sqrt{Var[x] + \\epsilon}} \\cdot \\gamma + \\beta$$  *Models* : GPT-1/2/3, OPT, GPT-J, BLOOM     **RMSNorm** (modern):   Drop the mean subtraction and bias term:  $$y = \\frac{x}{\\sqrt{||x||_2^2 + \\epsilon}} \\cdot \\gamma$$  *Models* : LLaMA family, DeepSeek V3, Qwen3 etc   <p>Why RMSNorm</p> <ul> <li>Fewer operations: RMSNorm requires fewer computations (no mean subtraction, no bias term) which reduces both FLOPs and memory bandwidth.</li> </ul>"},{"location":"slides/06_modern_architectures/slides-2/#activations-gated-linear-units-strong-trend-toward-swiglugeglu","title":"Activations &amp; Gated Linear Units (Strong trend toward SwiGLU/GeGLU)","text":"<p>Evolution of activations:</p> Activation Formula Notable Models ReLU $FF(x) = \\max(0, xW_1)W_2$ Original transformer, T5, Gopher, OPT GeLU $FF(x) = GELU(xW_1)W_2$ where $GELU(x) = x\\Phi(x)$ GPT-1/2/3, GPT-J, BLOOM SwiGLU $FF(x) = (Swish(xW) \\otimes xV)W_2$ LLaMA 1/2/3, PaLM, Mistral, most post-2023 GeGLU $FF(x) = (GELU(xW) \\otimes xV)W_2$ T5 v1.1, mT5, Phi3, Gemma 2/3 <p>where <code>Swish(x) = x * sigmoid(x)</code> and $\\otimes$ is elementwise multiplication.</p>"},{"location":"slides/06_modern_architectures/slides-2/#gated-linear-units-glu","title":"Gated Linear Units (GLU)","text":"<p>What do GLUs do?</p> <ul> <li>GLUs add a gating mechanism</li> <li>Hidden representation element-wise multiplied by a gate $xV$ (learned linear projection)</li> <li>$xV$ controls information flow through the MLP</li> </ul> <p>$$\\text{Standard:} \\quad \\sigma(xW_1) \\rightarrow \\sigma(xW_1) \\otimes (xV) \\quad \\text{(gated)}$$</p> <p></p>"},{"location":"slides/06_modern_architectures/slides-2/#position-encodings","title":"Position Encodings","text":"<p>Evolution:</p> Type How it works Models Sinusoidal Add fixed sin/cos to embedding Original Transformer Absolute (learned) Add learned position vector $u_i$ to embedding GPT-1/2/3, OPT Relative Add learned bias to attention scores T5, Gopher, Chinchilla ALiBi Linear attention bias BLOOM NoPE No position embedding at all SmolLM3, Kimi Linear RoPE Rotate query/key vectors GPT-J, PaLM, LLaMA, all 2024+ models"},{"location":"slides/06_modern_architectures/slides-2/#position-encodings_1","title":"Position Encodings","text":""},{"location":"slides/06_modern_architectures/slides-2/#why-do-we-need-position-encodings","title":"Why do we need Position Encodings?","text":"<p>Attention is a position-agnostic operation</p> <ul> <li>Treats the input as a set, not a sequence</li> <li>No inherent notion of order or position</li> </ul> <p>Example:</p> <p>The dog chased another dog vs Another dog chased the dog</p> <ul> <li>Both have the same set of tokens</li> <li>But different meanings</li> </ul> <p>Solution: Add positional encodings to inject order information into the model</p> <p></p>"},{"location":"slides/06_modern_architectures/slides-2/#integer-posion-encoding","title":"Integer Posion Encoding","text":"<ul> <li>Add the integer postion directly into embeddings.</li> <li>Problems:<ul> <li>Position encoding magnitude greater than token embedding magnitude</li> <li>Model should separately learn to handle content and position, which can make learning harder.</li> </ul> </li> </ul>"},{"location":"slides/06_modern_architectures/slides-2/#binary-position-encoding","title":"Binary Position Encoding","text":""},{"location":"slides/06_modern_architectures/slides-2/#binary-position-encoding_1","title":"Binary Position Encoding","text":"<p>Problems:</p> <ul> <li>Hamming distance artifacts</li> <li>Sparse representations (most bits are zero, which can make learning harder)</li> <li>No lernable interpolation</li> </ul>"},{"location":"slides/06_modern_architectures/slides-2/#sinusoidal-position-encoding","title":"Sinusoidal Position Encoding","text":"<p>$$ PE_{(pos,2i)} = sin(pos/10000^{2i/d_{\\text{model}}}) $$</p> <p>$$ PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{\\text{model}}}) $$</p>"},{"location":"slides/06_modern_architectures/slides-2/#sinusoidal-position-encoding_1","title":"Sinusoidal Position Encoding","text":"<p>Why sin and cos**? </p> <p>$sin(a+b) = sin(a)cos(b) + cos(a)sin(b)$</p> <p>$cos(a+b) = cos(a)cos(b) - sin(a)sin(b)$</p> <p>PE(pos+k) is a linear function of PE(pos)</p> <p>So </p> <p>$$ \\begin{aligned} \\sin(\\omega \\cdot (p+k)) &amp;= \\sin(\\omega p)\\cos(\\omega k) + \\cos(\\omega p)\\sin(\\omega k) \\end{aligned} $$</p> <p>$$ \\begin{aligned} \\cos(\\omega \\cdot (p+k)) &amp;= \\cos(\\omega p)\\cos(\\omega k) - \\sin(\\omega p)\\sin(\\omega k) \\end{aligned} $$</p> <p>Downsides :</p> <p>Sinusoidal PE is added to embeddings: $x_p = x_{token} + PE(p)$</p> <p>Then projected: $q_p = W_q x_p$, $k_p = W_kx_p$</p> <ul> <li>Position and content are entangled before attention</li> <li>Relative position is implicit, not structural</li> <li>Attention must learn how to extract distance</li> </ul>"},{"location":"slides/06_modern_architectures/slides-2/#absolute-vs-relative-position-encodings","title":"Absolute vs Relative Position Encodings","text":"<ul> <li> <p>Absolute position encodings  - Unique encoding to each position in the sequence. </p> </li> <li> <p>Relative position encodings - Encodes the relative distance between tokens, rather than their absolute position.</p> </li> </ul> <p></p> <p>Why do we need them?</p> <ul> <li> <p>\"The cat sat\" should have similar relationships whether at positions [5,6,7] or [105,106,107]</p> </li> <li> <p>Absolute encodings make it harder to learn patterns based on relative distance (e.g., \"the word two positions to the left of X\")</p> </li> </ul> <p>Benefits:</p> <ul> <li>Learn patterns based on relative distance between tokens</li> <li>More important for many tasks than absolute position</li> <li>No reliance on fixed absolute positions</li> </ul>"},{"location":"slides/06_modern_architectures/slides-2/#relative-position-encoding-example","title":"Relative Position Encoding Example","text":"<p>Example sentence: \"The dog chased another dog\"</p> <p>When attending from position 2 (\"chased\"):</p> Position Token Absolute Relative to pos 2 0 The 0 -2 1 dog 1 -1 2 chased 2 0 (self) 3 another 3 +1 4 dog 4 +2 <p>In practice:</p> <p></p> <pre><code># T5-style relative attention bias\n# For position i attending to position j:\nrelative_position = j - i  # e.g., \"dog\"(1) \u2192 \"chased\"(2) = 1 - 2 = -1\n\n# Bias added to attention scores (learned, not fixed):\nattention_score = (q_i @ k_j) + bias[clip(relative_position, -max_dist, max_dist)]\n</code></pre> <p>Key differences from absolute: - Same relative pattern at any absolute position (e.g., \"-1\" always means \"previous token\") - Model learns one bias per relative distance, not per absolute position</p>"},{"location":"slides/06_modern_architectures/slides-2/#position-encoding-desirable-properties","title":"Position Encoding : Desirable Properties","text":"<ol> <li>Inject position information into the model</li> <li>Allow generalization to longer sequences than seen during training</li> <li>Facilitate learning of relative position patterns (e.g., \"the word   two positions to the left of X\")</li> <li>Be computationally efficient (not too many parameters or FLOPs)</li> <li>Be compatible with attention mechanism (e.g., allow position information to influence attention scores)</li> </ol>"},{"location":"slides/06_modern_architectures/slides-2/#rotary-position-embeddings-rope","title":"Rotary Position Embeddings (RoPE)","text":"<p>We want attention scores to depend only on relative position $(i - j)$, not absolute positions. </p> <p>Mathematically, find $f(x, i)$ such that:</p> <p>$$\\langle f(x, i), f(y, j) \\rangle = g(x, y, i-j)$$</p> <p>RoPE's key idea: - Instead of adding PE to the input <code>x</code>, apply rotation to the query and key vectors based on their position.  - Position information is directly encoded in the attention scores</p> <p></p>"},{"location":"slides/06_modern_architectures/slides-2/#encoding-position-as-a-rotation","title":"Encoding position as a rotation","text":"<p>Rotating a 2D vector by an angle $\\theta$ </p>    $$ \\begin{bmatrix} x' \\\\\\\\ y' \\end{bmatrix} = \\begin{bmatrix} \\cos \\theta &amp; -\\sin \\theta \\\\\\\\ \\sin \\theta &amp; \\cos \\theta \\end{bmatrix} \\begin{bmatrix} x \\\\\\\\ y \\end{bmatrix} $$    <p>Rotating a word vector</p> <p>Given a word vector $x = (x_1, x_2)$ at position $m$, we can rotate it by an angle $\\theta_m$ to get the position-aware vector $x'$:</p>   $$ \\begin{bmatrix} x'_1 \\\\\\\\ x'_2 \\end{bmatrix} = \\begin{bmatrix} \\cos \\theta_m &amp; -\\sin \\theta_m \\\\\\\\ \\sin \\theta_m &amp; \\cos \\theta_m \\end{bmatrix} \\begin{bmatrix} x_1 \\\\\\\\ x_2 \\end{bmatrix} $$"},{"location":"slides/06_modern_architectures/slides-2/#dot-product-of-rotated-vectors","title":"Dot product of rotated vectors","text":"<p>Let's rotate two vectors $q$ and $k$ by angles $\\theta_q$ and $\\theta_k$ respectively. </p> <p>$q' = R_{(\\theta_q)} q$</p> <p>$k' = R_{(\\theta_k)} k$</p> <p>The dot product of the rotated vectors is: $$\\begin{aligned} q' \\cdot k' &amp;= (R(\\theta_q) q) \\cdot (R(\\theta_k) k) \\ &amp;= q^T R(\\theta_q)^T R(\\theta_k) k \\ &amp;= q^T R(\\theta_k - \\theta_q) k \\end{aligned}$$</p> <p>Dot products depend only on relative rotation.</p> <p>Now attention scores depend on : <code>q</code>, <code>k</code>, and the relative angle $(\\theta_k - \\theta_q)$, which encodes the relative position between the two tokens.</p> <p></p>"},{"location":"slides/06_modern_architectures/slides-2/#rotations-in-higher-dimensions","title":"Rotations in higher dimensions","text":"<ul> <li> <p>In higher dimensions, we can apply rotations in multiple planes</p> <ul> <li>Example: In 4D space, rotate independently in $(x_1, x_2)$ plane and $(x_3, x_4)$ plane</li> </ul> </li> <li> <p>For a $d$-dimensional vector:</p> <ul> <li>Apply $\\frac{d}{2}$ independent rotations</li> <li>Each rotation has its own angle $\\theta_m$</li> <li>Encodes position information compatible with attention mechanism</li> </ul> </li> <li> <p>Example: Model with hidden dimension $d = 512$</p> <ul> <li>Apply 256 independent rotations</li> <li>Each pair of dimensions gets rotated by different frequency</li> <li>Creates a rich positional representation</li> </ul> </li> </ul>"},{"location":"slides/06_modern_architectures/slides-2/#rotation-in-m-dimensions","title":"Rotation in <code>m</code> dimensions","text":"<p>$\\theta_i = B^{-2i/d} \\quad \\text{where } B \\text{ is the base (typically 10000)}$</p> <p>The wavelength of dimension $i$ is:</p> <p>$$\\lambda_i = \\frac{2\\pi}{\\theta_i} = 2\\pi \\cdot B^{2i/d}$$</p> <p>This creates a geometric progression of wavelengths:</p> <ul> <li>Shortest wavelength (highest freq, $i=0$): $\\lambda_{\\min} = 2\\pi \\approx 6.28$ tokens</li> <li>Longest wavelength (lowest freq, $i=d/2-1$): $\\lambda_{\\max} = 2\\pi \\cdot B \\approx 62,832$ tokens (when $B=10000$)</li> </ul>"},{"location":"slides/06_modern_architectures/slides-2/#computing-attention-with-rope","title":"Computing Attention with RoPE","text":"<pre><code>class RoPEAttention(nn.Module):\n\n    -----    \n\n    def forward(self, x: torch.Tensor):    \n        # Apply RoPE to queries and keys\n        q = apply_rotary_emb(q, self.freqs_cis[:seq_len])\n        k = apply_rotary_emb(k, self.freqs_cis[:seq_len])\n\n        # Scaled dot-product attention\n        scores = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n</code></pre> <p>Key implementation details:</p> <ol> <li>Precompute frequencies: Calculate $\\theta_i = 10000^{-2i/d}$ for all dimension pairs once</li> <li>Apply once per forward pass: Rotate Q and K by position-dependent angles</li> <li>No extra parameters: RoPE is fully deterministic, no learned weights</li> </ol>"},{"location":"slides/06_modern_architectures/slides-2/#other-hyperparameters","title":"Other Hyperparameters","text":"<ul> <li>How many attention heads?</li> <li>How many layers?</li> <li>Hidden dimension size?</li> <li>FFN dimension size?</li> <li>Vocab size?</li> </ul>"},{"location":"slides/06_modern_architectures/slides-2/#other-hyperparameters_1","title":"Other Hyperparameters","text":""},{"location":"slides/06_modern_architectures/slides-2/#ffn-dimensions","title":"FFN Dimensions","text":"<ul> <li>Typical FFN dimension is 4x the hidden dimension (dff\u200b=4\u22c5dmodel\u200b)</li> <li>in GLU variants (SwiGLU, GeGLU), use 2/3 scaling to keep parameter count same</li> </ul>"},{"location":"slides/06_modern_architectures/slides-2/#_1","title":"Slides 2","text":""},{"location":"slides/06_modern_architectures/slides-2/#_2","title":"Slides 2","text":""},{"location":"slides/06_modern_architectures/slides-2/#number-of-attention-heads","title":"Number of attention heads","text":"<ul> <li>Common choices are 16, 32, or 64 heads for large models. </li> <li>The number of heads is often chosen to be a divisor of the hidden dimension for simplicity (e.g., 512 hidden dimension with 16 heads means each head has 32 dimensions).</li> </ul>"},{"location":"slides/06_modern_architectures/slides-2/#_3","title":"Slides 2","text":""},{"location":"slides/06_modern_architectures/slides-2/#_4","title":"Slides 2","text":""},{"location":"slides/06_modern_architectures/slides-2/#vocabulary-size","title":"Vocabulary size","text":"<p>Monolingual models: 30-50K tokens (Original Transformer: 37K, GPT-2/3: 50K, LLaMA: 32K)</p> <p>Multilingual/Production: 100-250K tokens (GPT-4: 100K, PaLM: 256K, Qwen: 152K, Command A: 255K)</p>   ## Efficent Attention Variants"},{"location":"slides/06_modern_architectures/slides-2/#mha-overview","title":"MHA Overview","text":"<p>$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$</p> <p>$$ \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h)W^O $$</p> <ul> <li>Scaled Dot-Product Attention:</li> <li>$Q, K, V$: Query, Key, and Value matrices derived from input embeddings</li> <li>$d_k$: The dimensionality of the keys (and queries)</li> <li>$W^O$: Output projection matrix that combines the outputs of all heads into a single vector.</li> <li>$h$: The number of parallel attention heads.</li> </ul> <p></p>"},{"location":"slides/06_modern_architectures/slides-2/#mha-generation-phase","title":"MHA - Generation Phase","text":"<p>Generation Phase (Autoregressive Decoding):</p> <ul> <li>Generate tokens one at a time autoregressively</li> <li>At each step, compute attention over all previously generated tokens</li> <li>Keys and Values for past tokens remain unchanged</li> <li>Only the new token's Query needs to attend to all previous Keys/Values</li> </ul> <p></p>"},{"location":"slides/06_modern_architectures/slides-2/#mha-generation-phase_1","title":"MHA - Generation Phase","text":"<p>Problem: Without caching, we recompute K and V for all previous tokens at every step!</p> <ul> <li>Step 1: Compute K, V for token 1</li> <li>Step 2: Recompute K, V for tokens 1, 2</li> <li>Step 3: Recompute K, V for tokens 1, 2, 3</li> <li>...</li> <li>Step N: Recompute K, V for all N tokens \u2192 O(N\u00b2) redundant computation</li> </ul> <p>Solution: KV Cache \u2014 Store computed K and V vectors and reuse them</p> <p></p>"},{"location":"slides/06_modern_architectures/slides-2/#kv-cache-memory-usage","title":"KV Cache Memory Usage","text":"<p>KV Cache size needed for MHA:</p> Parameter Value KV head dim 128 Sequence length 32,768 Num KV heads 128 <p>Per-layer calculation:</p> <p>$$\\text{KV cache} = \\text{seq_len} \\times \\text{num_heads} \\times \\text{head_dim} \\times 2 \\times 2$$</p> <p>$$= 32768 \\times 128 \\times 128 \\times 2 \\times 2 = \\textbf{2.14 GB per layer}$$</p> <p>Total for 61 layers: ~ 131 GB (~ 4 MB per token)</p> Attention Type Memory per Token MHA 4 MB <p>Solution: Reduce KV heads to 1 (MQA) or use more efficient attention variants (GQA, MLA)</p>"},{"location":"slides/06_modern_architectures/slides-2/#multi-query-attention-mqa","title":"Multi-Query Attention (MQA)","text":"<p>Key Insight: Reduce number of KV heads to 1 = MQA (Multi-Query Attention)</p> <p>$$\\text{KV Cache} = 32768 \\times 128 \\times 1 \\times 2 \\times 2 = \\textbf{16 MB per layer}$$</p> <p>For all 61 layers: ~1 GB total (~32 KB per token)</p> <p>128\u00d7 reduction in KV cache memory!</p> Attention Type Memory per Token MHA 4 MB MQA 32 KB"},{"location":"slides/06_modern_architectures/slides-2/#grouped-query-attention-gqa","title":"Grouped-Query Attention (GQA)","text":"<p>Key Insight: Use 1 KV head for each group of query heads (e.g., 1 KV head for 8 query heads)</p> <ul> <li>Balance between MHA (full expressivity) and MQA (maximum efficiency)</li> <li>If 8 Query heads share 1 KV head</li> </ul> Attention Type Memory per Token MHA 4 MB MQA 32 KB GQA 500 KB <p>8\u00d7 reduction compared to MHA, while preserving more expressivity than MQA</p>"},{"location":"slides/06_modern_architectures/slides-2/#multi-head-latent-attention-mla","title":"Multi-head Latent Attention (MLA)","text":"<p>Key Insight: Compress K and V into a low-dimensional latent vector before caching</p> <ul> <li>Project hidden states into a compressed latent representation $c^{KV}$ (e.g., 512 dims)</li> <li>Cache only the compressed latent, not full K and V</li> <li>Reconstruct K and V from latent during attention computation</li> </ul>"},{"location":"slides/06_modern_architectures/slides-2/#deepseek-v3-mla","title":"DeepSeek V3 MLA","text":"<p>KV Cache size for MLA (DeepSeek V3):</p> Parameter Value KV compression dim ($d_c$) 512 RoPE dim ($d_r$) 64 Sequence length 32,768 <p>Per-layer calculation:</p> <p>$$\\text{MLA cache} = \\text{seq_len} \\times (d_c + d_r) \\times 2$$</p> <p>$$= 32768 \\times (512 + 64) \\times 2 = \\textbf{37.7 MB per layer}$$</p> <p>Total for 61 layers: ~ 2.3 GB (~ 70 KB per token)</p> Attention Type Memory per Token MHA 4 MB MQA 32 KB GQA (8 groups) 500 KB MLA 70 KB <p>~57\u00d7 reduction compared to MHA, ~7\u00d7 reduction compared to GQA</p>"},{"location":"slides/06_modern_architectures/slides-2/#qk-norm-attention-softmax-stability","title":"QK-Norm (Attention Softmax Stability)","text":"<p>Concept: Apply LayerNorm to Query ($Q$) and Key ($K$) vectors before computing dot-product attention.</p> <p>LayerNorm has evolved from Post-Norm (Transformer) to Pre-Norm (GPT-2) and now directly into the attention mechanism (QK-Norm) </p> <p></p>"},{"location":"slides/06_modern_architectures/slides-2/#further-reading","title":"Further Reading","text":"<p>The Big LLM Architecture Comparison</p>"},{"location":"slides/06_modern_architectures/slides/","title":"Slides","text":"# LLMs : A Hands-on Approach   ### Modern Architectures"},{"location":"slides/06_modern_architectures/slides/#topics-covered","title":"Topics Covered","text":"<ul> <li>GPT-2 Review<ul> <li>Training Loop</li> </ul> </li> <li>Modern LLM Architectures<ul> <li>Norm Types</li> <li>Activation Functions</li> <li>Positional Encodings</li> <li>Attention Variants</li> <li>Hyperparameters</li> </ul> </li> </ul>"},{"location":"slides/06_modern_architectures/slides/#recap-gpt-2-training-loop","title":"Recap : GPT-2 Training Loop","text":"<p>During Training we update model weights to minimize loss through backpropagation and gradient descent.</p> <p></p> <p>Training Loop in code</p>"},{"location":"slides/06_modern_architectures/slides/#for-epoch-in-rangenum_epochs-modeltrain-enable-dropout-for-input_batch-target_batch-in-train_loader-optimizerzero_grad-reset-gradients-loss-calc_loss_batchinput_batch-target_batch-model-device-lossbackward-calculate-gradients-optimizerstep-update-weights","title":"<pre><code>    for epoch in range(num_epochs):\n        model.train()  # Enable dropout\n\n        for input_batch, target_batch in train_loader:\n            optimizer.zero_grad()  # Reset gradients\n\n            loss = calc_loss_batch(input_batch, target_batch, model, device)\n            loss.backward()        # Calculate gradients\n            optimizer.step()       # Update weights\n</code></pre>","text":""},{"location":"slides/06_modern_architectures/slides/#loading-and-saving-model-weights","title":"Loading and Saving Model Weights","text":"<p>We must save trained models to:</p> <ul> <li>Avoid retraining</li> <li>Share models with others</li> <li>Resume training later</li> <li>Deploy to production</li> </ul> <pre><code># ============ SAVE ============\n\ntorch.save(model.state_dict(), \"model.pth\")\n\n# Save model + optimizer (for resuming training)\ntorch.save({\n    \"model_state_dict\": model.state_dict(),\n    \"optimizer_state_dict\": optimizer.state_dict(),\n}, \"model_and_optimizer.pth\")\n\n\n# ============ LOAD ============\n# Load weights into fresh model\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.load_state_dict(torch.load(\"model.pth\", map_location=device))\nmodel.eval()  # Set to evaluation mode\n\n# Resume training\ncheckpoint = torch.load(\"model_and_optimizer.pth\", map_location=device)\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.load_state_dict(checkpoint[\"model_state_dict\"])\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\noptimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\nmodel.train()  # Set to training mode\n</code></pre>"},{"location":"slides/06_modern_architectures/slides/#llm-loss-surfaces","title":"LLM Loss Surfaces","text":"<p>LLM training optimizes a high-dimensional non-convex loss surface defined by:</p> <p>$$ \\mathcal{L}(\\theta) = -\\frac{1}{N} \\sum_{i=1}^{N} \\log P_\\theta(x_i^{\\text{target}}) $$</p> <p>Key properties:</p> <ul> <li>Billions of parameters</li> <li>Extremely overparameterized</li> <li>Many equivalent minima</li> <li>Flat basins dominate</li> </ul> <p>More details in :  - Unveiling the Basin-Like Loss Landscape in Large Language Models  - Visualizing the Loss Landscape of Neural Nets</p>   ## Modern Architectures"},{"location":"slides/06_modern_architectures/slides/#gpt-2-architecture","title":"GPT-2 Architecture","text":"<p>Position embedding: learned, absolute</p> <p>FFN: GELU</p> <p>$$ \\text{FFN}(x) = GELU(xW_1 + b_1)W_2 + b_2 $$</p> <p>Norm type: Pre-Norm, LayerNorm</p> <p></p>"},{"location":"slides/06_modern_architectures/slides/#current-models","title":"Current Models","text":""},{"location":"slides/06_modern_architectures/slides/#llama-2-llama-3-and-qwen-3-architectures","title":"Llama 2, LlaMA 3 and Qwen 3 Architectures","text":"<p>Position embedding: RoPE (rotary position embeddings)</p> <p>FFN: *GLU variant (SwiGLU for LLaMA, GeGLU for Qwen)</p> <p>$$ \\textbf{SwiGLU}(x) = \\text{Swish}(xW) \\otimes (xV)W_2 $$</p> <p>Norm type: Post-Norm, RMSNorm</p> <p></p>"},{"location":"slides/06_modern_architectures/slides/#pre-norm-vs-post-norm","title":"Pre-Norm vs Post-Norm","text":"<p>Almost all models post-2020 use pre-norm.</p> <p></p> <p>Original Transformer : Post Norm</p> <p><code>x \u2192 Attention(x) \u2192 Add \u2192 LayerNorm \u2192 FFN \u2192 Add \u2192 LayerNorm</code></p> <p>GPT 2 : Pre-Norm</p> <p><code>x \u2192 LayerNorm \u2192 Attention \u2192 Add \u2192 LayerNorm \u2192 FFN \u2192 Add</code></p>"},{"location":"slides/06_modern_architectures/slides/#pre-norm-vs-post-norm_1","title":"Pre-Norm vs Post-Norm","text":"<p> <p>Why pre-norm wins:</p> <ul> <li>Better gradient flow throrugh residual connections. </li> <li>Practical evidence: almost all modern LLMs use pre-norm</li> </ul> <p>Note : Double norm also used in some models, but not as common as pre-norm. It applies LayerNorm both before and after the sub-layer.</p> <p>Question : <code>BERT was trained with post-norm and it was huge success. But most models use pre-norm. Why?</code></p>"},{"location":"slides/06_modern_architectures/slides/#layernorm-vs-rmsnorm","title":"LayerNorm vs RMSNorm","text":"Strong consensus toward RMSNorm    **LayerNorm** (original):   Normalize by subtracting mean and dividing by std dev, then scale ($\\gamma$) and shift ($\\beta$):  $$y = \\frac{x - E[x]}{\\sqrt{Var[x] + \\epsilon}} \\cdot \\gamma + \\beta$$  *Models* : GPT-1/2/3, OPT, GPT-J, BLOOM     **RMSNorm** (modern):   Drop the mean subtraction and bias term:  $$y = \\frac{x}{\\sqrt{||x||_2^2 + \\epsilon}} \\cdot \\gamma$$  *Models* : LLaMA family, DeepSeek V3, Qwen3 etc   <p>Why RMSNorm</p> <ul> <li>Fewer operations: RMSNorm requires fewer computations (no mean subtraction, no bias term) which reduces both FLOPs and memory bandwidth.</li> </ul>"},{"location":"slides/06_modern_architectures/slides/#dropping-bias-terms-in-ffn-and-layernorm","title":"Dropping bias Terms in FFN and LayerNorm","text":"<p>Most modern transformers have no bias terms in linear layers or LayerNorm.</p> <p>Original: $FFN(x) = \\max(0, xW_1 + b_1)W_2 + b_2$</p> <p>Modern: $FFN(x) = \\sigma(xW_1)W_2$ SiLU activation is used instead of ReLU, but the key point is that bias terms are removed.</p> <p>Reasons:</p> <ol> <li>Same memory/data movement argument as RMSNorm -- fewer parameters to load</li> <li>Optimization stability -- empirically, dropping bias terms stabilizes training of very large networks</li> </ol> <p>LayerNorm Recap</p> <ul> <li>Most models use RMSNorm</li> <li>Almost all models use pre-norm</li> </ul>"},{"location":"slides/06_modern_architectures/slides/#activations-gated-linear-units-strong-trend-toward-swiglugeglu","title":"Activations &amp; Gated Linear Units (Strong trend toward SwiGLU/GeGLU)","text":"<p>Evolution of activations:</p> Activation Formula Notable Models ReLU $FF(x) = \\max(0, xW_1)W_2$ Original transformer, T5, Gopher, OPT GeLU $FF(x) = GELU(xW_1)W_2$ where $GELU(x) = x\\Phi(x)$ GPT-1/2/3, GPT-J, BLOOM SwiGLU $FF(x) = (Swish(xW) \\otimes xV)W_2$ LLaMA 1/2/3, PaLM, Mistral, most post-2023 GeGLU $FF(x) = (GELU(xW) \\otimes xV)W_2$ T5 v1.1, mT5, Phi3, Gemma 2/3 <p>where <code>Swish(x) = x * sigmoid(x)</code> and $\\otimes$ is elementwise multiplication.</p>"},{"location":"slides/06_modern_architectures/slides/#gated-linear-units-glu","title":"Gated Linear Units (GLU)","text":"<p>What do GLUs do?</p> <ul> <li>GLUs add a gating mechanism</li> <li>Hidden representation element-wise multiplied by a gate $xV$ (learned linear projection)</li> <li>$xV$ controls information flow through the MLP</li> </ul> <p>$$\\text{Standard:} \\quad \\sigma(xW_1) \\rightarrow \\sigma(xW_1) \\otimes (xV) \\quad \\text{(gated)}$$</p> <p></p>"},{"location":"slides/06_modern_architectures/slides/#gated-linear-units-glu_1","title":"Gated Linear Units (GLU)","text":"<p>More number of parameters?</p> <p>The extra parameter V means GLU models have 3 weight matrices (W, V, W2) instead of 2. </p> <p>How to keep parameter count the same? (memory is the real bottleneck, not compute)</p>"},{"location":"slides/06_modern_architectures/slides/#gated-linear-units-glu_2","title":"Gated Linear Units (GLU)","text":"<p>More number of parameters?</p> <p>The extra parameter V means GLU models have 3 weight matrices (W, V, W2) instead of 2. </p> <p>How to keep parameter count the same? (memory is the real bottleneck, not compute)</p> <p>Scale the FF Params </p>"},{"location":"slides/06_modern_architectures/slides/#gated-linear-units-glu_3","title":"Gated Linear Units (GLU)","text":"<p>More number of parameters?</p> <p>The extra parameter V means GLU models have 3 weight matrices (W, V, W2) instead of 2. </p> <p>How to keep parameter count the same? (memory is the real bottleneck, not compute)</p> <p>Scale the FF Params  - Standard MLP     -  $W_1 \\in \\mathbb{R}^{d \\times d_{ff}}$ + $W_2 \\in \\mathbb{R}^{d_{ff} \\times d}$ = $2 \\cdot d \\cdot d_{ff}$ params     - Total FFN params = $2 \\cdot d \\cdot 4 d$ = $8 \\cdot d^2$.</p>"},{"location":"slides/06_modern_architectures/slides/#gated-linear-units-glu_4","title":"Gated Linear Units (GLU)","text":"<p>More number of parameters?</p> <p>The extra parameter V means GLU models have 3 weight matrices (W, V, W2) instead of 2. </p> <p>How to keep parameter count the same? (memory is the real bottleneck, not compute)</p> <p>Scale the FF Params  - Standard MLP     -  $W_1 \\in \\mathbb{R}^{d \\times d_{ff}}$ + $W_2 \\in \\mathbb{R}^{d_{ff} \\times d}$ = $2 \\cdot d \\cdot d_{ff}$ params     - Total FFN params = $2 \\cdot d \\cdot 4 d$ = $8 \\cdot d^2$.</p> <ul> <li>Gated MLP: <ul> <li>$W \\in \\mathbb{R}^{d \\times d_{ff}}$ + $V \\in \\mathbb{R}^{d \\times d_{ff}}$ + $W_2 \\in \\mathbb{R}^{d_{ff} \\times d}$ = $3 \\cdot d \\cdot d_{ff}$ params. </li> </ul> </li> </ul>"},{"location":"slides/06_modern_architectures/slides/#gated-linear-units-glu_5","title":"Gated Linear Units (GLU)","text":"<p>More number of parameters?</p> <p>The extra parameter V means GLU models have 3 weight matrices (W, V, W2) instead of 2. </p> <p>How to keep parameter count the same? (memory is the real bottleneck, not compute)</p> <p>Scale the FF Params  - Standard MLP     -  $W_1 \\in \\mathbb{R}^{d \\times d_{ff}}$ + $W_2 \\in \\mathbb{R}^{d_{ff} \\times d}$ = $2 \\cdot d \\cdot d_{ff}$ params     - Total FFN params = $2 \\cdot d \\cdot 4 d$ = $8 \\cdot d^2$.</p> <ul> <li> <p>Gated MLP: </p> <ul> <li>$W \\in \\mathbb{R}^{d \\times d_{ff}}$ + $V \\in \\mathbb{R}^{d \\times d_{ff}}$ + $W_2 \\in \\mathbb{R}^{d_{ff} \\times d}$ = $3 \\cdot d \\cdot d_{ff}$ params. </li> </ul> </li> <li> <p>To match: </p> <ul> <li>set $d_{ff}^{gated} = \\frac{2}{3} d_{ff}^{standard} = \\frac{2}{3} \\cdot 4d = \\frac{8}{3}d$. </li> <li>Total FFN params = $3 \\cdot d \\cdot \\frac{8}{3}d = 8 \\cdot d^2$.</li> </ul> </li> </ul> <p>Scaling Factors:  - Standard MLP: $d_{ff} = 4 \\cdot d$ - Gated MLP: $d_{ff} = \\frac{8}{3} \\cdot d \\approx 2.67 \\cdot d$</p>"},{"location":"slides/06_modern_architectures/slides/#serial-vs-parallel-layers","title":"Serial vs Parallel Layers","text":"<p>Normal transformer blocks are serial \u2013 they compute attention, then the MLP</p> <p>Standard transformer block can be written as:</p> <p>$$  y = x + \\text{MLP}(\\text{LayerNorm}(x + \\text{Attention}(\\text{LayerNorm}(x)))  $$</p> <p>Whereas the parallel formulation can be written as:</p> <p>$$  y = x + \\text{MLP}(\\text{LayerNorm}(x)) + \\text{Attention}(\\text{LayerNorm}(x))  $$</p> <p></p> <p>image source</p>"},{"location":"slides/06_modern_architectures/slides/#position-encodings","title":"Position Encodings","text":"<p>Evolution:</p> Type How it works Models Sinusoidal Add fixed sin/cos to embedding Original Transformer Absolute (learned) Add learned position vector $u_i$ to embedding GPT-1/2/3, OPT Relative Add learned bias to attention scores T5, Gopher, Chinchilla ALiBi Linear attention bias BLOOM NoPE No position embedding at all SmolLM3, Kimi Linear RoPE Rotate query/key vectors GPT-J, PaLM, LLaMA, all 2024+ models"},{"location":"slides/06_modern_architectures/slides/#position-encodings_1","title":"Position Encodings","text":""},{"location":"slides/06_modern_architectures/slides/#why-do-we-need-position-encodings","title":"Why do we need Position Encodings?","text":"<p>Attention is a position-agnostic operation</p> <ul> <li>Treats the input as a set, not a sequence</li> <li>No inherent notion of order or position</li> </ul> <p>Example:</p> <p>The dog chased another dog vs Another dog chased the dog</p> <ul> <li>Both have the same set of tokens</li> <li>But different meanings</li> </ul> <p>Solution: Add positional encodings to inject order information into the model</p> <p></p>"},{"location":"slides/06_modern_architectures/slides/#integer-posion-encoding","title":"Integer Posion Encoding","text":"<ul> <li>Add the integer postion directly into embeddings.</li> <li>Problems:<ul> <li>Position encoding magnitude greater than token embedding magnitude</li> <li>Model should separately learn to handle content and position, which can make learning harder.</li> </ul> </li> </ul>"},{"location":"slides/06_modern_architectures/slides/#binary-position-encoding","title":"Binary Position Encoding","text":""},{"location":"slides/06_modern_architectures/slides/#binary-position-encoding_1","title":"Binary Position Encoding","text":"<p>Problems:</p> <ul> <li>Hamming distance artifacts</li> <li>Sparse representations (most bits are zero, which can make learning harder)</li> <li>No lernable interpolation</li> </ul>"},{"location":"slides/06_modern_architectures/slides/#sinusoidal-position-encoding","title":"Sinusoidal Position Encoding","text":"<p>$$ PE_{(pos,2i)} = sin(pos/10000^{2i/d_{\\text{model}}}) $$</p> <p>$$ PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{\\text{model}}}) $$</p>"},{"location":"slides/06_modern_architectures/slides/#sinusoidal-position-encoding_1","title":"Sinusoidal Position Encoding","text":"<p>Why sin and cos**? </p> <p>$sin(a+b) = sin(a)cos(b) + cos(a)sin(b)$</p> <p>$cos(a+b) = cos(a)cos(b) - sin(a)sin(b)$</p> <p>PE(pos+k) is a linear function of PE(pos)</p> <p>So </p> <p>$$ \\begin{aligned} \\sin(\\omega \\cdot (p+k)) &amp;= \\sin(\\omega p)\\cos(\\omega k) + \\cos(\\omega p)\\sin(\\omega k) \\end{aligned} $$</p> <p>$$ \\begin{aligned} \\cos(\\omega \\cdot (p+k)) &amp;= \\cos(\\omega p)\\cos(\\omega k) - \\sin(\\omega p)\\sin(\\omega k) \\end{aligned} $$</p> <p>Downsides :</p> <p>Sinusoidal PE is added to embeddings: $x_p = x_{token} + PE(p)$</p> <p>Then projected: $q_p = W_q x_p$, $k_p = W_kx_p$</p> <ul> <li>Position and content are entangled before attention</li> <li>Relative position is implicit, not structural</li> <li>Attention must learn how to extract distance</li> </ul>"},{"location":"slides/06_modern_architectures/slides/#absolute-vs-relative-position-encodings","title":"Absolute vs Relative Position Encodings","text":"<ul> <li> <p>Absolute position encodings  - Unique encoding to each position in the sequence. </p> </li> <li> <p>Relative position encodings - Encodes the relative distance between tokens, rather than their absolute position.</p> </li> </ul> <p></p> <p>Why do we need them?</p> <ul> <li> <p>\"The cat sat\" should have similar relationships whether at positions [5,6,7] or [105,106,107]</p> </li> <li> <p>Absolute encodings make it harder to learn patterns based on relative distance (e.g., \"the word two positions to the left of X\")</p> </li> </ul> <p>Benefits:</p> <ul> <li>Learn patterns based on relative distance between tokens</li> <li>More important for many tasks than absolute position</li> <li>No reliance on fixed absolute positions</li> </ul>"},{"location":"slides/06_modern_architectures/slides/#relative-position-encoding-example","title":"Relative Position Encoding Example","text":"<p>Example sentence: \"The dog chased another dog\"</p> <p>When attending from position 2 (\"chased\"):</p> Position Token Absolute Relative to pos 2 0 The 0 -2 1 dog 1 -1 2 chased 2 0 (self) 3 another 3 +1 4 dog 4 +2 <p>In practice:</p> <p></p> <pre><code># T5-style relative attention bias\n# For position i attending to position j:\nrelative_position = j - i  # e.g., \"dog\"(1) \u2192 \"chased\"(2) = 1 - 2 = -1\n\n# Bias added to attention scores (learned, not fixed):\nattention_score = (q_i @ k_j) + bias[clip(relative_position, -max_dist, max_dist)]\n</code></pre> <p>Key differences from absolute: - Same relative pattern at any absolute position (e.g., \"-1\" always means \"previous token\") - Model learns one bias per relative distance, not per absolute position</p>"},{"location":"slides/06_modern_architectures/slides/#position-encoding-desirable-properties","title":"Position Encoding : Desirable Properties","text":"<ol> <li>Inject position information into the model</li> <li>Allow generalization to longer sequences than seen during training</li> <li>Facilitate learning of relative position patterns (e.g., \"the word   two positions to the left of X\")</li> <li>Be computationally efficient (not too many parameters or FLOPs)</li> <li>Be compatible with attention mechanism (e.g., allow position information to influence attention scores)</li> </ol>"},{"location":"slides/06_modern_architectures/slides/#rotary-position-embeddings-rope","title":"Rotary Position Embeddings (RoPE)","text":"<p>We want attention scores to depend only on relative position $(i - j)$, not absolute positions. </p> <p>Mathematically, find $f(x, i)$ such that:</p> <p>$$\\langle f(x, i), f(y, j) \\rangle = g(x, y, i-j)$$</p> <p>RoPE's key idea: - Instead of adding PE to the input <code>x</code>, apply rotation to the query and key vectors based on their position.  - Position information is directly encoded in the attention scores</p> <p></p>"},{"location":"slides/06_modern_architectures/slides/#encoding-position-as-a-rotation","title":"Encoding position as a rotation","text":"<p>Rotating a 2D vector by an angle $\\theta$ </p>    $$ \\begin{bmatrix} x' \\\\\\\\ y' \\end{bmatrix} = \\begin{bmatrix} \\cos \\theta &amp; -\\sin \\theta \\\\\\\\ \\sin \\theta &amp; \\cos \\theta \\end{bmatrix} \\begin{bmatrix} x \\\\\\\\ y \\end{bmatrix} $$    <p>Rotating a word vector</p> <p>Given a word vector $x = (x_1, x_2)$ at position $m$, we can rotate it by an angle $\\theta_m$ to get the position-aware vector $x'$:</p>   $$ \\begin{bmatrix} x'_1 \\\\\\\\ x'_2 \\end{bmatrix} = \\begin{bmatrix} \\cos \\theta_m &amp; -\\sin \\theta_m \\\\\\\\ \\sin \\theta_m &amp; \\cos \\theta_m \\end{bmatrix} \\begin{bmatrix} x_1 \\\\\\\\ x_2 \\end{bmatrix} $$"},{"location":"slides/06_modern_architectures/slides/#dot-product-of-rotated-vectors","title":"Dot product of rotated vectors","text":"<p>Let's rotate two vectors $q$ and $k$ by angles $\\theta_q$ and $\\theta_k$ respectively. </p> <p>$q' = R_{(\\theta_q)} q$</p> <p>$k' = R_{(\\theta_k)} k$</p> <p>The dot product of the rotated vectors is: $$\\begin{aligned} q' \\cdot k' &amp;= (R(\\theta_q) q) \\cdot (R(\\theta_k) k) \\ &amp;= q^T R(\\theta_q)^T R(\\theta_k) k \\ &amp;= q^T R(\\theta_k - \\theta_q) k \\end{aligned}$$</p> <p>Dot products depend only on relative rotation.</p> <p>Now attention scores depend on : <code>q</code>, <code>k</code>, and the relative angle $(\\theta_k - \\theta_q)$, which encodes the relative position between the two tokens.</p> <p></p>"},{"location":"slides/06_modern_architectures/slides/#rotations-in-higher-dimensions","title":"Rotations in higher dimensions","text":"<ul> <li> <p>In higher dimensions, we can apply rotations in multiple planes</p> <ul> <li>Example: In 4D space, rotate independently in $(x_1, x_2)$ plane and $(x_3, x_4)$ plane</li> </ul> </li> <li> <p>For a $d$-dimensional vector:</p> <ul> <li>Apply $\\frac{d}{2}$ independent rotations</li> <li>Each rotation has its own angle $\\theta_m$</li> <li>Encodes position information compatible with attention mechanism</li> </ul> </li> <li> <p>Example: Model with hidden dimension $d = 512$</p> <ul> <li>Apply 256 independent rotations</li> <li>Each pair of dimensions gets rotated by different frequency</li> <li>Creates a rich positional representation</li> </ul> </li> </ul>"},{"location":"slides/06_modern_architectures/slides/#rotation-in-m-dimensions","title":"Rotation in <code>m</code> dimensions","text":"<p>$\\theta_i = B^{-2i/d} \\quad \\text{where } B \\text{ is the base (typically 10000)}$</p> <p>The wavelength of dimension $i$ is:</p> <p>$$\\lambda_i = \\frac{2\\pi}{\\theta_i} = 2\\pi \\cdot B^{2i/d}$$</p> <p>This creates a geometric progression of wavelengths:</p> <ul> <li>Shortest wavelength (highest freq, $i=0$): $\\lambda_{\\min} = 2\\pi \\approx 6.28$ tokens</li> <li>Longest wavelength (lowest freq, $i=d/2-1$): $\\lambda_{\\max} = 2\\pi \\cdot B \\approx 62,832$ tokens (when $B=10000$)</li> </ul>"},{"location":"slides/06_modern_architectures/slides/#computing-attention-with-rope","title":"Computing Attention with RoPE","text":"<pre><code>class RoPEAttention(nn.Module):\n\n    -----    \n\n    def forward(self, x: torch.Tensor):    \n        # Apply RoPE to queries and keys\n        q = apply_rotary_emb(q, self.freqs_cis[:seq_len])\n        k = apply_rotary_emb(k, self.freqs_cis[:seq_len])\n\n        # Scaled dot-product attention\n        scores = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n</code></pre> <p>Key implementation details:</p> <ol> <li>Precompute frequencies: Calculate $\\theta_i = 10000^{-2i/d}$ for all dimension pairs once</li> <li>Apply once per forward pass: Rotate Q and K by position-dependent angles</li> <li>No extra parameters: RoPE is fully deterministic, no learned weights</li> </ol>"},{"location":"slides/06_modern_architectures/slides/#other-hyperparameters","title":"Other Hyperparameters","text":"<ul> <li>How many attention heads?</li> <li>How many layers?</li> <li>Hidden dimension size?</li> <li>FFN dimension size?</li> <li>Vocab size?</li> </ul>"},{"location":"slides/06_modern_architectures/slides/#feedforward-network-hyperparameters","title":"Feedforward network hyperparameters","text":"<ul> <li>Typical FFN dimension is 4x the hidden dimension (dff\u200b=4\u22c5dmodel\u200b)</li> <li>in GLU variants (SwiGLU, GeGLU), use 2/3 scaling to keep parameter count same</li> </ul>"},{"location":"slides/06_modern_architectures/slides/#number-of-attention-heads","title":"Number of attention heads","text":"<ul> <li>Common choices are 16, 32, or 64 heads for large models. </li> <li>The number of heads is often chosen to be a divisor of the hidden dimension for simplicity (e.g., 512 hidden dimension with 16 heads means each head has 32 dimensions).</li> </ul>"},{"location":"slides/06_modern_architectures/slides/#vocabulary-size","title":"Vocabulary size","text":"<p>Monolingual models: 30-50K tokens (Original Transformer: 37K, GPT-2/3: 50K, LLaMA: 32K)</p> <p>Multilingual/Production: 100-250K tokens (GPT-4: 100K, PaLM: 256K, Qwen: 152K, Command A: 255K)</p>   ## Efficent Attention Variants"},{"location":"slides/06_modern_architectures/slides/#mha-overview","title":"MHA Overview","text":""},{"location":"slides/06_modern_architectures/slides/#mha-generation-phase","title":"MHA - Generation Phase","text":""},{"location":"slides/06_modern_architectures/slides/#mha-generation-phase_1","title":"MHA - Generation Phase","text":"<p>KV Cache size needed for DeepSeek V3:</p> <p>KV head dim =  emb_dim // num_heads = 7168 // 56 = 128</p> <p>KV cache = sequence_length * num_heads * head_dim * 2 * 2 KV cache = 32768 * 128 * 128 * 2 * 2 = 2.14 GB per layer</p> <p>For all 61 layers, KV cache = ~131 GB for all layers ( ~ 4 MB per token)</p>"},{"location":"slides/06_modern_architectures/slides/#mha","title":"MHA |","text":"<p>4 MB|</p>"},{"location":"slides/06_modern_architectures/slides/#multi-query-attention-mqa","title":"Multi-Query Attention (MQA)","text":"<p>Reduce number of KV heads to 1 = MQA (Multi-Query Attention)</p> <p>KV Cache = 32768 * 128 * 1 * 2 * 2 = 16 MB per layer</p> <p>For all 61 layers, KV cache = ~1 GB for all layers ( ~ 32 KB per token)</p> <p>128X reduction.</p>"},{"location":"slides/06_modern_architectures/slides/#mha-mqa","title":"MHA | MQA |","text":"<p>4 MB | 32 KB</p>"},{"location":"slides/06_modern_architectures/slides/#grouped-query-attention-gqa","title":"Grouped-Query Attention (GQA)","text":"<ul> <li>Use 1 KV head for each group of query heads (e.g., 1 KV heads for 8 query heads)</li> </ul> <p>MHA | MQA | GQA | ----|------|-----|------- 4 MB | 32 KB | 500 KB</p> <p>8X reduction compared to MHA</p>"},{"location":"slides/06_modern_architectures/slides/#multi-head-latent-attention-mla","title":"Multi-head Latent Attention (MLA)","text":"<ul> <li>Project KV vectors into a smaller latent space (e.g., 576 dimensions) for attention computation</li> <li>Use this vector for attention instead of the full KV vectors (num_heads * head_dim * 2 dimensions)</li> </ul> <p>576 &lt;&lt;&lt; 128 * 128 * 2 = 32768</p> <p></p>"},{"location":"slides/06_modern_architectures/slides/#mha-mqa-gqa-mla","title":"MHA | MQA | GQA | MLA |","text":"<p>4 MB | 32 KB | 500 KB | 70 KB</p> <p>57X reduction compared to MHA, and 7X reduction compared to GQA.</p>"},{"location":"slides/06_modern_architectures/slides/#33-qk-norm-attention-softmax-stability","title":"3.3 QK-Norm (Attention Softmax Stability)","text":"<p>Concept: Apply LayerNorm to Query ($Q$) and Key ($K$) vectors before computing dot-product attention.</p> <p>Why use QK-Norm? - Softmax Stability: Bounds the inputs to the softmax, preventing extreme logit values - Training Robustness: Reduces training instability in very deep or large-scale architectures - Evolution of Norms: LayerNorm has evolved from Post-Norm (Transformer) to Pre-Norm (GPT-2) and now directly into the attention mechanism (QK-Norm) </p> <p></p> <p>Meta-lesson: LayerNorm is strikingly effective as a stability tool. It's been added at pre-norm position, post-norm-outside-residual position, and now inside attention for QK normalization.</p>"},{"location":"slides/06_modern_architectures/slides/#further-reading","title":"Further Reading","text":"<p>The Big LLM Architecture Comparison</p>"},{"location":"slides/06_modern_architectures/summary/","title":"Modern Transformer Architecture Variants","text":""},{"location":"slides/06_modern_architectures/summary/#overview-methodology","title":"Overview &amp; Methodology","text":"<p>The lecture takes a data-driven, empirical approach to understanding transformer architecture choices. Rather than theorizing about what should work, Hashimoto surveyed 19+ dense model releases from 2017-2025 (Original Transformer through SmolLM, Command A, Gemma 3) and compiled a spreadsheet tracking architecture decisions across all of them. The key insight: you can observe convergent evolution in neural architectures -- certain choices have won out decisively, while others remain genuinely open.</p> <p>Central theme: \"The best way to learn is hands-on experience; the second best way is to learn from others' experience.\"</p>"},{"location":"slides/06_modern_architectures/summary/#part-1-architecture-variations","title":"Part 1: Architecture Variations","text":""},{"location":"slides/06_modern_architectures/summary/#11-pre-norm-vs-post-norm-universal-consensus","title":"1.1 Pre-Norm vs Post-Norm (Universal consensus)","text":"<p>The one thing everyone agrees on. This is the single strongest consensus in modern LLM architecture.</p> <p>What changed: The original transformer (Vaswani 2017) placed LayerNorm after the residual addition (post-norm). Pre-norm moves it before the sub-layer, keeping the residual stream clean.</p> <p>Post-LN: <code>x \u2192 Attention(x) \u2192 Add \u2192 LayerNorm \u2192 FFN \u2192 Add \u2192 LayerNorm</code> Pre-LN: <code>x \u2192 LayerNorm \u2192 Attention \u2192 Add \u2192 LayerNorm \u2192 FFN \u2192 Add</code></p> <p>Why pre-norm wins: - Gradient propagation: The residual stream provides an identity connection from top to bottom. Post-norm inserts normalization into this path, disrupting gradient flow. Xiong (2020) showed post-LN has gradient attenuation that grows with depth -- at initialization, gradients explode in later layers; only after careful warmup does it stabilize. - Training stability: Salazar &amp; Nguyen (2019) showed post-norm training exhibits frequent gradient spikes (visible as high-variance gradient norms). Pre-norm dramatically reduces these spikes. - Practical benefit today: Pre-norm allows using larger learning rates and eliminates the need for careful warmup schedules. This matters enormously for large-scale training.</p>"},{"location":"slides/06_modern_architectures/summary/#teaching-intuition","title":"Teaching Intuition","text":"<p>Think of the residual stream as a highway running from the bottom of the network to the top. In post-norm, you're placing toll booths (LayerNorms) directly on the highway -- every car (gradient) has to stop and be processed. In pre-norm, you've moved the toll booths onto the exit ramps (the attention/FFN branches). The highway itself is completely clear, so gradients travel freely end-to-end.</p> <p>A concrete way to show this: draw the computational graph and trace the gradient path. In post-norm, the gradient must pass through LayerNorm's Jacobian at every layer (multiplied L times for L layers). In pre-norm, there's always a direct additive path through the residual that bypasses everything.</p>"},{"location":"slides/06_modern_architectures/summary/#student-questions-from-the-cs336-lecture","title":"Student Questions (from the CS336 lecture)","text":"<p>Q: Why exactly is LayerNorm in the residual stream bad? A: The residual gives you an identity connection from almost the top to the bottom of the network. This makes gradient propagation trivially easy -- there's no vanishing/exploding gradient along this path. Putting LayerNorm in the middle disrupts this identity. The empirical evidence confirms it: post-norm models show gradient attenuation (gradients grow across layers at init) and require careful warmup to stabilize.</p> <p>Q: If pre-norm preserves the residual, don't the activations grow unboundedly as they flow through layers? A: Yes, they can! This is actually a real concern. The residual stream accumulates contributions from every layer, and its magnitude can grow. This is part of why a final LayerNorm is needed before the output projection in pre-norm architectures. It's also part of the motivation for \"double norm\" -- adding normalization on the branch outputs controls their magnitude before they enter the residual.</p>"},{"location":"slides/06_modern_architectures/summary/#advanced-questions-for-students","title":"Advanced Questions for Students","text":"<ol> <li> <p>Derive it: Write out the backward pass for a 2-layer post-norm vs pre-norm network. Show explicitly where the LayerNorm Jacobian appears in each case and how it affects gradient magnitude.</p> </li> <li> <p>The double-norm puzzle: If pre-norm works because we keep the residual clean, then double-norm (pre + post outside residual) also keeps the residual clean. But now the branch output is normalized twice. Could this hurt expressiveness? What's the tradeoff?</p> </li> <li> <p>Thought experiment: BERT was trained with post-norm and was hugely successful. Why did post-norm work for BERT but became problematic for GPT-scale models? (Hint: think about depth, training duration, and learning rate schedules.)</p> </li> </ol> <p>New development -- \"Double norm\" (2024-2025): If the intuition is \"keep LayerNorms out of the residual stream,\" why not add a second norm after the sub-layer but outside the residual path? Recent models do exactly this: - Grok, Gemma 2: LayerNorm both before AND after attention/FFN blocks (outside residual) - OLMo 2: Only the post-norm outside the residual stream (not the pre-norm)</p> <p>This is argued to further improve stability for very large models.</p>"},{"location":"slides/06_modern_architectures/summary/#12-layernorm-vs-rmsnorm-strong-consensus-toward-rmsnorm","title":"1.2 LayerNorm vs RMSNorm (Strong consensus toward RMSNorm)","text":"<p>LayerNorm (original): Normalize by subtracting mean and dividing by std dev, then scale ($\\gamma$) and shift ($\\beta$): $$y = \\frac{x - E[x]}{\\sqrt{Var[x] + \\epsilon}} \\cdot \\gamma + \\beta$$</p> <p>RMSNorm (modern): Drop the mean subtraction and bias term: $$y = \\frac{x}{\\sqrt{||x||_2^2 + \\epsilon}} \\cdot \\gamma$$</p> <p>Notable models using LayerNorm: GPT-1/2/3, OPT, GPT-J, BLOOM Notable models using RMSNorm: LLaMA family, PaLM, Chinchilla, T5</p> <p>Why RMSNorm wins -- a nuanced argument:</p> <p>The naive argument is \"fewer operations = faster.\" But normalization is only 0.17% of FLOPs (tensor contractions are 99.8%). So saving a mean calculation seems meaningless.</p> <p>The real reason: FLOPS are not runtime. (Key insight from Ivanov et al 2023)</p> Operator class % FLOPs % Runtime Tensor contraction 99.80 61.0 Stat. normalization 0.17 25.5 Element-wise 0.03 13.5 <p>Normalization operations are 0.17% of FLOPs but 25.5% of runtime because they are dominated by data movement (memory bandwidth), not compute. RMSNorm has fewer parameters to move in and out of memory, which translates to real wallclock savings.</p> <p>Empirical validation (Narang et al 2020): RMSNorm achieves both: - Faster training (3.68 vs 3.50 steps/second) - Lower final loss (1.821 vs 1.838) - Better downstream performance (SGLUE 75.45 vs 71.66)</p>"},{"location":"slides/06_modern_architectures/summary/#teaching-intuition_1","title":"Teaching Intuition","text":"<p>Show students the FLOP breakdown table and ask: \"Should we bother optimizing normalization if it's only 0.17% of FLOPs?\" Let them argue \"no.\" Then reveal the runtime column -- normalization is 25% of wallclock time. This is the single most important systems insight for architecture design: FLOPs are a terrible proxy for runtime. Memory bandwidth, not compute, is the bottleneck for many operations. This lesson recurs throughout inference optimization, kernel fusion, and GPU architecture.</p> <p>An analogy: Imagine a factory where 99.8% of the work is done by robots (matrix multiplies) and 0.2% by humans (normalization). The robots are incredibly fast. The humans aren't slow at their task, but they spend most of their time walking to the warehouse to fetch parts (memory access). Optimizing the human's work saves far more time than you'd expect from the 0.2% number.</p>"},{"location":"slides/06_modern_architectures/summary/#student-questions","title":"Student Questions","text":"<p>Q: If mean-centering doesn't matter, does that tell us something about what LayerNorm actually does? A: Great question. It suggests that the scale normalization (dividing by magnitude) is the critical operation, not the centering. The model can learn to handle non-zero-mean activations just fine. What it really needs is for activations not to blow up or collapse in magnitude.</p> <p>Q: One exception: Cohere's Command A and R+ use LayerNorm rather than RMSNorm. Any idea why? A (from Hashimoto): \"I'm not quite sure why.\" This is a genuine open question -- it may relate to their specific training setup or the parallel layer architecture they use.</p>"},{"location":"slides/06_modern_architectures/summary/#advanced-questions-for-students_1","title":"Advanced Questions for Students","text":"<ol> <li> <p>The 25% puzzle: Normalization is 0.17% of FLOPs but 25% of runtime. Compute the arithmetic intensity (FLOPs per byte of memory accessed) for LayerNorm vs a matrix multiply. Why is it so different? (Hint: LayerNorm reads the entire vector, computes a scalar, then reads it again to normalize. The ratio of compute to data movement is terrible.)</p> </li> <li> <p>Would you expect the RMSNorm advantage to grow or shrink on future hardware? Consider trends in compute-to-memory-bandwidth ratios on GPUs (the \"arithmetic intensity gap\"). What happens as GPUs get more FLOPS but memory bandwidth scales more slowly?</p> </li> <li> <p>RMSNorm removes the bias $\\beta$ (shift parameter). Could this hurt for certain tasks? Think about cases where the optimal activation distribution is not zero-centered.</p> </li> </ol>"},{"location":"slides/06_modern_architectures/summary/#13-dropping-bias-terms-universal-trend","title":"1.3 Dropping Bias Terms (Universal trend)","text":"<p>Most modern transformers have no bias terms in linear layers or LayerNorm.</p> <p>Original: $FFN(x) = \\max(0, xW_1 + b_1)W_2 + b_2$ Modern: $FFN(x) = \\sigma(xW_1)W_2$</p> <p>Reasons: 1. Same memory/data movement argument as RMSNorm -- fewer parameters to load 2. Optimization stability -- empirically, dropping bias terms stabilizes training of very large networks (the mechanism isn't fully understood, but the effect is clear)</p>"},{"location":"slides/06_modern_architectures/summary/#teaching-intuition_2","title":"Teaching Intuition","text":"<p>Bias terms seem harmless -- they're just adding a constant. But think about what happens across many layers. Each bias adds an offset to the activation distribution. Across 80+ layers, these offsets accumulate in the residual stream. Now the model has to \"fight\" these accumulated biases just to keep activations in a reasonable range. Removing biases means the network only does linear transformations (matrix multiplies) and nonlinearities -- there's nothing pulling activations in arbitrary directions.</p> <p>Also connect this to RMSNorm: if you've already removed the $\\beta$ shift from normalization, keeping bias terms in linear layers reintroduces the same kind of shift you just removed. The whole trend is: let the model learn through matrix multiplies alone, don't add unnecessary constant offsets.</p>"},{"location":"slides/06_modern_architectures/summary/#advanced-questions-for-students_2","title":"Advanced Questions for Students","text":"<ol> <li> <p>Parameter efficiency: A bias vector in a $d_{model}$-dimensional linear layer adds $d_{model}$ parameters. For a model with $d_{model} = 4096$ and, say, 6 linear layers per block across 32 blocks, how many bias parameters is that? What fraction of total parameters? Is the memory argument actually compelling, or is something else going on?</p> </li> <li> <p>Bias and initialization: At initialization, a linear layer without bias computes $y = Wx$ where $W$ is typically initialized with small random values. Adding a bias $b$ (often initialized to zero) shouldn't change anything at init. So why would removing bias help stability? (This remains an open question -- discuss hypotheses.)</p> </li> </ol>"},{"location":"slides/06_modern_architectures/summary/#14-activations-gated-linear-units-strong-trend-toward-swiglugeglu","title":"1.4 Activations &amp; Gated Linear Units (Strong trend toward SwiGLU/GeGLU)","text":"<p>Evolution of activations:</p> Activation Formula Notable Models ReLU $FF(x) = \\max(0, xW_1)W_2$ Original transformer, T5, Gopher, OPT GeLU $FF(x) = GELU(xW_1)W_2$ where $GELU(x) = x\\Phi(x)$ GPT-1/2/3, GPT-J, BLOOM SwiGLU $FF(x) = (Swish(xW) \\otimes xV)W_2$ LLaMA 1/2/3, PaLM, Mistral, most post-2023 GeGLU $FF(x) = (GELU(xW) \\otimes xV)W_2$ T5 v1.1, mT5, Phi3, Gemma 2/3 <p>What makes GLUs different: Instead of just applying a nonlinearity, GLUs add a gating mechanism. The hidden representation is element-wise multiplied by a learned linear projection $xV$. This gate controls information flow through the MLP.</p> <p>$$\\text{Standard:} \\quad \\sigma(xW_1) \\rightarrow \\sigma(xW_1) \\otimes (xV) \\quad \\text{(gated)}$$</p> <p>The extra parameter V means GLU models have 3 weight matrices (W, V, W2) instead of 2. To keep parameter count matched, the hidden dimension $d_{ff}$ is scaled down by 2/3.</p> <p>Evidence GLUs work (Shazeer 2020, Narang et al 2020): Consistently better performance across multiple benchmarks. GLU variants (GeGLU, SwiGLU, ReGLU) all outperform non-gated counterparts.</p> <p>Important caveat: GLUs aren't necessary for good models. GPT-3 (GeLU), Nemotron 340B (Squared ReLU), and Falcon 2 11B (ReLU) are all strong models without gating. But evidence consistently favors GLUs.</p> <p>Q&amp;A insight -- does the non-monotonic region of Swish/GeLU cause problems? Hashimoto: In practice no, because optimization uses high learning rates with momentum, so activations don't converge to the zero point. Also, the FLOP cost of these fancier activations is negligible compared to the matrix multiplies; what matters is memory pressure, which is identical.</p>"},{"location":"slides/06_modern_architectures/summary/#teaching-intuition_3","title":"Teaching Intuition","text":"<p>Think of the MLP as a two-step process: (1) project into a high-dimensional space, apply nonlinearity, (2) project back. The gating mechanism adds a learned filter at step 1 -- the model gets to decide, for each hidden dimension, how much signal to let through. Without gating, the nonlinearity is a fixed function of the input. With gating, it's input-dependent -- the gate $xV$ learns which features are relevant for the current input and suppresses others.</p> <p>A good analogy: imagine a photo editor. The standard MLP applies a fixed filter (contrast, brightness). The gated MLP first analyzes the photo to decide which filter to apply and how strongly -- it's adaptive. The $xV$ projection is the \"analyzer\" and the element-wise multiplication is the \"adaptive filtering.\"</p> <p>Why the 2/3 scaling? Walk through the parameter count. Standard MLP: $W_1 \\in \\mathbb{R}^{d \\times d_{ff}}$ + $W_2 \\in \\mathbb{R}^{d_{ff} \\times d}$ = $2 \\cdot d \\cdot d_{ff}$ params. Gated MLP: $W \\in \\mathbb{R}^{d \\times d_{ff}}$ + $V \\in \\mathbb{R}^{d \\times d_{ff}}$ + $W_2 \\in \\mathbb{R}^{d_{ff} \\times d}$ = $3 \\cdot d \\cdot d_{ff}$ params. To match: set $d_{ff}^{gated} = \\frac{2}{3} d_{ff}^{standard} = \\frac{2}{3} \\cdot 4d = \\frac{8}{3}d$.</p>"},{"location":"slides/06_modern_architectures/summary/#student-questions-from-the-cs336-lecture_1","title":"Student Questions (from the CS336 lecture)","text":"<p>Q: ReLU is easily differentiable. GeLU/Swish involve the CDF of a Gaussian -- does that slow things down? A (from another student in the audience): \"What really matters is memory pressure, and it's the exact same because you're reading the same number of elements.\" The extra arithmetic is negligible; the bottleneck is loading and storing the activation tensor, which is the same size regardless of the nonlinearity.</p> <p>Q: Below a certain negative value, Swish and GeLU are not monotonically increasing -- they actually decrease. Doesn't this break gradient descent? A: Intuitively you might think activations could get \"trapped\" in this negative region. But in practice, with high learning rates and momentum, activations are being pushed around aggressively. They don't converge to the problematic zero point. The small negative bump has negligible effect on training dynamics.</p>"},{"location":"slides/06_modern_architectures/summary/#advanced-questions-for-students_3","title":"Advanced Questions for Students","text":"<ol> <li> <p>Parameter count exercise: Prove that setting $d_{ff} = \\frac{8}{3} d_{model}$ for a gated MLP gives exactly the same number of parameters as $d_{ff} = 4 \\cdot d_{model}$ for a standard MLP. Then: do the FLOPs also match? (Careful: GLU has an extra element-wise multiply.)</p> </li> <li> <p>Why gating works: Gating appears in LSTMs, GRUs, highway networks, and now GLU-style MLPs. What's the common principle? Can you formulate a general hypothesis for why learned multiplicative interactions help? (Consider: information routing, gradient flow, conditional computation.)</p> </li> <li> <p>Squared ReLU: Nemotron 340B uses $\\text{ReLU}^2(x) = (\\max(0, x))^2$. This has even sparser activations than ReLU (smoother around zero, but kills more values). Why might sparsity in activations be beneficial? Could this relate to the \"superposition hypothesis\" from mechanistic interpretability?</p> </li> <li> <p>Design exercise: You're building a 7B parameter model. You've decided on SwiGLU. Your $d_{model} = 4096$. Calculate: (a) What should $d_{ff}$ be? (b) Total MLP parameters per layer. (c) If you used standard ReLU MLP instead, what would $d_{ff}$ be for the same parameter count?</p> </li> </ol>"},{"location":"slides/06_modern_architectures/summary/#15-serial-vs-parallel-layers-minor-variation","title":"1.5 Serial vs Parallel Layers (Minor variation)","text":"<p>Serial (standard): Attention first, then MLP sequentially. $$y = x + MLP(LN(x + Attention(LN(x))))$$</p> <p>Parallel: Attention and MLP computed simultaneously, results added. $$y = x + MLP(LN(x)) + Attention(LN(x))$$</p> <p>Benefits of parallel: - ~15% faster training (PaLM paper) because MLP and attention matrix multiplies can be fused - LayerNorm can be shared between the two branches - Quality neutral at large scale (small degradation at 8B, none at 62B per PaLM)</p> <p>Models using parallel: GPT-J (pioneer), PaLM, GPT-NeoX, Cohere Command A/R+, Falcon 2 11B</p> <p>Current status: Most recent models still use serial. Parallel hasn't been widely adopted despite the compute win, possibly because the quality tradeoff isn't worth it at smaller scales.</p>"},{"location":"slides/06_modern_architectures/summary/#teaching-intuition_4","title":"Teaching Intuition","text":"<p>In a serial block, the MLP gets to see the output of attention -- it can refine what attention computed. In a parallel block, MLP and attention work independently on the same input and their outputs are simply summed. This is like the difference between two people working sequentially (one reviews and improves the other's work) vs in parallel (both work independently, results are merged).</p> <p>The PaLM team's finding is telling: at 8B scale, parallel hurts slightly. At 62B, no difference. Their hypothesis: at large enough scale, the model has so much capacity that the loss from not composing attention + MLP is negligible. The compute savings from fusing operations outweigh the small expressiveness loss.</p>"},{"location":"slides/06_modern_architectures/summary/#student-questions-from-the-cs336-lecture_2","title":"Student Questions (from the CS336 lecture)","text":"<p>Q: Is serial more efficient than parallel? A: Actually the reverse -- parallel is more compute-efficient (15% faster training). The concern is about expressiveness: serial composes two computations, while parallel merely adds them. You might expect serial to be more expressive. The tradeoff is: parallel gains systems efficiency but potentially loses expressiveness.</p>"},{"location":"slides/06_modern_architectures/summary/#advanced-questions-for-students_4","title":"Advanced Questions for Students","text":"<ol> <li> <p>Expressiveness argument: The serial formulation computes $MLP(LN(x + Attn(LN(x))))$ -- the MLP can be a function of the attention output. The parallel computes $MLP(LN(x)) + Attn(LN(x))$ -- both branches see only $x$. Give a concrete example of a computation that serial can express but parallel cannot. (Hint: think about attention selecting information that the MLP then needs to process.)</p> </li> <li> <p>Fusion opportunity: Explain specifically which matrix multiplies can be fused in the parallel formulation but not in serial. Draw the computation graph for both and identify the kernel fusion boundaries.</p> </li> <li> <p>Why hasn't parallel won? Despite the 15% training speedup, most 2024-2025 models use serial. Hypothesize: is this conservatism (copying LLaMA), or is there a real quality concern? How would you design an experiment to settle this?</p> </li> </ol>"},{"location":"slides/06_modern_architectures/summary/#16-position-embeddings-converged-to-rope","title":"1.6 Position Embeddings (Converged to RoPE)","text":"<p>This is the area that saw the most exploration in 2017-2022, but has now largely converged.</p> <p>Evolution:</p> Type How it works Models Sinusoidal Add fixed sin/cos to embedding Original Transformer Absolute (learned) Add learned position vector $u_i$ to embedding GPT-1/2/3, OPT Relative Add learned bias to attention scores T5, Gopher, Chinchilla ALiBi Linear attention bias BLOOM NoPE No position embedding at all SmolLM3, Kimi Linear RoPE Rotate query/key vectors GPT-J, PaLM, LLaMA, all 2024+ models <p>RoPE's key idea:</p> <p>We want attention scores to depend only on relative position $(i - j)$, not absolute positions. Mathematically, find $f(x, i)$ such that: $$\\langle f(x, i), f(y, j) \\rangle = g(x, y, i-j)$$</p> <p>Why existing methods fail this: - Sinusoidal: Cross-terms leak absolute position info - Absolute: Obviously not relative - T5 relative: Modifies the attention computation, but the result isn't a standard inner product</p> <p>RoPE's solution: Rotations preserve inner products. If you rotate each embedding vector by an angle proportional to its position, the inner product between any two vectors depends only on their angular difference (= relative position).</p> <p>Implementation: Pair up dimensions (d/2 pairs), apply 2D rotations with different frequencies per pair (analogous to sinusoidal frequencies). The rotation matrix is block-diagonal with 2x2 rotation blocks:</p> <p>$$R_{\\Theta,m} = \\text{block-diag}(\\text{Rot}(m\\theta_1), \\text{Rot}(m\\theta_2), \\ldots, \\text{Rot}(m\\theta_{d/2}))$$</p> <p>Critical difference from sinusoidal: RoPE is multiplicative (applied at each attention layer to Q and K), not additive (at input). This is essential for enforcing position invariance at every attention operation.</p> <p>Why RoPE won: All 19 papers Hashimoto surveyed from 2024-2025 use RoPE. Beyond the theoretical elegance, RoPE has proven practically effective and has spawned many context-length extension algorithms (NTK-aware scaling, YaRN, etc.).</p>"},{"location":"slides/06_modern_architectures/summary/#teaching-intuition_5","title":"Teaching Intuition","text":"<p>The \"we know\" example from the lecture is gold for teaching. Walk through it step by step:</p> <ol> <li>Start with two word vectors: $\\vec{we}$ and $\\vec{know}$ (arrows in 2D for visualization).</li> <li>Sentence \"we know that\" -- \"we\" is at position 0, \"know\" at position 1. Rotate \"we\" by 0 radians, \"know\" by $\\theta$ radians. The angle between them is $\\theta$.</li> <li>Sentence \"of course we know\" -- \"we\" is at position 2, \"know\" at position 3. Rotate \"we\" by $2\\theta$, \"know\" by $3\\theta$. The angle between them is still $\\theta$.</li> <li>Key insight: The inner product $\\langle R_{2\\theta}\\vec{we}, R_{3\\theta}\\vec{know} \\rangle = \\langle R_0\\vec{we}, R_\\theta\\vec{know} \\rangle$ because rotations preserve inner products and only the difference in rotation angles matters.</li> </ol> <p>This is the entire idea. Everything else is implementation details (how to handle high dimensions, how to pick $\\theta$s).</p> <p>The high-dimensional extension: We can't easily rotate in $d$-dimensional space (too many degrees of freedom). RoPE's clever trick: pair up consecutive dimensions and rotate each pair independently. This gives $d/2$ independent 2D rotations, each with its own frequency -- exactly analogous to how sinusoidal embeddings have different frequencies for different dimension pairs.</p> <p>Why it's multiplicative, not additive: Draw the code path. Absolute embeddings: add position vector at the input, once. RoPE: multiply Q and K by rotation matrix at every attention layer. This is necessary because if you only add at the bottom, the position signal gets diluted by subsequent transformations. By applying at each attention operation, you guarantee position information is fresh.</p>"},{"location":"slides/06_modern_architectures/summary/#student-questions-from-the-cs336-lecture_3","title":"Student Questions (from the CS336 lecture)","text":"<p>Q: Are the rotation angles ($\\theta$) hyperparameters or learned? A: Neither -- they follow a fixed schedule (like sinusoidal embeddings). Different dimension pairs rotate at different speeds: some fast (capturing local/nearby token relationships), some slow (capturing long-range relationships). The thetas are not learned because rotating is just a matrix multiply with fixed coefficients -- if you were learning thetas, you'd need to differentiate through trig functions, which could cause issues.</p> <p>Q: Is the rate of rotation consistent across models? A: There's some variation in the $\\theta$ schedule across models. The original RoPE uses $\\theta_i = 10000^{-2i/d}$. Later work (NTK-aware RoPE, YaRN) modifies these to enable context length extension.</p> <p>Q: Do the rotations create difficulty with training? A: No, because a rotation is just a fixed matrix multiply (since $\\theta$s and positions $m$ are fixed during forward pass). It's no different from any other linear transformation. Gradients flow through it normally. If you were learning the rotation parameters, then differentiating through trig functions could be problematic, but you're not.</p>"},{"location":"slides/06_modern_architectures/summary/#advanced-questions-for-students_5","title":"Advanced Questions for Students","text":"<ol> <li> <p>Prove the relative position property: Given $f(x, m) = R_m W x$ where $R_m$ is the RoPE rotation matrix, show that $\\langle f(x, m), f(y, n) \\rangle = g(x, y, m-n)$ for some function $g$. Where does the proof break if we use additive position embeddings instead?</p> </li> <li> <p>Context length extrapolation: RoPE was originally trained on sequences of length $L$ with $\\theta_i = 10000^{-2i/d}$. At inference, we want to use length $4L$. What goes wrong? The rotation angles $m\\theta_i$ for positions $m &gt; L$ were never seen during training. How do NTK-aware scaling and YaRN address this? (Hint: they modify the base frequency or interpolate positions.)</p> </li> <li> <p>RoPE vs ALiBi: ALiBi (Attention with Linear Biases) achieves relative position encoding by subtracting a linear penalty from attention scores: $\\text{score}_{ij} = q_i \\cdot k_j - m \\cdot |i-j|$, where $m$ is a per-head slope. Compare the expressiveness: RoPE modulates the content-based similarity via rotation, while ALiBi adds a content-independent distance penalty. In what scenarios might one be better than the other?</p> </li> <li> <p>NoPE layers (No Position Embedding): In the Command A / LLaMA 4 architecture, some layers use full attention with no position embedding at all. How can a transformer layer function without position information? What does this layer compute? (It can attend to all positions equally based on content only -- think of it as a pure \"what's similar to me?\" lookup.)</p> </li> </ol>"},{"location":"slides/06_modern_architectures/summary/#part-2-hyperparameters","title":"Part 2: Hyperparameters","text":""},{"location":"slides/06_modern_architectures/summary/#21-ffn-dimension-ratio-d_ff-4-cdot-d_model-or-83-for-glus","title":"2.1 FFN Dimension Ratio: $d_{ff} = 4 \\cdot d_{model}$ (or 8/3 for GLUs)","text":"<p>The rule: Almost universally, the FFN hidden dimension is 4x the model dimension.</p> <p>For GLU variants (which have an extra parameter matrix V), scale down by 2/3 to maintain parameter parity: $d_{ff} = \\frac{8}{3} d_{model} \\approx 2.67 \\cdot d_{model}$</p> <p>Models following the 8/3 rule: LLaMA 70B (2.68), Qwen 14B (2.67), DeepSeek 67B (2.68), Yi 34B (2.85) Slightly larger: PaLM (4.0), Mistral 7B (3.5), LLaMA-2 70B (3.5)</p> <p>Notable exception -- T5 11B: Uses $d_{ff} = 65,536$ with $d_{model} = 1024$ -- a 64x multiplier. Rationale from the paper: larger FFN means larger matrix multiplies, which TPUs handle more efficiently. However, T5 v1.1 walked this back to a 2.5 multiplier with GeGLU and got a better model.</p> <p>Empirical evidence (Kaplan et al 2020): There's a wide basin between ratios 1-10 where performance is near-optimal. The 4x default sits comfortably within this basin.</p>"},{"location":"slides/06_modern_architectures/summary/#teaching-intuition_6","title":"Teaching Intuition","text":"<p>The MLP does two things: project up to a higher-dimensional space (where nonlinearities can carve out complex decision boundaries), then project back to the model dimension. The ratio controls how much \"room\" the model has for intermediate computation. At 4x, you're saying: \"give the MLP 4 times as many hidden neurons as its input dimension to do its computation.\"</p> <p>Why does a wide basin exist (1-10x all work)? Because the model can compensate: if $d_{ff}$ is smaller, each MLP does less work, but you have many layers, so the total computation budget is still large. It's only at extreme ratios (&gt;10x) where you're spending parameters very inefficiently that things degrade.</p> <p>The T5 story is great for teaching: \"LLM training is a game of copying hyperparameters from other people\" (Hashimoto). T5 was bold enough to try 64x -- and it worked! But then T5 v1.1 quietly walked it back to 2.5x and got a better model. The lesson: you can break conventions, but the conventions exist for reason.</p>"},{"location":"slides/06_modern_architectures/summary/#advanced-questions-for-students_6","title":"Advanced Questions for Students","text":"<ol> <li> <p>Where do the MLP parameters live? For a standard model with $d_{ff} = 4 d_{model}$, what fraction of total layer parameters are in the MLP vs attention? Compute for both standard (2 matrices: $d \\times 4d$ and $4d \\times d$) and gated (3 matrices with $d_{ff} = 8/3 \\cdot d$). Which dominates?</p> </li> <li> <p>The T5 rationale: T5 chose 64x because \"modern accelerators are most efficient for large dense matrix multiplications.\" Explain this argument in terms of arithmetic intensity. When does making one dimension very large help GPU utilization? When does it become counterproductive?</p> </li> </ol>"},{"location":"slides/06_modern_architectures/summary/#22-head-dimension-ratio-d_head-times-n_heads-d_model","title":"2.2 Head Dimension Ratio: $d_{head} \\times n_{heads} = d_{model}$","text":"<p>The standard practice is to split the model dimension evenly across heads, so each head gets $d_{model}/n_{heads}$ dimensions. This means adding more heads doesn't increase compute cost.</p> Model Num heads Head dim Model dim Ratio GPT-3 96 128 12288 1 T5 128 128 1024 16 LLaMA-2 64 128 8192 1 PaLM 48 258 18432 1.48 <p>T5 is again the outlier (ratio of 16). Bhojanapalli et al (2020) argued theoretically against the 1:1 ratio (low-rank bottleneck per head), but in practice the bottleneck doesn't seem to bite.</p>"},{"location":"slides/06_modern_architectures/summary/#teaching-intuition_7","title":"Teaching Intuition","text":"<p>Multi-head attention is a clever trick: instead of one big attention computation, we run $h$ smaller ones in parallel. The key insight is that this doesn't cost extra compute. We still compute one big $XQ$ multiplication ($d \\times d$ parameters) and then reshape the result into $h$ heads, each with $d/h$ dimensions. The total parameter count and FLOPs are the same whether we use 1 head or 96 heads.</p> <p>But each head only has $d/h$ dimensions to work with. If $d/h$ is very small (say, 8 or 16), each head can only represent very low-rank attention patterns. The theoretical concern is: with 128 heads and $d = 4096$, each head only has 32 dimensions -- is that enough to compute meaningful attention patterns?</p> <p>In practice: yes, it seems to be. Models with the 1:1 ratio work great. The theoretical low-rank bottleneck doesn't seem to matter empirically.</p>"},{"location":"slides/06_modern_architectures/summary/#advanced-questions-for-students_7","title":"Advanced Questions for Students","text":"<ol> <li> <p>Rank analysis: Each attention head computes $\\text{softmax}(Q_h K_h^T / \\sqrt{d_h})$, where $Q_h, K_h \\in \\mathbb{R}^{n \\times d_h}$. What is the maximum rank of $Q_h K_h^T$? For $d_h = 128$ and $n = 4096$, is the attention matrix rank-limited? Does this matter?</p> </li> <li> <p>Breaking the 1:1 ratio: With GQA, we have fewer KV heads than query heads. This means the KV projection is $d_{model} \\rightarrow n_{kv_heads} \\times d_{head}$ where $n_{kv_heads} &lt; n_{q_heads}$. The 1:1 ratio now only applies to Q. What are the implications for the attention patterns' expressiveness?</p> </li> </ol>"},{"location":"slides/06_modern_architectures/summary/#23-aspect-ratio-d_model-n_layers-approx-100-200","title":"2.3 Aspect Ratio: $d_{model} / n_{layers} \\approx 100-200$","text":"<p>How deep vs wide should the model be?</p> Model $d_{model}/n_{layers}$ GPT-3/OPT/Mistral/Qwen 128 LLaMA/LLaMA-2/Chinchilla 102 PaLM (540B) 156 T5 (11B) 43 (outlier) GPT-2 33 (outlier) <p>Sweet spot is 100-200. Evidence from Kaplan et al (2020) shows this is optimal across scales (50M, 274M, 1.5B parameters).</p> <p>Systems consideration: Very deep models are harder to parallelize (pipeline parallelism has latency constraints), while very wide models can use tensor parallelism (requires fast networking). Networking constraints may drive depth/width choices.</p> <p>Depth vs width for downstream tasks: Tay et al (2021) found that while upstream loss depends mainly on parameter count, downstream accuracy (e.g., SuperGLUE) may favor deeper models at equal FLOPs. This isn't fully settled.</p>"},{"location":"slides/06_modern_architectures/summary/#teaching-intuition_8","title":"Teaching Intuition","text":"<p>Aspect ratio is the \"shape\" of your model -- tall and thin vs short and wide. Think of it like a building: more floors (layers) means more sequential processing, while wider floors (larger $d_{model}$) means more parallel processing per step.</p> <p>Why ~128 hidden dims per layer? No one has a clean theoretical answer. It's empirically derived: Kaplan et al (2020) show that across 3 orders of magnitude in model size, the optimal aspect ratio stays roughly constant. This is remarkable -- it means when you double your parameter budget, you should make the model both deeper AND wider in roughly equal proportion.</p> <p>The systems angle is critical for teaching: A very deep model (many layers, small $d_{model}$) is hard to parallelize across GPUs because layers are sequential -- GPU 1 must finish layer 1 before GPU 2 can start layer 2 (pipeline parallelism). A very wide model (few layers, huge $d_{model}$) can split each layer's matrix multiply across many GPUs (tensor parallelism), but this requires extremely fast inter-GPU communication (NVLink, not just PCIe). Real clusters have specific network topologies that constrain what's feasible. So the \"optimal\" aspect ratio isn't just about model quality -- it's about what your hardware can efficiently execute.</p>"},{"location":"slides/06_modern_architectures/summary/#advanced-questions-for-students_8","title":"Advanced Questions for Students","text":"<ol> <li> <p>Scaling exercise: You have a 7B parameter budget. Using the consensus ratios ($d_{ff} = 8/3 \\cdot d_{model}$, SwiGLU, $d_{head} = 128$, aspect ratio ~128), work out: $d_{model}$, $n_{layers}$, $n_{heads}$, $d_{ff}$. Verify total parameter count. Compare your answer to LLaMA-2 7B's actual architecture.</p> </li> <li> <p>Pipeline vs tensor parallelism tradeoff: You have 8 GPUs connected with NVLink (fast) within a node, and 4 nodes connected with InfiniBand (slower). You're training a model with 80 layers and $d_{model} = 8192$. How would you distribute the model? What changes if the model had 160 layers and $d_{model} = 4096$ (same parameter count)?</p> </li> <li> <p>The depth-downstream mystery: Tay et al (2021) showed deeper models are better for downstream tasks even when upstream loss is the same. Why might depth help with fine-tuning? (Hypothesize: deeper networks compose more functions, enabling more abstract representations that transfer better. Or: deeper networks have more \"slots\" for task-specific adaptation during fine-tuning.)</p> </li> </ol>"},{"location":"slides/06_modern_architectures/summary/#24-vocabulary-size","title":"2.4 Vocabulary Size","text":"<p>Monolingual models: 30-50K tokens (Original Transformer: 37K, GPT-2/3: 50K, LLaMA: 32K) Multilingual/Production: 100-250K tokens (GPT-4: 100K, PaLM: 256K, Qwen: 152K, Command A: 255K)</p> <p>Trend is upward as models serve more diverse users and languages. Larger vocabularies particularly help low-resource languages by packing them into fewer tokens (reducing inference cost for those users).</p>"},{"location":"slides/06_modern_architectures/summary/#teaching-intuition_9","title":"Teaching Intuition","text":"<p>Vocabulary size involves a fundamental tradeoff: compression vs coverage. Larger vocab = each token carries more information (fewer tokens per sentence = faster inference, cheaper API calls), but also = larger embedding matrix (more parameters, slower softmax). Smaller vocab = model sees more tokens per concept (more compositional, potentially better generalization), but longer sequences (slower, more expensive).</p> <p>For production multilingual systems, the equation tilts heavily toward larger vocabs. Cohere's argument: with a large, multilingual tokenizer, a Hindi sentence might use 50 tokens instead of 200. That's 4x cheaper inference for Hindi users -- a direct business and equity argument.</p>"},{"location":"slides/06_modern_architectures/summary/#student-questions-from-the-cs336-lecture_4","title":"Student Questions (from the CS336 lecture)","text":"<p>Q: Do multilingual vocabularies actually improve performance in one language (e.g., English)? A: For high-resource languages like English, the impact is small -- you can get by with 32K tokens. The value of larger vocabularies is primarily for lower-resource languages, where they dramatically reduce token count and inference cost.</p>"},{"location":"slides/06_modern_architectures/summary/#advanced-questions-for-students_9","title":"Advanced Questions for Students","text":"<ol> <li> <p>Embedding matrix cost: The embedding matrix is $V \\times d_{model}$ parameters. For $V = 256000$ and $d = 4096$, how large is this in GB (at bf16)? What fraction of a 7B model's total parameters? At what point does the embedding matrix become the dominant cost?</p> </li> <li> <p>The efficiency argument: Larger vocab \u2192 fewer tokens per document \u2192 fewer forward passes at inference. But also: larger vocab \u2192 larger softmax computation at every step. When does the trade off favor larger vocabs? Derive a rough expression for total inference cost as a function of vocab size, average compression ratio, and model size.</p> </li> <li> <p>Tokenizer-model co-design: Should vocabulary size decisions be made independently of architecture decisions? Consider: a model with $d_{model} = 1024$ may not have enough representational capacity to embed 256K distinct tokens meaningfully. How would you design an experiment to find the optimal vocab size as a function of model size?</p> </li> </ol>"},{"location":"slides/06_modern_architectures/summary/#25-regularization-weight-decay-yes-dropout-no","title":"2.5 Regularization: Weight Decay Yes, Dropout No","text":"<p>Dropout has gone out of fashion for pretraining. Newer models (post-2022) mostly don't use it. Rationale: with trillions of tokens and single-epoch training, there's no overfitting concern.</p> <p>Weight decay persists (typically 0.1), but not for regularization. This is counterintuitive.</p> <p>Why weight decay helps (Andriushchenko et al 2023): - Train/val gap is identical regardless of weight decay amount -- it's not controlling overfitting - Weight decay interacts with learning rate schedules: with cosine LR decay, high weight decay models start slower but accelerate dramatically near the end of training - The result is lower training loss (which equals better val loss since there's no overfitting)</p>"},{"location":"slides/06_modern_architectures/summary/#teaching-intuition_10","title":"Teaching Intuition","text":"<p>This is a great \"intuition-busting\" topic. Walk students through the reasoning:</p> <p>Step 1 (the puzzle): You have trillions of tokens, billions of parameters, and you train for one epoch. There's zero overfitting risk. Why would you regularize?</p> <p>Step 2 (the observation): Look at Andriushchenko's plots. Train loss vs val loss: identical lines regardless of weight decay. It's not preventing overfitting. But training loss itself is lower with weight decay.</p> <p>Step 3 (the mechanism): Weight decay shrinks weights. With a cosine learning rate schedule, the learning rate starts high and decays to near-zero. Early in training (high LR), weight decay is fighting against the large updates, slowing learning. Late in training (low LR), the weights have been kept small by weight decay, and the model can now make very precise, fine-grained adjustments. It's as if weight decay creates a \"compressed spring\" that releases energy at the end of training.</p> <p>The punchline: Weight decay in LLM pretraining is not regularization in the classical sense. It's an optimization trick that improves training dynamics by interacting with the learning rate schedule.</p>"},{"location":"slides/06_modern_architectures/summary/#student-questions-from-the-cs336-lecture_5","title":"Student Questions (from the CS336 lecture)","text":"<p>Q: Why did dropout go out of fashion? A: There's no evidence it helps training loss, and since there's no overfitting problem (single epoch over vast data), there's no regularization need either. It also complicates distributed training (need to synchronize dropout masks or deal with variance).</p> <p>Q: If weight decay doesn't affect val loss relative to train loss, why do we care about the training dynamics? A: Because the game is minimizing training loss -- that's the objective. Weight decay somehow gets us to lower training losses (which are also lower val losses, since the gap is constant). The surprising part is that it achieves this through optimization dynamics, not regularization.</p>"},{"location":"slides/06_modern_architectures/summary/#advanced-questions-for-students_10","title":"Advanced Questions for Students","text":"<ol> <li> <p>AdamW vs L2 regularization: In standard SGD, weight decay and L2 regularization are equivalent. In Adam, they're not (this is why AdamW exists). Explain the difference. Why does this distinction matter for LLM training? (Hint: Adam's adaptive learning rates interact differently with the penalty term.)</p> </li> <li> <p>The cosine schedule interaction: Sketch what happens to the effective learning rate (LR \u00d7 gradient magnitude) with and without weight decay under a cosine schedule. Why does weight decay create a \"spring-loading\" effect? Could you achieve the same effect with a different LR schedule and no weight decay?</p> </li> <li> <p>Dropout in fine-tuning: Even though dropout isn't used in pretraining, it's still common in fine-tuning. Why might the calculus be different? (Fine-tuning: small dataset, many epochs, real overfitting risk. The arguments against dropout vanish.)</p> </li> </ol>"},{"location":"slides/06_modern_architectures/summary/#part-3-stability-tricks-new-in-2024-2025","title":"Part 3: Stability Tricks (New in 2024-2025)","text":"<p>This is the area Hashimoto identified as having the most new development in the past year. As models get larger and train longer, stability becomes critical.</p>"},{"location":"slides/06_modern_architectures/summary/#31-the-problem-softmaxes","title":"3.1 The Problem: Softmaxes","text":"<p>Two softmaxes in a transformer are potential instability sources: 1. Output softmax (final logit \u2192 probability conversion) 2. Attention softmax (QK^T \u2192 attention weights)</p> <p>Both involve exponentials (can overflow) and division (can be zero).</p>"},{"location":"slides/06_modern_architectures/summary/#teaching-intuition_11","title":"Teaching Intuition","text":"<p>Show students the OLMo training curves: the blue curve (unstable) has gradient norm spikes everywhere -- imagine paying $10M to train a model and watching it diverge at step 400K. The orange curve (stable) is smooth. The difference? A few small architectural interventions. This motivates why stability tricks matter: they're not about making models better, they're about making sure your $10M training run doesn't crash and burn.</p> <p>Why softmaxes are dangerous: $\\text{softmax}(x)_i = e^{x_i} / \\sum_j e^{x_j}$. If any $x_i$ becomes very large (say, 1000), $e^{1000}$ overflows float32/bfloat16. If all $x_i$ are very negative, $\\sum_j e^{x_j} \\approx 0$, and you divide by zero. These are the two failure modes: overflow and underflow. Both create NaN/Inf gradients that corrupt the entire training.</p>"},{"location":"slides/06_modern_architectures/summary/#32-z-loss-output-softmax-stability","title":"3.2 Z-Loss (Output Softmax Stability)","text":"<p>Idea (Devlin 2014, popularized by PaLM 2022): Add an auxiliary loss to keep the softmax normalizer $Z(x) = \\sum_{r'} e^{U_{r'}(x)}$ close to 1:</p> <p>$$L = \\sum_i [\\log(P(x_i)) - \\alpha \\cdot \\log^2(Z(x_i))]$$</p> <p>When $\\log(Z) \\approx 0$ (i.e., $Z \\approx 1$), the softmax computation simplifies to just $U_r(x)$ -- no exponentials or log-sum-exp, which is numerically clean.</p> <p>Adopted by: PaLM (pioneer), Baichuan 2, DCLM, OLMo 2</p>"},{"location":"slides/06_modern_architectures/summary/#teaching-intuition_12","title":"Teaching Intuition","text":"<p>Walk through the math slowly. The log-softmax is $\\log P(x) = U_r(x) - \\log Z(x)$. If $Z(x)$ is well-behaved (close to 1), then $\\log Z \\approx 0$ and $\\log P \\approx U_r$ -- you're just reading off the logit directly. No exponentials, no division. Clean gradients.</p> <p>The Z-loss penalty $\\alpha \\cdot \\log^2 Z$ is a gentle nudge: \"please keep $Z$ close to 1.\" It doesn't force it -- the model is free to have $Z \\neq 1$ if that helps the main loss -- but there's a cost for letting $Z$ drift far from 1.</p> <p>Analogy: It's like putting a leash on a dog. The dog (the model) can go wherever it wants, but if it pulls too far (Z gets large), the leash (Z-loss) tugs back. The coefficient $\\alpha = 10^{-4}$ makes it a very loose leash -- only extreme deviations get penalized.</p>"},{"location":"slides/06_modern_architectures/summary/#33-qk-norm-attention-softmax-stability","title":"3.3 QK-Norm (Attention Softmax Stability)","text":"<p>Idea: Apply LayerNorm to queries and keys before computing attention scores. This bounds the inputs to the softmax, preventing extreme values.</p> <p>Origin: Vision/multimodal community (Dehgani 2023 for large ViTs, then Chameleon, IDEFICS from Hugging Face). The innovation then migrated to pure text LLMs.</p> <p>Adopted by: Gemma 2, DCLM, OLMo 2</p> <p>Meta-lesson: LayerNorm is strikingly effective as a stability tool. It's been added at pre-norm position, post-norm-outside-residual position, and now inside attention for QK normalization.</p>"},{"location":"slides/06_modern_architectures/summary/#34-logit-soft-capping-less-common","title":"3.4 Logit Soft-Capping (Less Common)","text":"<p>Idea (Gemma 2): Cap attention logits using tanh: $$\\text{logits} \\leftarrow \\text{soft_cap} \\cdot \\tanh(\\text{logits} / \\text{soft_cap})$$</p> <p>Prevents logits from exceeding $\\pm$soft_cap. Gemma 2 uses soft_cap=50 for attention, 30 for final layer.</p> <p>Mixed evidence: Nvidia ablation showed soft-capping slightly hurts perplexity (11.24 vs 11.19 baseline), while QK-norm improves it (10.84-10.85). So QK-norm appears to be the better stability intervention.</p>"},{"location":"slides/06_modern_architectures/summary/#student-questions-from-the-cs336-lecture_6","title":"Student Questions (from the CS336 lecture)","text":"<p>Q: For QK-norm, the LayerNorm is applied during training. Is it kept at inference time? A: Yes, absolutely. The LayerNorm has learned parameters ($\\gamma$) that the model depends on. Removing it at inference would be like removing a layer from the network -- the model would produce garbage because it was trained to expect normalized QK inputs.</p>"},{"location":"slides/06_modern_architectures/summary/#advanced-questions-for-students_11","title":"Advanced Questions for Students","text":"<ol> <li> <p>Z-loss vs QK-norm: Both stabilize softmaxes, but they work differently. Z-loss is a loss function modification (the forward pass is unchanged). QK-norm is an architecture modification (adds parameters and computation). What are the tradeoffs? When might you prefer one over the other?</p> </li> <li> <p>Where do the instabilities come from? In the attention softmax, the inputs are $QK^T / \\sqrt{d_k}$. If $Q$ and $K$ have entries that grow during training (activation drift), then $QK^T$ can have entries that are very large. Trace through why activations might grow during training. (Hint: residual stream accumulation, lack of normalization, training dynamics.)</p> </li> <li> <p>The LayerNorm meta-lesson: Hashimoto's joke: \"stack more layer norms.\" We've now seen LayerNorm used at: pre-norm position, double-norm position, QK-norm position. Is there a principled theory for where normalization helps? Or is it just \"throw a norm at any unstable computation and it usually helps\"? What are the costs of excessive normalization?</p> </li> <li> <p>Soft-capping as an alternative to FlashAttention: The standard numerical trick for stable softmax is to subtract $\\max(x)$ before exponentiating. FlashAttention handles this automatically. Logit soft-capping with tanh prevents large values altogether. If you're already using FlashAttention with the max-subtraction trick, does soft-capping provide additional benefit? Why might it hurt perplexity?</p> </li> </ol>"},{"location":"slides/06_modern_architectures/summary/#part-4-attention-variants","title":"Part 4: Attention Variants","text":""},{"location":"slides/06_modern_architectures/summary/#41-gqa-mqa-inference-optimization","title":"4.1 GQA / MQA (Inference Optimization)","text":"<p>Problem: During autoregressive generation, the KV cache creates terrible arithmetic intensity. At training time, attention has high arithmetic intensity $O((1/k + 1/bn)^{-1})$. At inference (incremental decoding), it drops to $O((n/d + 1/b)^{-1})$ -- the $n/d$ term is problematic because you want long sequences ($n$ large) but can't easily increase $d$.</p> <p>Multi-Query Attention (MQA): Keep multiple query heads but use a single shared key and value head. This dramatically reduces KV cache size and memory movement.</p> <p>Grouped-Query Attention (GQA): A middle ground -- share K/V across groups of query heads (e.g., 8 query heads share 2 KV heads). Provides a knob to trade off expressiveness vs inference efficiency.</p> <p>Evidence: MQA has a small PPL hit (30.2 vs 29.9 multi-head; Shazeer 2019). GQA shows low to no quality hit while providing major inference speedups (Ainslie 2023).</p>"},{"location":"slides/06_modern_architectures/summary/#teaching-intuition_13","title":"Teaching Intuition","text":"<p>The KV cache explanation is essential. Walk through it step-by-step:</p> <ol> <li> <p>At training time, we process the entire sequence at once. We compute $Q, K, V$ for all positions simultaneously. The attention matrix multiply $QK^T$ is a fat matrix multiply -- GPUs love this.</p> </li> <li> <p>At inference, we generate tokens one-at-a-time. For each new token, we compute only one new row of $Q$ (1 token). But we need to multiply it against all previous $K$'s (accumulated in the KV cache). This is a matrix-vector multiply (one row of Q against the full K matrix) -- terrible arithmetic intensity.</p> </li> <li> <p>The bottleneck isn't compute, it's memory. We have to load the entire KV cache from GPU memory for every single token generation. For a model with 32 heads, 128 dims per head, and 100K context tokens, the KV cache is $2 \\times 32 \\times 128 \\times 100000 \\times 2$ bytes $\\approx$ 1.6 GB per layer. For 80 layers: 128 GB. Loading this from HBM for every token is brutally slow.</p> </li> <li> <p>MQA's fix: If we share K and V across all heads, the cache shrinks by a factor of $n_{heads}$ (e.g., 32x smaller). GQA with 4 KV groups gives an 8x reduction. Dramatically less memory to move around.</p> </li> </ol> <p>Key insight to drive home: GQA/MQA is purely about inference efficiency. It barely affects training. The architecture decision is driven by deployment economics -- how many tokens per second can you serve, at what cost per query.</p>"},{"location":"slides/06_modern_architectures/summary/#advanced-questions-for-students_12","title":"Advanced Questions for Students","text":"<ol> <li> <p>KV cache size calculation: For LLaMA-2 70B (80 layers, 64 heads, $d_{head} = 128$, GQA with 8 KV groups), calculate the KV cache size in GB for a context of 128K tokens at bf16. How does this compare to the model weights size? At what context length does the KV cache exceed the model weights?</p> </li> <li> <p>The arithmetic intensity argument: Derive the arithmetic intensity for standard multi-head attention at inference (one token at a time). Show that it's $O((n/d + 1/b)^{-1})$. Then derive it for MQA and show the improvement. At what batch size does MQA stop being beneficial? (i.e., when does the $1/b$ term dominate regardless.)</p> </li> <li> <p>Quality tradeoff: MQA forces all query heads to attend using the same K and V. Conceptually, this means different attention heads can no longer look at different \"aspects\" of the keys and values -- they all share the same representation. Why doesn't this hurt more? (Hypothesize: the query projections already provide enough diversity, and the shared K/V still contains all the information.)</p> </li> <li> <p>Training GQA from scratch vs converting: Ainslie et al (2023) showed you can convert a trained MHA model to GQA by mean-pooling the KV heads, then fine-tuning briefly. Why does this work? What information is lost in the mean-pooling, and why can fine-tuning recover it?</p> </li> </ol>"},{"location":"slides/06_modern_architectures/summary/#42-sliding-window-full-attention-interleaving-2025-state-of-the-art","title":"4.2 Sliding Window + Full Attention Interleaving (2025 State of the Art)","text":"<p>The modern trick for long context: - Every Nth layer (e.g., every 4th) uses full self-attention with no position embedding (NoPE) - Remaining layers use sliding window attention with RoPE</p> <p>Example (Cohere Command A): Blocks 1-3 use SWA+RoPE, Block 4 uses Full+NoPE, repeat.</p> <p>Why this works: - Short-range: RoPE + sliding window handles local context efficiently - Long-range: Full attention layers with no position embedding can extrapolate to arbitrary lengths (no position encoding to break down) - Systems: Full attention only happens every N layers, controlling compute cost</p> <p>Models using this: LLaMA 4, Gemma 3, Cohere Command A</p>"},{"location":"slides/06_modern_architectures/summary/#teaching-intuition_14","title":"Teaching Intuition","text":"<p>The \"interleaving\" trick is brilliant and worth unpacking carefully:</p> <p>Think of the transformer as building up understanding layer by layer. In the SWA+Full interleaving:</p> <ul> <li> <p>SWA layers (with RoPE): \"Look at nearby tokens and understand local patterns.\" The sliding window means you only see, say, 4096 tokens around you. RoPE tells you exactly how far away each token is. These layers are cheap (sparse attention) and handle local syntax, word relationships, etc.</p> </li> <li> <p>Full attention layers (with NoPE -- no position embedding): \"Now look at the entire document and find relevant information anywhere.\" No position embedding means the model doesn't know where things are -- just what they are. This is purely content-based retrieval. And because there's no position encoding, it doesn't break when the document is longer than anything seen in training.</p> </li> </ul> <p>The key insight for context length: The reason RoPE breaks at longer contexts is that the rotation angles $m\\theta$ for large $m$ were never seen during training -- the model doesn't know how to handle them. By using NoPE for long-range attention, you sidestep this entirely. The full-attention layers don't care about position, so they work at any length.</p> <p>Analogy: It's like reading a book with two strategies. Most of the time (SWA layers), you read carefully, knowing exactly which page and paragraph you're on (RoPE). Occasionally (full attention layers), you pause and think \"where else in this entire book was this concept mentioned?\" -- doing a pure content search without caring about page numbers (NoPE).</p>"},{"location":"slides/06_modern_architectures/summary/#advanced-questions-for-students_13","title":"Advanced Questions for Students","text":"<ol> <li> <p>Design tradeoff: If every 4th layer uses full attention (quadratic in sequence length) and the other 3 use sliding window (linear), what's the overall complexity as a function of sequence length $n$ and window size $w$? For a model with 32 layers and $w = 4096$, at what context length does the full attention cost dominate?</p> </li> <li> <p>NoPE expressiveness: A full attention layer with no position embedding computes: $\\text{softmax}(QK^T/\\sqrt{d})V$. Position information is only available through what earlier layers have \"written\" into the residual stream. How does the model disambiguate two identical tokens at different positions? (Answer: earlier SWA+RoPE layers have already baked position-dependent information into the token representations.)</p> </li> <li> <p>Why not all-NoPE? If NoPE enables infinite context, why not use it everywhere? Design an experiment showing where pure NoPE would fail. (Hint: word order matters in language -- \"dog bites man\" vs \"man bites dog\" need position information to distinguish.)</p> </li> <li> <p>KV cache implications: In the interleaved architecture, SWA layers only cache the last $w$ tokens. Full attention layers cache everything. Calculate the KV cache savings compared to full attention everywhere. For a 128K context with $w = 4096$ and every 4th layer being full attention, what's the reduction?</p> </li> </ol>"},{"location":"slides/06_modern_architectures/summary/#the-llama-like-modern-consensus-architecture","title":"The \"LLaMA-like\" Modern Consensus Architecture","text":"<p>Combining all the above, the modern default is:</p> Component Choice Normalization RMSNorm, pre-norm Bias terms None Activation/FFN SwiGLU with $d_{ff} \\approx 2.67 \\cdot d_{model}$ Position embedding RoPE (applied at each attention layer) Layer structure Serial (attention \u2192 MLP) Attention GQA for inference efficiency Aspect ratio $d_{model}/n_{layers} \\approx 100-200$ Head dim $d_{head} \\times n_{heads} = d_{model}$ Regularization Weight decay 0.1, no dropout Stability Z-loss + QK-norm"},{"location":"slides/06_modern_architectures/summary/#key-references","title":"Key References","text":"<ul> <li>Pre/Post-Norm: Xiong et al 2020 \"On Layer Normalization in the Transformer Architecture\"; Salazar &amp; Nguyen 2019</li> <li>RMSNorm: Zhang &amp; Sennrich 2019; Narang et al 2020</li> <li>GLU/SwiGLU: Shazeer 2020 \"GLU Variants Improve Transformer\"; Narang et al 2020</li> <li>RoPE: Su et al 2021 \"RoFormer: Enhanced Transformer with Rotary Position Embedding\"</li> <li>Parallel Layers: Wang &amp; Komatsuzaki 2021 (GPT-J); Chowdhery et al 2022 (PaLM)</li> <li>Aspect Ratio/Scaling: Kaplan et al 2020 \"Scaling Laws for Neural Language Models\"; Tay et al 2021</li> <li>FLOPS vs Runtime: Ivanov et al 2023</li> <li>Weight Decay: Andriushchenko et al 2023</li> <li>Z-Loss: Devlin 2014; Chowdhery et al 2022 (PaLM)</li> <li>QK-Norm: Dehgani 2023; Henry et al (OLMo 2)</li> <li>Logit Soft-Capping: Gemma 2 Technical Report</li> <li>GQA/MQA: Shazeer 2019; Ainslie et al 2023</li> <li>Sliding Window: Child et al 2019; Jiang et al 2023 (Mistral)</li> </ul>"},{"location":"slides/06_modern_architectures/summary/#cross-cutting-themes-for-discussion","title":"Cross-Cutting Themes for Discussion","text":"<p>These are higher-level discussion topics that span multiple sections of the lecture. Good for class discussion or exam questions.</p>"},{"location":"slides/06_modern_architectures/summary/#theme-1-theory-vs-practice-in-architecture-design","title":"Theme 1: Theory vs Practice in Architecture Design","text":"<p>Almost no architecture choice in modern LLMs has a clean theoretical justification. Pre-norm: gradient arguments are hand-wavy. RMSNorm: the real reason is data movement, not math. GLUs: \"they just work.\" RoPE: has nice math, but the practical success is what drove adoption.</p> <p>Discussion prompt: \"Is deep learning architecture design more like engineering or science? Are we doing principled design or glorified hyperparameter search? What would a more principled approach look like?\"</p>"},{"location":"slides/06_modern_architectures/summary/#theme-2-the-role-of-systems-in-architecture","title":"Theme 2: The Role of Systems in Architecture","text":"<p>A recurring theme: architecture choices are increasingly driven by hardware constraints, not model quality.</p> <ul> <li>RMSNorm wins because of memory bandwidth, not mathematical superiority</li> <li>Parallel layers win because of kernel fusion, not expressiveness</li> <li>GQA/MQA exist because of KV cache memory, not training quality</li> <li>Aspect ratios are constrained by parallelism strategies, not optimal loss</li> <li>SWA+Full interleaving is designed for inference efficiency, not training loss</li> </ul> <p>Discussion prompt: \"As hardware changes (more compute, relatively less memory bandwidth), which architecture choices might change? What happens if we move to architectures optimized for, say, TPUs vs GPUs vs custom accelerators?\"</p>"},{"location":"slides/06_modern_architectures/summary/#theme-3-convergent-evolution-and-conservatism","title":"Theme 3: Convergent Evolution and Conservatism","text":"<p>Hashimoto's metaphor of \"convergent evolution\" is apt. The field has converged on a \"LLaMA-like\" template. But is this convergence because these choices are genuinely optimal, or because everyone is copying from the same successful models?</p> <p>Discussion prompt: \"T5 tried radical hyperparameters (64x FFN ratio, 16x head ratio) and it worked. Yet almost no one followed. Why? Is the field too conservative? How would you distinguish 'this is the optimal choice' from 'this is what everyone copies'?\"</p>"},{"location":"slides/06_modern_architectures/summary/#theme-4-what-we-dont-know","title":"Theme 4: What We Don't Know","text":"<p>Honest areas of ignorance from the lecture: - Why exactly do bias terms hurt stability? - Why does weight decay improve training loss through optimizer dynamics? - Is there a principled theory for where normalization helps? - What's the true optimal aspect ratio, and does it shift with scale? - Why haven't parallel layers been more widely adopted despite clear compute wins?</p> <p>Discussion prompt: \"Pick one of these open questions. Design a set of experiments that would give a convincing answer. What scale would you need to run at? What baselines and controls?\"</p>"},{"location":"slides/06_modern_architectures/summary/#suggested-exercises-for-students","title":"Suggested Exercises for Students","text":""},{"location":"slides/06_modern_architectures/summary/#exercise-1-architecture-autopsy-20-min","title":"Exercise 1: Architecture Autopsy (20 min)","text":"<p>Give students a recent model paper (e.g., Qwen-2.5 or SmolLM technical report). Have them fill in a table:</p> Component This model's choice \"Standard\" choice Same or different? Normalization type ? RMSNorm Norm position ? Pre-norm Activation ? SwiGLU Position embedding ? RoPE $d_{ff}/d_{model}$ ? 8/3 Bias terms ? None Attention type ? GQA <p>Then discuss: where does this model deviate from the consensus, and why?</p>"},{"location":"slides/06_modern_architectures/summary/#exercise-2-parameter-budget-calculator-30-min","title":"Exercise 2: Parameter Budget Calculator (30 min)","text":"<p>Given a fixed parameter budget (e.g., 3B, 7B, 13B), have students derive the full architecture specification using the consensus rules. Then compare their answer to real models of the same size.</p> <pre><code># Constraints:\n# d_ff = 8/3 * d_model (SwiGLU)\n# d_head = 128\n# n_heads = d_model / d_head\n# d_model / n_layers \u2248 128\n#\n# Total params \u2248 n_layers * (\n#   3 * d_model * d_ff        # SwiGLU MLP: W, V, W2\n#   + 4 * d_model * d_model   # Attention: Q, K, V, O projections\n#   + 2 * d_model              # RMSNorm (before attention + before MLP)\n# ) + vocab_size * d_model     # Embedding\n</code></pre>"},{"location":"slides/06_modern_architectures/summary/#exercise-3-stability-detective-discussion-15-min","title":"Exercise 3: Stability Detective (Discussion, 15 min)","text":"<p>Show students a training loss curve with gradient norm spikes (like the OLMo blue curve). Ask them to: 1. Identify the likely cause (softmax instability, activation growth, etc.) 2. Propose 3 interventions from the lecture (z-loss, QK-norm, logit capping) 3. Predict which would help most and why 4. Discuss what would happen if they just retrained from the last checkpoint before the spike</p>"},{"location":"slides/06_modern_architectures/summary/#exercise-4-rope-implementation-homework","title":"Exercise 4: RoPE Implementation (Homework)","text":"<p>Have students implement RoPE from scratch in PyTorch (no library calls): 1. Generate the rotation matrix for given positions and dimensions 2. Apply it to query and key tensors 3. Verify the relative position property: show that $\\langle R_m q, R_n k \\rangle = \\langle R_0 q, R_{n-m} k \\rangle$ 4. Bonus: Implement NTK-aware scaling for context length extension</p>"},{"location":"slides/07_moe/slides/","title":"Slides","text":"# LLMs : A Hands-on Approach   ### Mixture of Experts"},{"location":"slides/07_moe/slides/#overview","title":"Overview","text":"<ul> <li>Motivation</li> <li>MoE Architecture</li> <li>What do MoEs look like in LLMs?</li> <li>Dense vs Sparse MoE</li> <li>Routing Mechanisms</li> <li>Expert Configuration</li> <li>DeepSeek V3 MoE Architecture</li> </ul>"},{"location":"slides/07_moe/slides/#motivation","title":"Motivation","text":""},{"location":"slides/07_moe/slides/#decoder-only-llms","title":"Decoder-only LLMs","text":"<ul> <li>Decoder-only is the predominant architecture for LLMs</li> <li> <p>Core components</p> <ul> <li>Self-attention</li> <li>Feedforward (FFN)</li> <li>LayerNorm and residual connections</li> </ul> </li> <li> <p>Most parameters are in the FFN layers</p> </li> </ul>"},{"location":"slides/07_moe/slides/#decoder-only-llms_1","title":"Decoder-only LLMs","text":"<ul> <li>Most parameters are in the FFN layers</li> <li>The FFN layers are the main bottleneck for scaling up model capacity</li> <li>Increasing FFN size leads to quadratic growth in parameters and compute</li> </ul>"},{"location":"slides/07_moe/slides/#moe-architecture","title":"MoE Architecture","text":"<ul> <li>Replace big FFN with multiple smaller FFNs (experts)</li> <li>Only a subset of experts are active for each input token</li> <li>Routing mechanism decides which experts to activate</li> <li>Increase parameters without increasing compute</li> </ul>"},{"location":"slides/07_moe/slides/#moe-architecture_1","title":"MoE Architecture","text":"<ul> <li>All layers can be MoE</li> <li>Some layers (e.g., every 2nd layer) can be MoE in an interleaved fashion.</li> </ul> <pre><code>def moe_layer(token, experts, router, top_k): \n    # Ask the router \"which experts should handle this token?\" \n    logits = router(token) \n\n    # With N total experts, pick only top_k (top_k &lt;&lt; N) \n    top_k_logits, top_k_experts = top_k(logits, top_k) \n    # Compute experts' mixing weights\n    weights = softmax(top_k_logits)\n\n    # Mix only top_k experts together to provide the final output    \n    output = 0 \n    for i, expert_idx in enumerate(top_k_experts): \n        output += weights[i] * experts[expert_idx](token) \n    return output \n</code></pre>"},{"location":"slides/07_moe/slides/#why-are-moes-getting-popular","title":"Why are MoEs getting popular?","text":"<ul> <li> <p>Efficient Scaling: : Add model capacity (total parameters) without increasing active parameters (compute) significantly</p> </li> <li> <p>Efficient Pretraining and Inference - Faster training and inference compared to dense models of similar capacity</p> </li> </ul> <p></p>"},{"location":"slides/07_moe/slides/#why-are-moes-getting-popular_1","title":"Why are MoEs getting popular?","text":"<ul> <li>Improved Performance: - Better performance on many tasks by leveraging specialization and ensemble effects</li> <li>DeepSeek V2 236B MoE with 21B active parameters outperforms dense models with 100B+ parameters</li> </ul>"},{"location":"slides/07_moe/slides/#moe-architectures","title":"MoE Architectures","text":"<p>Three things vary across MoE architectures:</p> <ol> <li>Routing function - how tokens get assigned to experts</li> <li>Expert sizes - how many experts, how large each one is, shared vs routed</li> <li>Training objectives - how to train the router and keep experts balanced</li> </ol>"},{"location":"slides/07_moe/slides/#routing-mechanisms","title":"Routing Mechanisms","text":"<ul> <li>Token choice top K</li> <li>Expert choice top K</li> <li>Hash-based routing</li> <li>Learnable routing networks</li> </ul>"},{"location":"slides/07_moe/slides/#top-k-routing-in-detail","title":"Top-K routing in detail","text":"<p>1. Compute Gating Scores (e.g., dot product)</p>   $$s_{i,t} = \\text{Softmax}_i\\left(\\mathbf{u}_t^{lT} \\mathbf{e}_i^l\\right)$$  - $\\mathbf{u}_t^l$ : input token representation at layer $l$  - $\\mathbf{e}_i^l$ : expert embedding for expert $i$ at layer $l$   <p>2. Select Top-K Experts</p> <p></p> <p>3. Compute Mixed Output</p>  $$ \\mathbf{h}_t = \\sum_{i=1}^{N} g_{i,t} \\cdot \\text{FFN}_i(\\mathbf{u}_t) + \\mathbf{u}_t $$  where $\\text{FFN}_i$ is the feedforward network for expert $i$"},{"location":"slides/07_moe/slides/#top-k-routing-in-more-detail","title":"Top-K routing in more detail","text":""},{"location":"slides/07_moe/slides/#why-not-just-softmax-without-top-k","title":"Why not just softmax without top-K?","text":"<ul> <li>You immediately lose the systems efficiency</li> </ul> <ul> <li>Without top-K, you pay the training cost of all N experts per token</li> </ul> <ul> <li>The whole point of MoE is sparse activation during both training and inference</li> </ul>"},{"location":"slides/07_moe/slides/#router-collapse","title":"Router Collapse","text":""},{"location":"slides/07_moe/slides/#the-problem","title":"The Problem","text":"<ul> <li> <p>Gating network routes tokens to only a small subset of experts</p> </li> <li> <p>Leaves most experts underutilized or completely inactive</p> </li> <li> <p>The \"Rich-get-richer\" effect: Specialized experts attract more tokens $\\rightarrow$ get more gradients $\\rightarrow$ become better $\\rightarrow$ attract even more tokens</p> </li> <li> <p>Degenerate policy: Router learns $s_{i,t} \\approx 0$ for most experts</p> </li> </ul> <p>$$ \\text{Effective capacity} \\ll N \\times \\text{expert size} $$</p>"},{"location":"slides/07_moe/slides/#consequences","title":"Consequences","text":"<ul> <li>Wasted memory: Unused experts consume GPU memory but add no value</li> <li>Reduced model quality: Equivalent to training a much smaller dense model</li> <li>Poor generalization: Active experts become overloaded and overfit</li> </ul>"},{"location":"slides/07_moe/slides/#mitigating-router-collapse","title":"Mitigating Router Collapse","text":""},{"location":"slides/07_moe/slides/#strategies","title":"Strategies","text":"<ul> <li> <p>Auxiliary Balancing Loss: Penalize uneven distribution (e.g., $F \\cdot P$ loss)</p> </li> <li> <p>Expert Capacity: Cap the tokens per expert per batch to force overflow</p> </li> <li> <p>Random Noise: Add noise to router logits to encourage exploration</p> </li> <li> <p>Expert Choice: Experts pick tokens (rather than tokens picking experts)</p> </li> </ul>"},{"location":"slides/07_moe/slides/#expert-configuration","title":"Expert Configuration","text":"<p>Fine-Grained Experts - Make experts smaller and use more of them. - Instead of $N$ full-sized FFN copies, make each expert much smaller (1/4 to 1/14 of standard FFN size)</p> <p>The fine-grained ratio = (expert intermediate dim) / (standard FFN intermediate dim)</p> <p></p>"},{"location":"slides/07_moe/slides/#expert-configuration_1","title":"Expert Configuration","text":"<p>Shared Experts -  One or more experts that process all tokens regardless of routing - Shared experts provide a common processing pathway for all tokens, ensuring that every token benefits from some shared knowledge. - Routed experts can specialize on different subsets of tokens. This hybrid approach can improve performance and stability.</p> <p></p>"},{"location":"slides/07_moe/slides/#expert-configuration_2","title":"Expert Configuration","text":""},{"location":"slides/07_moe/slides/#expert-routing-configurations-for-major-moes","title":"Expert Routing Configurations for Major MoEs","text":"Model Routed Active Shared Fine-grained ratio GShard 2048 2 0 -- Switch Transformer 64 1 0 -- Mixtral 8 2 0 -- DBRX 16 4 0 -- Grok 8 2 0 -- DeepSeek V1 64 6 2 1/4 Qwen 1.5 60 4 4 1/8 DeepSeek V3 256 8 1 1/14 OLMoE 64 8 0 1/8 MiniMax 32 2 0 ~1/4 Llama 4 (Maverick) 128 1 1 1/2"},{"location":"slides/07_moe/slides/#training-moes","title":"Training MoEs","text":"<p>The Core Challenge</p> <ul> <li>Sparsity vs. Differentiability: We need sparsity for training-time efficiency.</li> <li>Problem: Sparse gating decisions (hard top-K selection) are not differentiable.</li> <li>Gradient descent cannot directly optimize the discrete \"choice\" of an expert.</li> </ul> <p>The Efficiency Trade-off</p> <ul> <li>Activating all experts simplifies gradients but destroys compute efficiency.</li> <li>FLOPs Cost: \"Having a model that's 256 times more expensive to train is a total no-go.\"</li> <li>Goal: Maintain sparse execution while ensuring the routing mechanism can still be trained effectively.</li> </ul>"},{"location":"slides/07_moe/slides/#heuristic-balancing-losses","title":"Heuristic Balancing Losses","text":"<p>Switch Transformer F*P Loss (Standard)</p> <p></p> <p>where: - $f_i = \\frac{1}{T} \\sum_{x \\in \\mathcal{B}} \\mathbb{1}{\\text{argmax } p(x) = i}$ is the fraction of tokens dispatched to expert $i$ - $P_i = \\frac{1}{T} \\sum_{x \\in \\mathcal{B}} p_i(x)$ is the mean router probability for expert $i$ - $\\alpha$ is the balancing coefficient - $N$ is the number of experts</p>"},{"location":"slides/07_moe/slides/#expert-capacity","title":"Expert Capacity","text":""},{"location":"slides/07_moe/slides/#hardware-efficiency-vs-dynamic-routing","title":"Hardware Efficiency vs. Dynamic Routing","text":"<p>Expert Capacity: The maximum number of tokens assigned to each expert per batch.</p> <p>$$ \\text{Expert Capacity} = \\frac{\\text{Tokens per Batch}}{\\text{Number of Experts}} \\times \\text{Capacity Factor} $$</p> <p></p>"},{"location":"slides/07_moe/slides/#handling-overflow-dropped-tokens","title":"Handling Overflow (Dropped Tokens)","text":"<ul> <li>Token Dropping: Occurs when the number of tokens routed to an expert exceeds its capacity.</li> <li>Residual Bypass: Dropped tokens skip expert computation and flow directly through the residual connection to the next layer.</li> </ul>"},{"location":"slides/07_moe/slides/#computing-output-of-moe-layer","title":"Computing output of MoE layer","text":"<ul> <li> <p>For each selected expert, compute the output for the assigned tokens</p> </li> <li> <p>Combine the outputs from the selected experts (e.g., weighted sum) and add the residual connection</p> </li> </ul> <p></p> <p>Example MoE Training code</p>"},{"location":"slides/07_moe/slides/#total-vs-activate-parameters","title":"Total vs Activate Parameters","text":"<ul> <li>Total Parameters: The entire set of model parameters, including all available experts (both active and inactive).</li> <li>Active Parameters: The parameters actually triggered to process a single input token. This determines the compute cost (FLOPs) per token.</li> <li>Key Advantage: MoEs scale model capacity (Total) while maintaining the inference latency and training cost of a much smaller dense model (Active).</li> </ul>"},{"location":"slides/07_moe/slides/#total-vs-activate-parameters_1","title":"Total vs Activate Parameters","text":"<p>Mixtral 8x7B MoE</p> <p></p> <ul> <li>Total Parameters: ~46.7B</li> <li>Active Parameters: ~12.9B </li> <li>Configuration:<ul> <li>8 experts per layer</li> <li>Top-2 gating ($K=2$ experts active per token)</li> </ul> </li> <li>Parameter Sharing:<ul> <li>Only the Feed-Forward Network (FFN) blocks are replicated.</li> <li>Shared components: Attention layers, LayerNorms, and Embeddings.</li> </ul> </li> </ul> <p>$$ \\text{Active Params} = \\text{Shared} + (K \\times \\text{Expert FFN}) $$ $$ \\text{Total Params} = \\text{Shared} + (N \\times \\text{Expert FFN}) $$</p> <p></p>"},{"location":"slides/07_moe/slides/#deepseek-v3-moe","title":"DeepSeek V3 MoE","text":"<p>DeepSeek V2 </p> <ul> <li>a 236 billion parameter MoE with 21 billion active parameters</li> </ul> <p></p> <p>DeepSeek V3 - a 671B parameter MoE with 37B active parameters</p>"},{"location":"slides/07_moe/slides/#deepseek-v3-architecture","title":"DeepSeek V3 Architecture","text":"<ul> <li>Sparse Activation: 671B total parameters, only 37B active per token</li> <li>Shared + Routed Experts: Combines fine-grained routed experts (256) with 1 shared expert, 8 activated experts processing all tokens</li> <li>Multi-Head Latent Attention (MLA): Compresses KV cache into latent space, synergizes with sparse MoE for efficient inference</li> <li>Multi-Token Prediction: Jointly optimizes predicting multiple tokens in parallel for improved coherence and throughput</li> </ul>   ## Questions?"},{"location":"slides/07_moe/summary/","title":"Mixture of Experts (MoE) in LLMs","text":"<p>Notes from Stanford CS336 Lecture 4 (Spring 2025) by Tatsunori Hashimoto. Sources: Lecture video, Lecture slides</p>"},{"location":"slides/07_moe/summary/#why-moes-matter-now","title":"Why MoEs Matter Now","text":"<p>Nearly every frontier LLM is an MoE: GPT-4 (rumored via the NVIDIA leak of \"GPT-MoE-1.8T\"), Grok, Mixtral, DeepSeek V2/V3, Llama 4, DBRX, Qwen, and OLMoE. MoEs went from a niche research topic to the dominant architecture for high-performance open models in under two years. As Hashimoto puts it: \"at almost all compute scales, training a mixture of experts model, if you do it well, is going to give you benefits over a dense model.\"</p> <p>Three reasons MoEs are winning:</p> <ol> <li> <p>Same FLOPs, more parameters, better performance. At fixed compute, adding more experts consistently lowers loss. Switch Transformer (Fedus et al. 2022) showed that going from 1 expert to 256 experts monotonically reduces test loss at the same FLOP budget. \"As you increase the number of experts, the training loss just keeps going down and down.\" If you believe what matters is having more parameters to memorize facts about the world, MoE is a great architecture.</p> </li> <li> <p>Faster to train. OLMoE showed a 1.3B-active / 6.9B-total MoE achieves ~3x less FLOPs or tokens to match a dense 1.3B model on HellaSwag, and trains ~2x faster wall-clock. Switch Transformer 128-expert model reached the same perplexity as T5-Base with a 7x training speedup.</p> </li> <li> <p>Naturally parallelizable. Each expert can live on a separate device. Expert parallelism is a new dimension beyond data/model/pipeline parallelism -- experts on different GPUs process their assigned tokens independently, connected by all-to-all communication. Because experts are sparsely activated, all you do is route each token to the appropriate device and the computation happens there.</p> </li> </ol> <p>Benchmark evidence: DeepSeek-V2 matches LLaMA 3 70B on MMLU with only ~21B activated parameters. Llama 4 Maverick beats Gemini 2.0 Flash and GPT-4o on multiple benchmarks at a fraction of the inference cost ($0.19-$0.49 per 1M tokens vs $4.39 for GPT-4o).</p>"},{"location":"slides/07_moe/summary/#why-havent-moes-been-more-popular-earlier","title":"Why Haven't MoEs Been More Popular Earlier?","text":"<ul> <li>Infrastructure complexity -- advantages mainly materialize at multi-node scale with many accelerators. When you have to split up your models anyway, it makes sense to shard experts across different devices. But until you get to that point, MoEs aren't clearly better. \"They're very complex and very messy.\"</li> <li>Training instability -- sparse models suffer from worse training instabilities than dense transformers (loss spikes, router collapse). The routing decision (which expert to send a token to) is a discrete, non-differentiable choice -- fundamentally at odds with the smooth gradients deep learning relies on.</li> <li>Fine-tuning challenges -- MoEs overfit more easily on small fine-tuning datasets due to the large total parameter count relative to activated parameters.</li> </ul>"},{"location":"slides/07_moe/summary/#a-misleading-name","title":"A Misleading Name","text":"<p>\"Mixture of Experts\" is a terribly named concept. You hear it and think there must be experts specialized for different domains -- a coding expert, an English expert, etc. It is very far from that mental model. MoE is simply a sparse architecture with multiple sub-networks activated selectively. The \"experts\" don't learn clean semantic specializations.</p>"},{"location":"slides/07_moe/summary/#core-architecture","title":"Core Architecture","text":""},{"location":"slides/07_moe/summary/#dense-vs-sparse-transformer","title":"Dense vs Sparse Transformer","text":"<p>In a dense transformer, every token passes through one large FFN block. In a sparse (MoE) transformer:</p> <ul> <li>The single FFN is replaced by $N$ smaller \"expert\" FFN sub-networks</li> <li>A router (gating network) selects only $K$ of them per token</li> <li>The outputs of the selected experts are combined via weighted sum</li> </ul> <p>Key insight: You can increase the number of experts without affecting FLOPs. Only $K$ experts are activated per token, so compute stays roughly constant while total parameters grow.</p>"},{"location":"slides/07_moe/summary/#what-gets-replaced","title":"What Gets Replaced","text":"<ul> <li>Typical: Replace the MLP/FFN layer with MoE layer (most models do this)</li> <li>Less common: MoE for attention heads (ModuleFormer, JetMoE) -- each attention head is also routed</li> </ul> <p>The attention layers remain shared/dense in most architectures. The MoE action is specifically in the MLPs.</p>"},{"location":"slides/07_moe/summary/#moe-design-dimensions","title":"MoE Design Dimensions","text":"<p>Three things vary across MoE architectures:</p> <ol> <li>Routing function -- how tokens get assigned to experts</li> <li>Expert sizes -- how many experts, how large each one is, shared vs routed</li> <li>Training objectives -- how to train the router and keep experts balanced</li> </ol>"},{"location":"slides/07_moe/summary/#routing-mechanisms","title":"Routing Mechanisms","text":""},{"location":"slides/07_moe/summary/#three-routing-paradigms","title":"Three Routing Paradigms","text":"Paradigm How it works Who uses it Token chooses expert Each token picks its top-K experts Most models (standard) Expert chooses token Each expert picks its top-K tokens Naturally balanced, but variable tokens/expert Global routing Solve an optimization/matching problem over all tokens and experts Theoretical, rarely used <p>Almost all production MoEs use token-choice top-K routing. Ablations from OLMoE show token choice (TC) and expert choice (EC) perform similarly, with TC having a slight edge on downstream tasks.</p>"},{"location":"slides/07_moe/summary/#top-k-routing-in-detail","title":"Top-K Routing in Detail","text":"<p>The standard formulation (used in DeepSeek V1-V2, Grok, Qwen, Mixtral, DBRX):</p> <p>$$\\mathbf{h}t^l = \\sum{i=1}^{N} \\left( g_{i,t} \\cdot \\text{FFN}_i(\\mathbf{u}_t^l) \\right) + \\mathbf{u}_t^l$$</p> <p>where the gating weights are:</p> <p>$$g_{i,t} = \\begin{cases} s_{i,t}, &amp; s_{i,t} \\in \\text{TopK}({s_{j,t} | 1 \\leq j \\leq N}, K) \\ 0, &amp; \\text{otherwise} \\end{cases}$$</p> <p>$$s_{i,t} = \\text{Softmax}_i\\left(\\mathbf{u}_t^{lT} \\mathbf{e}_i^l\\right)$$</p> <p>The router is just a logistic regressor -- a linear projection from hidden state to expert scores, followed by softmax. The simplicity is notable: complex routing (RL-based, learned routing networks) provides only marginal benefit. The router operates on the hidden state (after position embeddings, attention, etc.), not on raw token IDs.</p> <p>Why is the router so basic? (Q&amp;A) Two reasons: (1) Systems concerns -- more FLOPs spent on routing means less available for actual computation, and (2) there are hard limits on how well you can route, because the learning signal for routing is very indirect. With top-2, you can only compare the two experts you actually evaluated. Even making the router an MLP doesn't clearly help.</p> <p>Why not just softmax without top-K? You immediately lose the systems efficiency. Without top-K, you pay the training cost of all N experts per token -- the whole point of MoE is sparse activation during both training and inference.</p> <p>Why softmax before top-K? The softmax here is really a \"normalize to one\" operation to make the gating weights a proper weighted average. After top-K, the weights no longer sum to one -- some architectures renormalize after top-K, some don't. It doesn't matter much since subsequent layer norms can adjust scale.</p> <p>Why K &gt;= 2? The original argument was exploration: with K=1, you always exploit the best expert and never learn about alternatives. K=2 gives you a second \"arm\" that provides exploration signal, like epsilon-greedy in bandits. K=2 doubles activated FLOPs, which is why \"activated parameters\" is the metric MoE papers report.</p> <p>How are K experts combined? The outputs of the K selected experts are summed (weighted by gating scores). This is NOT an expectation over FFNs -- each FFN_i is a different function, and the gates are sparse.</p>"},{"location":"slides/07_moe/summary/#common-top-k-values","title":"Common Top-K Values","text":"Model K Switch Transformer 1 GShard, Grok, Mixtral 2 Qwen, DBRX 4 DeepSeek V1 6 DeepSeek V3 8 (of 256) OLMoE 8 (of 64)"},{"location":"slides/07_moe/summary/#hash-routing-baseline","title":"Hash Routing (Baseline)","text":"<p>A non-learned baseline where tokens are assigned to experts via a hash function. Surprisingly competitive -- \"even if you're hashing, you will still get gains, which is pretty wild.\" Why does this work? Even with hashing, the same tokens consistently go to the same expert, so specialization still occurs (just non-semantic). For Zipfian distributions, frequent words like \"the\" might dominate one expert, giving accidental semantic clustering. A truly random routing (different expert each time, not input-dependent) would likely be terrible.</p>"},{"location":"slides/07_moe/summary/#other-routing-methods","title":"Other Routing Methods","text":"<ul> <li>Reinforcement learning (Bengio 2013): Use REINFORCE to learn routing policy. \"It's probably the most principled thing you can do -- you have a non-differentiable routing decision, think of it as a policy, throw RL at it.\" But gradient variance and complexity make it impractical. Not clearly better than hashing. Basically abandoned.</li> <li>BASE routing (Clark 2022): Solve a linear assignment problem for globally optimal token-to-expert matching. Elegant but too expensive for the marginal benefit.</li> </ul>"},{"location":"slides/07_moe/summary/#expert-configuration","title":"Expert Configuration","text":""},{"location":"slides/07_moe/summary/#fine-grained-experts-deepseek-innovation","title":"Fine-Grained Experts (DeepSeek Innovation)","text":"<p>Instead of $N$ full-sized FFN copies, make each expert much smaller (1/4 to 1/14 of standard FFN size) and use many more of them. This enables finer-grained specialization.</p> <p>The logic: \"lots of experts is good\" -&gt; \"I want lots of experts but don't want to pay the parameter cost\" -&gt; cut each expert into smaller pieces. If the standard FFN has a 1:4 hidden-to-intermediate ratio, you can use 1:2 instead (half the size), doubling the expert count for the same total parameters. Since each fine-grained expert is smaller, having more active experts is FLOPs-free.</p> <p>The fine-grained ratio = (expert intermediate dim) / (standard FFN intermediate dim).</p> <p>Example: DeepSeek V1 has 64 experts at 1/4 ratio with 6 routed + 2 shared = 8 active. Each active expert is quarter-sized, so total active computation is roughly 2x a dense FFN.</p>"},{"location":"slides/07_moe/summary/#shared-experts","title":"Shared Experts","text":"<p>One or more experts that process all tokens regardless of routing. The idea: maybe some processing always needs to happen no matter which token you're seeing. Having a shared expert dedicated to this avoids wasting routing decisions on universal computation.</p> <p>Mixed evidence: DeepSeek ablations show shared experts help. OLMoE ablations show no benefit. There was a period where Chinese LM companies tried many shared experts (Qwen had 4), but the field has converged to 0 or 1. The original motivation for 2 shared experts (DeepSeek V1) was to keep all experts the same size -- two quarter-sized shared experts instead of one half-sized one.</p>"},{"location":"slides/07_moe/summary/#expert-routing-configurations-for-major-moes","title":"Expert Routing Configurations for Major MoEs","text":"Model Routed Active Shared Fine-grained ratio GShard 2048 2 0 -- Switch Transformer 64 1 0 -- Mixtral 8 2 0 -- DBRX 16 4 0 -- Grok 8 2 0 -- DeepSeek V1 64 6 2 1/4 Qwen 1.5 60 4 4 1/8 DeepSeek V3 256 8 1 1/14 OLMoE 64 8 0 1/8 MiniMax 32 2 0 ~1/4 Llama 4 (Maverick) 128 1 1 1/2"},{"location":"slides/07_moe/summary/#ablation-results","title":"Ablation Results","text":"<p>DeepSeek ablations: Fine-grained expert segmentation AND shared expert isolation both contribute to stronger performance across HellaSwag, PIQA, ARC, TriviaQA, NaturalQuestions. Going from 0 shared + 2/16 routed (GShard-style) to 1 shared + 7/63 routed (finer segmentation) progressively improves all benchmarks.</p> <p>OLMoE ablations: Gains from fine-grained experts confirmed. However, shared experts showed no benefit in their setup (32 routed vs 31 routed + 1 shared performed the same). Increasing expert count from 8 to 32 to 64 consistently improved results.</p>"},{"location":"slides/07_moe/summary/#training-moes","title":"Training MoEs","text":""},{"location":"slides/07_moe/summary/#the-core-challenge","title":"The Core Challenge","text":"<p>We need sparsity for training-time efficiency, but sparse gating decisions are not differentiable (hard top-K selection). If we turn on all experts, we pay the full FLOPs cost -- \"having a model that's 256 times more expensive to train is a total no-go.\"</p> <p>Three approaches: 1. Reinforcement learning to optimize gating policies -- the \"right\" solution theoretically, but gradient variance and complexity make it impractical 2. Stochastic perturbations -- add noise to make routing soft/differentiable (bandit-style exploration) 3. Heuristic balancing losses -- what everyone actually uses in practice</p> <p>\"Having gone through deep learning classes of many kinds, you can kind of guess internally which one people use in practice.\"</p>"},{"location":"slides/07_moe/summary/#stochastic-perturbations","title":"Stochastic Perturbations","text":"<p>Shazeer et al. (2017) -- Noisy top-K gating:</p> <p>$$G(x) = \\text{Softmax}(\\text{KeepTopK}(H(x), k))$$ $$H(x)i = (x \\cdot W_g)_i + \\text{StandardNormal}() \\cdot \\text{Softplus}((x \\cdot W{\\text{noise}})_i)$$</p> <p>where $\\text{KeepTopK}$ sets non-top-K values to $-\\infty$. The Gaussian noise encourages exploration of different expert assignments.</p> <p>Fedus et al. (2022) -- Multiplicative jitter:</p> <pre><code>if is_training:\n    router_logits += mtf.random_uniform(shape=router_logits.shape,\n                                         minval=1-eps, maxval=1+eps)\nrouter_logits = mtf.to_float32(router_logits)  # float32 for stability\nrouter_probs = mtf.softmax(router_logits, axis=-1)\n</code></pre> <p>This was later found to hurt quality slightly and removed in Zoph et al. (2022). Input jitter improves stability (3/3 stable vs 4/6 baseline) but at a quality cost (-1.777 vs -1.755).</p>"},{"location":"slides/07_moe/summary/#heuristic-balancing-losses","title":"Heuristic Balancing Losses","text":"<p>Without load balancing, routers collapse to using only 1-2 experts, wasting all other capacity. The OLMoE paper shows this clearly: without load balancing loss (LBL), \"the model just picks one or two experts and all the other experts are dead. The router never sends anything to them. So you're just wasting memory -- you've effectively gotten a smaller model.\" Even ignoring systems concerns, you want expert balancing just to use all your parameters effectively.</p>"},{"location":"slides/07_moe/summary/#switch-transformer-fp-loss-standard","title":"Switch Transformer F*P Loss (Standard)","text":"<p>$$\\mathcal{L}{\\text{balance}} = \\alpha \\cdot N \\cdot \\sum{i=1}^{N} f_i \\cdot P_i$$</p> <p>where: - $f_i = \\frac{1}{T} \\sum_{x \\in \\mathcal{B}} \\mathbb{1}{\\text{argmax } p(x) = i}$ is the fraction of tokens dispatched to expert $i$ - $P_i = \\frac{1}{T} \\sum_{x \\in \\mathcal{B}} p_i(x)$ is the mean router probability for expert $i$ - $\\alpha$ is the balancing coefficient - $N$ is the number of experts</p> <p>Why it works: The derivative w.r.t. $p_i(x)$ is $\\frac{\\alpha N}{T^2} \\sum \\mathbb{1}_{\\text{argmax } p(x)=i}$, so more frequent use = stronger downweighting. This creates a natural pressure toward uniform distribution.</p> <p>Tradeoff: Large $\\alpha$ improves balance but introduces interference gradients that harm model quality.</p>"},{"location":"slides/07_moe/summary/#deepseek-v1-v2-multi-level-balancing","title":"DeepSeek V1-V2: Multi-Level Balancing","text":"<p>Two levels of balancing:</p> <p>Per-expert balancing (same as Switch Transformer):</p> <p>$$\\mathcal{L}{\\text{ExpBal}} = \\alpha_1 \\sum{i=1}^{N'} f_i P_i$$</p> <p>Per-device balancing (reduces communication costs):</p> <p>$$\\mathcal{L}{\\text{DevBal}} = \\alpha_2 \\sum{i=1}^{D} f_i' P_i'$$</p> <p>where $f_i'$ and $P_i'$ aggregate over experts on device $i$.</p>"},{"location":"slides/07_moe/summary/#deepseek-v3-auxiliary-loss-free-balancing","title":"DeepSeek V3: Auxiliary-Loss-Free Balancing","text":"<p>A novel approach that avoids the quality-balance tradeoff:</p> <ol> <li>Add a learned bias $b_i$ to each expert's routing score</li> <li>Use online gradient descent per batch (not backprop) to update $b_i$</li> <li>If expert $i$ gets too many tokens, decrease $b_i$; if too few, increase it</li> </ol> <p>$$g'{i,t} = \\begin{cases} s{i,t}, &amp; s_{i,t} + b_i \\in \\text{TopK}({s_{j,t} + b_j | 1 \\leq j \\leq N_r}, K_r) \\ 0, &amp; \\text{otherwise} \\end{cases}$$</p> <p>A complementary sequence-wise auxiliary loss is still added for stability (so it's not fully auxiliary-loss-free). \"If you read the DeepSeek V3 paper, they make a big deal about how this makes training so stable, so great. And then you keep reading and they're like, actually we decided we needed the heuristic loss back.\"</p> <p>$$\\mathcal{L}{\\text{Bal}} = \\alpha \\sum{i=1}^{N_r} f_i P_i$$</p> <p>where $f_i$ and $P_i$ are computed per-sequence rather than per-batch.</p> <p>Why per-sequence? At inference time, you can't control which sequences you receive. Out-of-distribution inputs might overwhelm certain experts. Sequence-level balancing provides stronger guarantees than batch-level when individual sequences are adversarial or unusual.</p>"},{"location":"slides/07_moe/summary/#stability-issues","title":"Stability Issues","text":""},{"location":"slides/07_moe/summary/#router-softmax-overflow","title":"Router Softmax Overflow","text":"<p>Exponential functions amplify small perturbations. In bfloat16, a roundoff error of 0.5 on logit values around 128 can alter the softmax output by 36%.</p> <p>Solution: Always compute routing in float32, even when the rest of the model uses bf16/fp16.</p>"},{"location":"slides/07_moe/summary/#z-loss-regularization","title":"Z-Loss Regularization","text":"<p>A penalty on the log-sum-exp normalizer of router logits that prevents logit values from growing too large:</p> <p>$$L_z(x) = \\frac{1}{B} \\sum_{i=1}^{B} \\left( \\log \\sum_{j=1}^{N} e^{x_j^{(i)}} \\right)^2$$</p> <p>OLMoE ablations show that without z-loss, training suffers severe instability -- massive spikes in validation loss and HellaSwag performance that z-loss completely eliminates. Z-loss weight of 0.001 works well.</p>"},{"location":"slides/07_moe/summary/#token-dropping-and-stochasticity","title":"Token Dropping and Stochasticity","text":"<p>When an expert receives more tokens than its capacity factor allows, excess tokens are dropped. This creates a source of randomness even at temperature 0 during inference -- other queries in the same batch can affect which of your tokens get dropped.</p> <p>This was speculated to be the source of GPT-4's observed stochasticity (when temperature=0 still gave different outputs). Token dropping happens at the batch level, meaning other people's queries can cause your tokens to be dropped -- a cross-batch effect you almost never think about in standard inference. If your batch happens to have many tokens that love expert 3, and the device for expert 3 doesn't have enough memory, some tokens get dropped and receive zero MLP computation (just the residual connection passes through).</p>"},{"location":"slides/07_moe/summary/#systems-training-moes-at-scale","title":"Systems: Training MoEs at Scale","text":""},{"location":"slides/07_moe/summary/#expert-parallelism","title":"Expert Parallelism","text":"<p>MoEs enable a new dimension of parallelism beyond data/model/pipeline:</p> <ol> <li>Compute routing decisions locally on each device</li> <li>All-to-all dispatch: send tokens to the device hosting their assigned expert</li> <li>Parallel FFN computation: each device runs its local experts on received tokens</li> <li>All-to-all gather: collect outputs and route back to originating device</li> </ol> <p>This combines with existing parallelism strategies: - Data Parallelism: replicate model, split data - Model Parallelism: split model weights across devices - Expert + Data Parallelism: different experts on different devices, data split across replicas - Expert + Model + Data Parallelism: all three combined for largest-scale training</p>"},{"location":"slides/07_moe/summary/#sparse-matrix-multiplication","title":"Sparse Matrix Multiplication","text":"<p>MoE routing creates variable-size batches per expert. Three approaches:</p> <ol> <li>Batched matrix multiplication -- pad all expert batches to same size, run in parallel. Wastes FLOPs on padding.</li> <li>Block diagonal matrix multiplication -- frame expert computation as block-diagonal matmul. More efficient but still assumes equal-sized blocks.</li> <li>Block sparse matrix multiplication -- express as sparse matmul that handles variable expert loads without padding. Used by MegaBlocks library (used in many open MoEs).</li> </ol>"},{"location":"slides/07_moe/summary/#deepseek-v2-top-m-device-routing","title":"DeepSeek V2 Top-M Device Routing","text":"<p>To reduce all-to-all communication, DeepSeek V2 adds a constraint: first select the top-M devices (GPU nodes with highest affinity scores), then select top-K experts only within those devices. This keeps most communication local.</p>"},{"location":"slides/07_moe/summary/#issues-with-fine-tuning-moes","title":"Issues with Fine-Tuning MoEs","text":"<p>Sparse MoEs overfit more easily on smaller fine-tuning datasets. The total parameter count is much larger than what's activated per token, creating a capacity-data mismatch.</p> <p>Solutions: - Freeze MoE experts, fine-tune non-MoE layers (Zoph et al.): Only update attention and non-MoE FFN layers. SuperGLUE scores stay competitive (86 vs 86.2) while avoiding overfitting. - Use more fine-tuning data (DeepSeek approach): DeepSeek uses 1.4M SFT examples covering math, code, writing, QA, reasoning, summarization to avoid overfitting their MoE.</p>"},{"location":"slides/07_moe/summary/#upcycling-dense-to-moe-conversion","title":"Upcycling: Dense-to-MoE Conversion","text":"<p>A cost-effective alternative to training MoE from scratch:</p> <ol> <li>Start from a pretrained dense model</li> <li>Copy the FFN weights to initialize $N$ experts</li> <li>Apply small perturbations to each copy (break symmetry)</li> <li>Initialize the router from scratch</li> <li>Continue pretraining</li> </ol>"},{"location":"slides/07_moe/summary/#upcycling-results","title":"Upcycling Results","text":"<p>MiniCPM-MoE (13.6B total): Upcycled from MiniCPM-2.4B with top-K=2, 8 experts, ~4B active params. Trained with ~520B additional tokens. Outperforms DeepSeekMoE 16B and Mistral-7B on most benchmarks (MMLU 58.80, GSM8K 61.56, HumanEval 51.05).</p> <p>Qwen MoE (14.3B total, 2.7B active): Initialized from Qwen 1.8B with top-K=4, 60 experts, 4 shared. One of the first confirmed upcycling successes. Achieves MMLU 62.5, competitive with Mistral-7B (64.1) while using only 2.7B active parameters.</p> <p>The upcycling approach is particularly attractive because it reuses expensive pretraining investment and converges faster than training from scratch.</p>"},{"location":"slides/07_moe/summary/#deepseek-architecture-evolution","title":"DeepSeek Architecture Evolution","text":"<p>The lecture traces the full evolution, noting that \"DeepSeek V3 is not very different architecturally from the earliest DeepSeek models -- they had nailed the architecture when training much smaller 2B parameter models.\"</p> DeepSeek V1 DeepSeek V2 DeepSeek V3 Total params 16B 236B 671B Active params 2.8B 21B 37B Experts 64 fine-grained -- 256 Routed/token 6 -- 8 Shared experts 2 -- 1 Fine-grained ratio 1/4 -- 1/14 Gating Softmax Softmax Sigmoid Balancing F*P aux loss F*P + device balance Bias-based (aux-loss-free) Attention Standard MHA Multi-Head Latent Attention MLA Special -- Top-M device routing Multi-token prediction <p>DeepSeek V3 MoE innovations: - Sigmoid gating instead of softmax (softer, doesn't force competition between experts). The gate is still normalized to sum to 1, just via a different mechanism. - Auxiliary-loss-free balancing with learned per-expert bias offsets (but sequence-wise aux loss added back) - Retains top-M device routing from V2, drops the communication balancing loss</p> <p>Key insight: \"DeepSeek V3 is not very different architecturally from the earliest DeepSeek models -- they had nailed the architecture when training much smaller 2B parameter models. They really just got the engineering right.\"</p>"},{"location":"slides/07_moe/summary/#non-moe-components-of-deepseek-v3","title":"Non-MoE Components of DeepSeek V3","text":"<p>Multi-Head Latent Attention (MLA) -- introduced in V2, an alternative to GQA for KV cache compression:</p> <p>Instead of reducing the number of KV heads (GQA approach), MLA projects keys and values into a lower-dimensional latent space C: - Input $h_t$ is projected to a small $C$ (compressed KV representation) - Only $C$ is cached (much smaller than full K, V) - K and V are reconstructed by up-projecting from $C$ when needed - The clever trick: the up-projection matrix $W_{UK}$ can be merged with the query projection matrix via matrix associativity, so no extra FLOPs are needed</p> <p>Complication with RoPE: RoPE rotation matrices sit between the query projection and the latent up-projection, breaking the matrix merge trick. DeepSeek's solution: apply RoPE only on non-compressed dimensions.</p> <p>Multi-Token Prediction (MTP) -- a lightweight auxiliary task: - A small one-layer transformer takes the hidden state and predicts one additional token ahead - Despite the paper's elaborate diagram showing multi-token capability, \"they only do MTP with one token ahead\" - Helps training signal but is a minor component</p> <p>DeepSeek V3 results: Outperforms GPT-4o, Claude 3.5 Sonnet, and Llama 3.1 405B on MMLU-Pro (75.9), GPQA-Diamond (59.1), MATH 500 (90.2), AIME 2024 (39.2), Codeforces (51.6 percentile), and SWE-bench Verified (42.0).</p>"},{"location":"slides/07_moe/summary/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>MoE = replace FFN with N smaller expert FFNs + router. Same FLOPs, more parameters, better performance. Don't think \"specialized experts\" -- think \"sparse computation.\"</li> <li>Routing is embarrassingly simple. A linear projection + softmax + top-K. Even hashing works. Complex routing (RL, optimal transport) provides marginal benefit at prohibitive cost.</li> <li>Load balancing is the real game. Without it, routers collapse to 1-2 experts. The F*P auxiliary loss is standard; DeepSeek V3's bias-based approach is the latest (but they still add an aux loss back).</li> <li>Fine-grained experts are a no-brainer. Smaller experts, more of them, same FLOPs. Shared experts are more debatable (DeepSeek says yes, OLMoE says no difference).</li> <li>Stability requires care: float32 router computation + z-loss regularization. Without z-loss, expect severe training spikes.</li> <li>Upcycling works. Copy a dense model's FFN, perturb, add a router, continue training. Cheap way to get MoE benefits.</li> <li>Expert parallelism is a natural fit for distributed training. Top-M device routing (DeepSeek V2+) controls communication costs at scale.</li> <li>Architectures don't change much. DeepSeek V1 to V3 is mostly the same MoE design, just scaled up with engineering improvements. \"They nailed the architecture at the 2B scale.\"</li> </ol>"},{"location":"slides/07_moe/summary/#references","title":"References","text":"<ul> <li>Shazeer et al. (2017) -- Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</li> <li>Lepikhin et al. (2020) -- GShard: Scaling Giant Models with Conditional Computation</li> <li>Fedus et al. (2022) -- Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</li> <li>Zoph et al. (2022) -- ST-MoE: Designing Stable and Transferable Sparse Expert Models</li> <li>Clark et al. (2022) -- Unified Scaling Laws for Routed Language Models</li> <li>Dai et al. (2024) -- DeepSeekMoE: Towards Ultimate Expert Specialization</li> <li>DeepSeek-AI (2024) -- DeepSeek-V2: A Strong, Economical, and Efficient MoE Language Model</li> <li>DeepSeek-AI (2024) -- DeepSeek-V3 Technical Report</li> <li>Muennighoff et al. (2024) -- OLMoE: Open Mixture-of-Experts Language Models</li> <li>Gale et al. (2023) -- MegaBlocks: Efficient Sparse Training with Mixture-of-Experts</li> </ul>"},{"location":"slides/reveal/css/theme/","title":"Index","text":""},{"location":"slides/reveal/css/theme/#dependencies","title":"Dependencies","text":"<p>Themes are written using Sass to keep things modular and reduce the need for repeated selectors across files. Make sure that you have the reveal.js development environment installed before proceeding: https://revealjs.com/installation/#full-setup</p>"},{"location":"slides/reveal/css/theme/#creating-a-theme","title":"Creating a Theme","text":"<p>To create your own theme, start by duplicating a <code>.scss</code> file in /css/theme/source. It will be automatically compiled from Sass to CSS (see the gulpfile) when you run <code>npm run build -- css-themes</code>.</p> <p>Each theme file does four things in the following order:</p> <ol> <li> <p>Include /css/theme/template/mixins.scss Shared utility functions.</p> </li> <li> <p>Include /css/theme/template/settings.scss Declares a set of custom variables that the template file (step 4) expects. Can be overridden in step 3.</p> </li> <li> <p>Override This is where you override the default theme. Either by specifying variables (see settings.scss for reference) or by adding any selectors and styles you please.</p> </li> <li> <p>Include /css/theme/template/theme.scss The template theme file which will generate final CSS output based on the currently defined variables.</p> </li> </ol>"},{"location":"slides/reveal/examples/markdown/","title":"Markdown Demo","text":""},{"location":"slides/reveal/examples/markdown/#external-11","title":"External 1.1","text":"<p>Content 1.1</p> <p>Note: This will only appear in the speaker notes window.</p>"},{"location":"slides/reveal/examples/markdown/#external-12","title":"External 1.2","text":"<p>Content 1.2</p>"},{"location":"slides/reveal/examples/markdown/#external-2","title":"External 2","text":"<p>Content 2.1</p>"},{"location":"slides/reveal/examples/markdown/#external-31","title":"External 3.1","text":"<p>Content 3.1</p>"},{"location":"slides/reveal/examples/markdown/#external-32","title":"External 3.2","text":"<p>Content 3.2</p>"},{"location":"slides/reveal/examples/markdown/#external-33-image","title":"External 3.3 (Image)","text":""},{"location":"slides/reveal/examples/markdown/#external-34-math","title":"External 3.4 (Math)","text":"<p><code>\\[ J(\\theta_0,\\theta_1) = \\sum_{i=0} \\]</code></p>"},{"location":"slides/reveal/test/simple/","title":"Simple","text":""},{"location":"slides/reveal/test/simple/#slide-11","title":"Slide 1.1","text":"<pre><code>var a = 1;\n</code></pre>"},{"location":"slides/reveal/test/simple/#slide-12","title":"Slide 1.2","text":""},{"location":"slides/reveal/test/simple/#slide-2","title":"Slide 2","text":""}]}