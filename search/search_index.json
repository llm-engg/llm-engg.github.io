{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Engineering LLMs : A Hands on Approach","text":"<p>Jan - May, 2026 @ Center for Continuing Education, Indian Institute of Science</p>"},{"location":"#logistics","title":"Logistics","text":"<ul> <li>Duration: 18 weeks (Jan - May 2026)</li> <li>Format: Online, Tue and Thu, 7:30 - 9:00 PM IST</li> <li>Contact: TBA</li> </ul>"},{"location":"#course-description","title":"Course Description","text":"<p>LLMs have become mainstay of NLP and are transforming every domain, from software development, research, and business intelligence to education. However, deploying them efficiently remains a specialized engineering challenge.</p> <p>This course provides an engineering-focused exploration of Large Language Models (LLMs). Participants will go from understanding transformer architectures and GPU internals to mastering fine-tuning, inference optimization, and large-scale deployment across GPUs, clusters, and edge devices. Through a theory-to-practice approach, including case studies, hands-on labs, and projects, learners will cover key topics such as model architecture, fine-tuning techniques, inference optimization, serving strategies, and applications in retrieval-augmented generation (RAG) and agentic systems.</p>"},{"location":"#learning-outcomes","title":"Learning Outcomes","text":"<p>By the end of this course, participants will be able to:</p> <ul> <li>Understand LLM Architecture: Master transformer architectures, attention mechanisms, and modern LLM variants (GPT-OSS, Qwen, Gemma, etc.).</li> <li>Optimize Inference: Implement efficient inference strategies including quantization, KV caching, and serving via inference engines like vLLM.</li> <li>Fine-tune Models: Apply various fine-tuning techniques including LoRA, QLoRA, instruction tuning, and preference alignment.</li> <li>Build RAG Systems: Design and implement Retrieval-Augmented Generation pipelines.</li> <li>Develop AI Agents: Create tool-using agents with the ReAct framework.</li> <li>Deploy at Scale: Set up production-ready LLM serving infrastructure with cost optimization.</li> <li>Multimodal Models: Work with vision-language models and speech.</li> <li>Evaluation: Understand evaluation strategies for LLMs and RAG systems.</li> </ul>"},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Proficiency in Python and familiarity with any deep learning framework (PyTorch preferred).  </li> <li>Basic understanding of neural networks.  </li> <li>Working knowledge of Linux, Docker, and Git.  </li> <li>Optional but recommended: experience with GPU computing.</li> </ul>"},{"location":"#course-navigation","title":"Course Navigation","text":"<ul> <li> <p>Weekly Schedule</p> </li> <li> <p>Assignments and Labs</p> </li> </ul>"},{"location":"assignments/","title":"Assignments and Labs","text":"<p>TBD</p>"},{"location":"course-details/","title":"Course Details","text":""},{"location":"course-details/#course-overview","title":"Course Overview","text":"<p>This is a comprehensive hands-on course on Large Language Models (LLMs) designed specifically for industry professionals. The course covers the complete spectrum of LLM engineering, from foundational concepts to advanced deployment and optimization strategies.</p>"},{"location":"course-details/#course-objectives","title":"Course Objectives","text":"<p>By the end of this course, participants will be able to:</p> <ul> <li>Understand LLM Architecture: Master transformer architectures, attention mechanisms, and modern LLM variants (GPT, BERT, Mixtral, etc.)</li> <li>Optimize Inference: Implement efficient inference strategies including quantization, KV caching, and multi-GPU serving</li> <li>Fine-tune Models: Apply various fine-tuning techniques including LoRA, QLoRA, instruction tuning, and preference alignment</li> <li>Build RAG Systems: Design and implement Retrieval-Augmented Generation pipelines with vector databases and evaluation</li> <li>Develop AI Agents: Create tool-using agents with ReAct frameworks and multi-agent orchestration</li> <li>Deploy at Scale: Set up production-ready LLM serving infrastructure with cost optimization</li> <li>Handle Multimodal AI: Work with vision-language models and speech integration</li> <li>Ensure Security: Implement security measures against prompt injection and other LLM-specific threats</li> </ul>"},{"location":"course-details/#course-structure","title":"Course Structure","text":"<p>The course is organized into 18 weeks covering 8 major themes:</p>"},{"location":"course-details/#1-llm-foundations-weeks-1-3","title":"1. LLM Foundations (Weeks 1-3)","text":"<ul> <li>Transformer architecture deep dive</li> <li>Tokenization and pretraining objectives</li> <li>Modern architectures (GPT, Qwen, Gemma)</li> <li>Scaling laws and emergent properties</li> </ul>"},{"location":"course-details/#2-gpu-infrastructure-week-4","title":"2. GPU &amp; Infrastructure (Week 4)","text":"<ul> <li>GPU architecture and CUDA programming</li> <li>Multi-GPU and multi-node parallelism</li> <li>Hardware selection and cluster building</li> </ul>"},{"location":"course-details/#3-inference-optimization-weeks-5-7","title":"3. Inference Optimization (Weeks 5-7)","text":"<ul> <li>Inference bottlenecks and profiling</li> <li>Quantization techniques (INT8, INT4, GPTQ, AWQ)</li> <li>Inference engines (vLLM, TensorRT-LLM)</li> <li>Multi-GPU serving strategies</li> </ul>"},{"location":"course-details/#4-fine-tuning-weeks-8-10","title":"4. Fine-tuning (Weeks 8-10)","text":"<ul> <li>Parameter-efficient fine-tuning (LoRA, QLoRA)</li> <li>Instruction tuning and data curation</li> <li>Preference alignment (RLHF, DPO)</li> <li>Reasoning and chain-of-thought training</li> </ul>"},{"location":"course-details/#5-rag-systems-week-11","title":"5. RAG Systems (Week 11)","text":"<ul> <li>Vector databases and hybrid search</li> <li>RAG evaluation and optimization</li> <li>Graph RAG and advanced retrieval</li> </ul>"},{"location":"course-details/#6-ai-agents-weeks-12-13","title":"6. AI Agents (Weeks 12-13)","text":"<ul> <li>ReAct framework and tool calling</li> <li>Agent fine-tuning and evaluation</li> <li>Multi-agent orchestration</li> </ul>"},{"location":"course-details/#7-advanced-topics-weeks-14-17","title":"7. Advanced Topics (Weeks 14-17)","text":"<ul> <li>Model evaluation and monitoring</li> <li>Multimodal models (vision, audio)</li> <li>Edge deployment and tiny models</li> <li>Security and privacy engineering</li> <li>Emerging architectures (Mamba, hybrid models)</li> </ul>"},{"location":"course-details/#8-student-presentations-week-18","title":"8. Student Presentations (Week 18)","text":"<ul> <li>Project showcases and peer learning</li> </ul>"},{"location":"course-details/#course-timings","title":"Course Timings","text":"<ul> <li>Duration: 18 weeks</li> <li>Format: Weekly sessions with hands-on labs</li> <li>Target Audience: Industry professionals and advanced practitioners</li> <li>Prerequisites: Python programming, basic ML knowledge</li> <li>Level: Advanced/Professional</li> </ul>"},{"location":"course-details/#hands-on-approach","title":"Hands-on Approach","text":"<p>Each week includes:</p> <ul> <li>Theory Sessions: Core concepts and architecture deep dives</li> <li>Practical Labs: Implementation exercises and code walkthroughs</li> <li>Case Studies: Real-world examples and industry applications</li> <li>Projects: Progressive building of LLM applications</li> </ul>"},{"location":"course-details/#key-tools-technologies","title":"Key Tools &amp; Technologies","text":"<ul> <li>Frameworks: PyTorch, Hugging Face Transformers, vLLM</li> <li>Infrastructure: CUDA, NCCL, DeepSpeed, FSDP</li> <li>Deployment: Docker, Kubernetes, cloud platforms</li> <li>Monitoring: Prometheus, Grafana, PyTorch Profiler</li> <li>Development: Python, Jupyter, Git, MkDocs</li> </ul>"},{"location":"course-details/#assessment","title":"Assessment","text":"<ul> <li>Weekly hands-on assignments (60%)</li> <li>Mid-term project (20%)</li> <li>Final project presentation (20%)</li> </ul>"},{"location":"course-details/#prerequisites","title":"Prerequisites","text":"<ul> <li>Programming: Proficient in Python</li> <li>Machine Learning: Understanding of neural networks, backpropagation</li> <li>Mathematics: Linear algebra, calculus, statistics</li> <li>Hardware: Basic understanding of GPU computing (preferred)</li> <li>Tools: Familiarity with Git, command line, Jupyter notebooks</li> </ul>"},{"location":"schedule/","title":"Course Schedule","text":""},{"location":"schedule/#weekwise-schedule","title":"Weekwise Schedule","text":"<p>Tentative and subject to change</p> Theme Week Topics LLM Foundations I 1.1 Orientation, Transformer architecture 1.2 Transformer Architecture - GPT 1 and 2 2.1 Tokenization, Pretraining objectives 2.2 Mixture of Experts LLM Foundation II 3.1 Case studies: State-of-the-art open-source LLM architectures 3.2 Scaling Laws, Emergent properties GPU Basics 4.1 GPU architecture deep dive 4.2 Parallelism: Multi GPU, Multi Node 5.1 On-Prem Hardware Stack Deep Dive Inference 5.2 Inference Strategies 6.1 Inference Math and Bottlenecks 6.2 Efficient Attention &amp; KV Caching Efficient Inference &amp; Quantization 7.1 Quantization Fundamentals 7.2 Inference Engines and Multi GPU Fine-Tuning Fundamentals 8.1 Full Fine-Tuning vs. PEFT \u2014 When to Use Each 8.2 Instruction Tuning 9.1 Alignment (RLHF, DPO etc) 9.2 More RL Reasoning 10.1 Reasoning &amp; Chain-of-Thought 10.2 CoT, Tree-of-Thought, Self-Consistency \u2014 Prompt Engineering as Code RAG 11.1 RAG Fundamentals - Context-engineering, embeddings, search and rerankers 11.2 Evaluating RAG Agents 12.1 ReAct Framework: Thought \u2192 Action \u2192 Observation Tool Use &amp; Function Calling 12.2 MCP introduction 12.3 Agentic RAG, Multi Agent Orchestration, Multimodal Agents Agent Finetuning 13.1 Fine Tuning for Tool calling 13.2 Agent Evaluation &amp; Safety Evaluation 14.1 Evaluation 14.2 Observability &amp; Monitoring Multimodal Models 15.1 Multi Modal Architecture: Image, Audio and Video models, Running Locally 15.2 Fine tuning multimodal models LLMs on the Edge 16.1 Edge-Optimized LLM Architectures, case studies 16.2 Edge Optimization techniques Security &amp; Privacy Engineering 17.1 Threat Model: Prompt Injection, Jailbreaking, Data Leakage Frontiers 17.2 Emerging Topics: Mamba, Qwen Next, Hybrid architectures Presentations 18.1 Student Presentations I 18.2 Student Presentations II"},{"location":"schedule/#materials","title":"Materials","text":"<ul> <li>Lecture slides and notes will be shared here as class progresses.</li> </ul>"}]}