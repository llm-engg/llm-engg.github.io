## Large Language Models : A Hands on Approach

### Course Orientation

---

## Course Overview

- **Duration**: ~18 Weeks
- **Format**: 2 sessions per week (Tue & Thu, 7:00–8:30 PM IST)
- **Style**: Lectures, demos and labs
- **Assessment**: Quizzes, assignments, and final project/presentation
- **Credits**: 3 + 1 (Credits added in ABC)

---

## Why This Course?

LLMs have changed how we build intelligent applications.

This course aims to bridge the gap between:

**Using LLMs via APIs** → **Engineering LLM systems**

---

## Learning Outcomes

- Understand LLM architectures
- Fine-tune models for specific domains and tasks
- Optimize inference for cost and latency
- Build robust LLM-powered applications (RAG, agents, tool use)
- Evaluate, debug, and improve model performance
- Leverage open-source ecosystems for LLM engineering

---

## What This Course Will NOT Cover

- Mathematical foundations
- Pretraining LLMs from scratch
- State of the art in research
- Data engineering pipelines
- Ethics and societal impacts

---

## Prerequisites

- **Programming**: Python proficiency
- **ML Basics**: Understanding of neural networks and deep learning
- **Helpful**: Familiarity with PyTorch, basic NLP concepts

---

## Tools and Resources

### Software Stack

- Python
- PyTorch
- HuggingFace Ecosystem(models, libraries and datasets)

---

## Compute Stack

| Option | Cost | Notes |
|--------|------|-------|
| Google Colab | Free | Limited GPU, good for experiments |
| GCP Free Credits | Free | $300 for new accounts |
| Colab Pro+ | Paid | More GPU time, better GPUs |
| RunPod | Paid | Flexible, longer runs |
| Lightning AI | Paid | Easy setup, good UX |

*Free options suffice for most of our course needs*

---

## Evaluation Criteria

| Component | Weight | Notes
|-----------|--------|-------
| Quizzes | 25% | 5 short in-class quizzes
| Assignments | 30% | 2 assignments (Mid Feb and Mid Mar)
| Final Project/Presentation | 40% | Mar 3rd week onwards
| Participation | 5% | In-class and forum engagement

--

## ABC 

- [Academic Bank of Credits](https://www.abc.gov.in/)
- Credits will be added to your ABC account after successful course completion
- More information will be shared during the course

---

## Final Project Option

Individual or group project (max 2).

Build an end-to-end LLM application:

- Model fine-tuning
- Inference optimization
- LLM Application (RAG, Agents)
- Open source contribution
- Other ideas welcome too

**Deliverables**: Documentation + Demo / Presentation

---

## Presentation Option

- **Topic**: Novel concept not covered in class
- **Format**: Research paper or case study
- **Deliverables**: Slides or summary document
- **Evaluation**: Depth of understanding, presentation quality
- **Duration**: 10 minutes, 8-10 slides


---

## Course Roadmap (1/2)

| Module | Topic |
|:---|:---|
| 1 | **LLM Foundations** - Transformers, GPT-2, Modern Architectures, MoE, OSS Models |
| 2 | **GPUs** - Architecture and Programming,  Multi-GPU Parallelism, Hardware Stack |
| 3 | **Inference** - Sampling, KV Caching, Quantization, Speculative Decoding, Model Serving |
| 4 | **Fine-Tuning** - SFT, PEFT (LoRA, QLoRA etc),RLHF (DPO, GRPO etc), Distillation |
| 5 | **Reasoning** - Chain-of-Thought (CoT), Test Time Scaling, Finetuning |

---

## Course Roadmap (2/2)

| Module | Topic |
|:---|:---|
| 6 | **RAG & Agents** - RAG Fundamentals, ReAct, Tools, Protocols, Agents, Finetuning |
| 7 | **Evaluation** - Moderl Evaluations, Benchmarks, LLM-as-a-Judge |
| 8 | **Multimodal** - Multimodal Architectures, Finetuning |
| 9 | **Edge Deployment** - Edge architectures, Optimization and Deployment |

---

## Making the Best of This Course

- **Participate actively**: Ask questions in class, engage in discussions
- **Practice hands-on**: Run the code, experiment with parameters
- **Use AI tools**: Leverage AI assistants for coding and learning
- **Stay connected**: Use forum for async discussions

---

## Q&A

### Any questions?
