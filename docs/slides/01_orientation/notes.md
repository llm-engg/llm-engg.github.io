## Course Overview and Structure

- **Duration**: ~18 Weeks
- **Format**: 2 sessions per week, Tuesday and Thursday (7:00 PM to 8:30 PM IST)
- **Style**: Mix of lectures, demos, labs, and projects
- **Assessment**: Assignments, quizzes, and final project

## Why this course?

LLMs have changed how we build intelligent applications. This course bridges the gap between **using LLMs via APIs** and **engineering LLM systems you can deploy and operate in production**.

### Learning Outcomes

You will learn how to:
- Understand LLM architectures
- Fine-tune models for specific domains and tasks
- Optimize inference for cost and latency
- Build robust LLM-powered applications (RAG, agents, tool use)
- Evaluate, debug, and improve model performance systematically
- Make use open-source ecosystems for LLM engineering

### What This Course Will NOT Cover

- Mathematical foundations
- Pretraining LLMs from scratch 
- State of the art in research
- Ethics and societal impacts
- Data engineering pipelines

## Prerequisites

- **Programming**: Python proficiency
- **ML Basics**: Understanding of neural networks and deep learning concepts
- **Helpful**: Familiarity with PyTorch, basic NLP concepts

## Tools and Resources

### Software
- Python
- PyTorch
- HuggingFace (transformers, datasets, peft, trl)

### Hardware / Compute
| Option | Cost | Notes |
|--------|------|-------|
| Google Colab | Free | Limited GPU time, good for experiments |
| GCP Free Credits | Free | $300 credits for new accounts |
| Colab Pro+ | Paid | More GPU time, better GPUs |
| RunPod | Paid | Flexible, good for longer runs |
| Lightning AI | Paid | Easy setup, good UX |

- *first two will suffice for most assignments and labs, for final projects you might need paid options depending on your project scope

## Evaluation Criteria

| Component | Weight | Description |
|-----------|--------|-------------|
| Quizzes | 20% | 4â€“5 short in-class quizzes |
| Assignments | 30% | 2 assignments covering key modules |
| Final Project or Presentation | 50% | Project : Build and deploy an end-to-end LLM application (agents or RAG/systems), or improve an existing approach (e.g., inference optimization or fine-tuning). Presentation : Cover a novel concept not covered in the class (rsearch paper or case study) |

### Final Project or Presentation Details
Project Option:
- Build an end-to-end LLM application
- Options: Fine-tuned model, RAG system, agent application
- Deliverables: documentation, demo and 
- Evaluation: Functionality, 

Presentation Option:
- Topic: Novel concept not covered in class
- Deliverables: Slides or summary document
- Evaluation: Depth of understanding, Presentation quality

## Grading Policy
TODO (@yknegi): Add grading policy details
ABC 

## Course Roadmap
TODO (@yknegi): remove course roadmap if not needed
| Module | Topic | Key Concepts |
|:---|:---|:---|
| 1 | **LLM Foundations** | Transformers, GPT-2 style, Modern Architectures, Mixture of Experts, Open-Source LLMs |
| 2 | **GPUs** | GPU Architecture, Multi-GPU Parallelism, Hardware Stack |
| 3 | **Optimizing Inference** | Sampling, Memory Optimization (KV Caching), Quantization, Inference Engines (vLLM, SGLang) |
| 4 | **Fine-Tuning** | Full Fine-Tuning, PEFT (LoRA, QLoRA), Instruction Tuning, Alignment (DPO, RLHF), Distillation, Reasoning |
| 5 | **RAG and Agents** | RAG Fundamentals, Agentic RAG, ReAct, Tools, MCP, Multi-Agent Systems |
| 6 | **Evaluation** | Frameworks, MMLU, LMSYS Arena, LLM-as-a-Judge, Error Analysis |
| 7 | **Multimodal** | Multimodal Architectures, Visual Instruction Fine-tuning |
| 8 | **Edge Deployment** | Edge Architectures, llama.cpp, Optimization, and Deployment |

## Making the Best of This Course

- **Participate actively**: Ask questions, engage in discussions
- **Practice hands-on**: Run the code, experiment with parameters
- **Stay connected**: Use forum for async discussions

## Q&A

Any questions?


